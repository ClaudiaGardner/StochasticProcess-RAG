# 第 0 章 - 题目与解答

**本章共 52 道题目**

---

## 例题0.1

### 题目

抛一枚硬币，根据实际情况，可能选择如下模型。 模型1 （1）样本空间： $\Omega=\{$ 正，反}。 （2）事件类： $\mathcal{F=} \{\emptyset, \ \{\pm\} , \ \{\acute{\mathcal{X}} \} , \ \{\pm, \ \acute{\mathcal{X}} \} \}_{\phi}$ . (3）概率： $P ( \{\mathtt{E} \} )=\frac{1} {2} , \mathrm{~} P ( \{\mathtt{E} \} )=\frac{1} {2}$ . 模型2 （1）样本空间、事件类同模型1。 M2BM $P ...

### 解答

**Clarifying the Models**

I'm now focusing on a succinct and informative presentation of the three coin-toss models. The goal is to articulate each probability space $(\Omega, \mathcal{F}, P)$ clearly. I will ensure the explanations emphasize the critical differences in the sample spaces and the probability measures, adhering to a direct, academic tone. The knowledge extension section will highlight the underlying axioms and potential misconceptions.


### 题目分析
本题并非一道传统的计算题，而是一个展示如何将物理实验（抛硬币）抽象为数学模型（概率空间）的概念性例题。题目给出了针对同一物理现象的三种不同建模方式。
本题考查的核心概念是**概率空间（Probability Space）**的定义及其三个组成部分：**样本空间（Sample Space, $\Omega$）**、**事件域（$\sigma$-代数, $\mathcal{F}$）**以及**概率测度（Probability Measure, $P$）**。
解题目标是解析这三个模型的构建逻辑，明确概率论公理化定义中各要素的含义。

### 解题过程

概率论的数学基础建立在柯尔莫哥洛夫（Kolmogorov）公理体系之上。一个概率模型由三元组 $(\Omega, \mathcal{F}, P)$ 描述。我们将逐一分析题目中的三个模型。

#### 1. 概率空间的基本定义
在分析具体模型前，需明确三元组的定义：
- $\Omega$：样本空间，试验所有可能结果的集合。
- $\mathcal{F}$：事件域，是 $\Omega$ 的子集构成的集合（$\sigma$-代数），满足对补集和可列并集封闭。
- $P$：概率测度，是定义在 $\mathcal{F}$ 上的集函数，满足非负性、归一性和可列可加性。

#### 2. 模型 1 分析：理想的均匀硬币
这是最经典的古典概型。
- **样本空间**：$\Omega_1 = \{\text{正}, \text{反}\}$。包含两个基本结果。
- **事件域**：$\mathcal{F}_1$ 取 $\Omega_1$ 的幂集（即所有子集的集合）：
  $$ \mathcal{F}_1 = 2^{\Omega_1} = \{\emptyset, \{\text{正}\}, \{\text{反}\}, \{\text{正}, \text{反}\}\} $$
- **概率测度**：假设硬币是均匀的（Fair Coin），根据对称性：
  $$ P(\{\text{正}\}) = \frac{1}{2}, \quad P(\{\text{反}\}) = \frac{1}{2} $$
  且满足 $P(\Omega_1) = 1, P(\emptyset) = 0$。

#### 3. 模型 2 分析：非均匀硬币
此模型展示了在样本空间和事件域相同的情况下，概率测度可以不同。
- **样本空间**：$\Omega_2 = \Omega_1 = \{\text{正}, \text{反}\}$。
- **事件域**：$\mathcal{F}_2 = \mathcal{F}_1$。
- **概率测度**：假设硬币质地不均匀或受外力影响，出现正面的可能性较小：
  $$ P(\{\text{正}\}) = \frac{1}{4}, \quad P(\{\text{反}\}) = \frac{3}{4} $$
  这依然构成一个合法的概率空间，因为它满足 $P(\{\text{正}\}) + P(\{\text{反}\}) = 1$。

#### 4. 模型 3 分析：考虑竖立情况的硬币
此模型修正了样本空间，考虑了极小概率的物理情况（硬币竖立）。
- **样本空间**：$\Omega_3 = \{\text{正}, \text{反}, \text{立}\}$。此时基本结果有三个。
- **事件域**（题目从略，此处补全）：通常取幂集 $\mathcal{F}_3 = 2^{\Omega_3}$，包含 $2^3=8$ 个事件：
  $$ \mathcal{F}_3 = \{\emptyset, \{\text{正}\}, \{\text{反}\}, \{\text{立}\}, \{\text{正}, \text{反}\}, \{\text{正}, \text{立}\}, \{\text{反}, \text{立}\}, \Omega_3\} $$
- **概率测度**（题目从略，此处补全）：设 $p_1, p_2, p_3$ 分别为正、反、立的概率，需满足：
  $$ P(\{\text{正}\}) = p_1, \quad P(\{\text{反}\}) = p_2, \quad P(\{\text{立}\}) = p_3 $$
  且必须满足归一性条件：
  $$ p_1 + p_2 + p_3 = 1, \quad p_i \ge 0 $$
  例如，实际情况可能为 $p_1 \approx 0.4999, p_2 \approx 0.4999, p_3 \approx 0.0002$。

### 答案

该例题展示了针对“抛硬币”这一物理试验构建的三个概率空间 $(\Omega, \mathcal{F}, P)$：

**模型 1（理想均匀模型）**：
$$ \Omega = \{\text{正}, \text{反}\}, \quad P(\{\text{正}\}) = P(\{\text{反}\}) = 0.5 $$

**模型 2（非均匀模型）**：
$$ \Omega = \{\text{正}, \text{反}\}, \quad P(\{\text{正}\}) = 0.25, \quad P(\{\text{反}\}) = 0.75 $$

**模型 3（包含竖立模型）**：
$$ \Omega = \{\text{正}, \text{反}, \text{立}\}, \quad \sum_{\omega \in \Omega} P(\{\omega\}) = 1 $$

### 知识延伸

1.  **建模的非唯一性**：同一个随机试验可以根据研究目的和精度要求建立不同的概率模型。模型 1 适用于大多数理论推导；模型 2 适用于有偏差的实际情况；模型 3 适用于高精度物理研究。
2.  **$\sigma$-代数（事件域）的重要性**：对于有限样本空间，通常取幂集为事件域。但在连续型样本空间（如 $\Omega = [0, 1]$）中，不能取幂集，通常取 **Borel $\sigma$-代数**，以避免出现不可测集。
3.  **柯尔莫哥洛夫公理（Kolmogorov Axioms）**：
    *   **非负性**：对任意 $A \in \mathcal{F}$，有 $P(A) \ge 0$。
    *   **规范性（归一性）**：$P(\Omega) = 1$。
    *   **可列可加性**：若 $A_1, A_2, \dots$ 互不相容（即 $A_i \cap A_j = \emptyset, i \neq j$），则：
        $$ P\left(\bigcup_{n=1}^{\infty} A_n\right) = \sum_{n=1}^{\infty} P(A_n) $$
4.  **易错点**：初学者常混淆样本空间 $\Omega$（结果的集合）与事件域 $\mathcal{F}$（事件的集合）。概率是定义在 $\mathcal{F}$ 上的，而不是直接定义在 $\Omega$ 的元素上的（虽然在离散情况下通常通过基本事件的概率求和得到）。

---

## 例题0.2

### 题目

掷一均匀骰子 （1）样本空间 Ω = {1，2，3，4，5，6}。 （2）取 A = {1，3，5}。{Ω，A，∅} 是事件类吗？ （3）{Ω，A， ¯A，∅} 是事件类吗？ （4）F = 2 Ω 是事件类吗？ （5）对于有限样本空间，我们通常取所有子集构成的类为事件类，即任意一个子集都视作事件。若有限 样本空间大小为 |Ω|，问共有多少子集？ （6）若样本空间为有限样本空间，则概率的定义通常以定义单点集的概率为基本方式。若骰子是均匀的， 则定义 P({1}) = P({2}) = · · · = P({6}) = 1 6。 注：我们可以用 P(1) 简单记概率 P({1})，不过要注意概率 P 是一个广义的函数，其定义域是事件类， 而自变量是事件。我们也可以用 P(A) 来表示事件 A 的概率。同一样本空间及事件类上可以定义不同的概率函 数。此时可以用 Q(A) 等表示事件 A 的概率。

### 解答

**Examining Solutions**

\n\n

I'm currently formalizing the "Solution Process" section, meticulously following the event class definition. Each part of the prompt is addressed rigorously. I've also begun writing the "Answer" section, which will summarize the key findings. The "Knowledge Extension" is being drafted, too, focusing on relevant concepts. I am checking LaTeX formatting and adjusting the tone for conciseness.


### 题目分析
本题以掷均匀骰子为背景，主要考查概率论公理化定义中的**样本空间**（Sample Space）和**事件域**（Event Class，又称 $\sigma$-代数或 $\sigma$-域）的概念。
题目中涉及的数学符号包括：样本空间 $\Omega$，空集 $\emptyset$，事件 $A$ 及其补集 $\bar{A}$，以及幂集 $2^\Omega$（即 $\Omega$ 的所有子集构成的集合）。
核心求解目标是验证给定的集合族是否满足 $\sigma$-代数的定义，并计算有限样本空间的子集总数。

### 解题过程

**定义回顾**：
在概率论中，**事件类**（或 $\sigma$-代数）$\mathcal{F}$ 是样本空间 $\Omega$ 的子集构成的一个非空集合族，且满足以下三个条件：
1.  $\Omega \in \mathcal{F}$；
2.  若 $A \in \mathcal{F}$，则 $\bar{A} \in \mathcal{F}$（对补集封闭）；
3.  若 $A_1, A_2, \dots \in \mathcal{F}$，则 $\bigcup_{n=1}^{\infty} A_n \in \mathcal{F}$（对可列并集封闭）。
对于有限样本空间，条件3等价于对有限并集封闭。

**（1）样本空间**
已知 $\Omega = \{1, 2, 3, 4, 5, 6\}$。

**（2）验证 $\mathcal{C}_1 = \{\Omega, A, \emptyset\}$ 是否为事件类**
已知 $A = \{1, 3, 5\}$。
检查对补集的封闭性：
$A \in \mathcal{C}_1$，其补集 $\bar{A} = \Omega \setminus A = \{2, 4, 6\}$。
显然 $\bar{A} \notin \mathcal{C}_1$。
**结论**：$\{\Omega, A, \emptyset\}$ **不是**事件类。

**（3）验证 $\mathcal{C}_2 = \{\Omega, A, \bar{A}, \emptyset\}$ 是否为事件类**
1.  **包含样本空间**：$\Omega \in \mathcal{C}_2$，满足。
2.  **对补集封闭**：
    *   $\Omega^c = \emptyset \in \mathcal{C}_2$
    *   $\emptyset^c = \Omega \in \mathcal{C}_2$
    *   $A^c = \bar{A} \in \mathcal{C}_2$
    *   $(\bar{A})^c = A \in \mathcal{C}_2$
    满足。
3.  **对并集封闭**（有限个元素的组合）：
    *   $A \cup \bar{A} = \Omega \in \mathcal{C}_2$
    *   $A \cup \emptyset = A \in \mathcal{C}_2$
    *   $A \cup \Omega = \Omega \in \mathcal{C}_2$
    *   $\bar{A} \cup \emptyset = \bar{A} \in \mathcal{C}_2$
    *   ... 其他组合同理。
    满足。
**结论**：$\{\Omega, A, \bar{A}, \emptyset\}$ **是**事件类。这是一个由 $A$ 生成的最小 $\sigma$-代数，记为 $\sigma(A)$。

**（4）验证 $\mathcal{F} = 2^\Omega$ 是否为事件类**
$2^\Omega$ 表示 $\Omega$ 的所有子集构成的集合（幂集）。
1.  $\Omega$ 是其自身的子集，故 $\Omega \in 2^\Omega$。
2.  任意子集 $E \subseteq \Omega$，其补集 $E^c$ 依然是 $\Omega$ 的子集，故 $E^c \in 2^\Omega$。
3.  任意子集序列的并集依然是 $\Omega$ 的子集，故属于 $2^\Omega$。
**结论**：$2^\Omega$ **是**事件类。这是样本空间上最大的 $\sigma$-代数。

**（5）计算有限样本空间的子集个数**
设样本空间 $\Omega$ 的大小（元素个数）为 $|\Omega| = n$。
构造 $\Omega$ 的一个子集相当于对 $\Omega$ 中的每一个元素进行选择（“在子集中”或“不在子集中”）。
每个元素有 2 种选择。
根据乘法原理，总的子集个数为：
$$ \underbrace{2 \times 2 \times \dots \times 2}_{n \text{ times}} = 2^n $$
在本题中，对应的大小为 $2^{|\Omega|}$。
若 $|\Omega|=6$，则子集个数为 $2^6 = 64$。

**（6）关于概率定义的说明**
此部分为题目陈述的概率定义方式。对于离散有限样本空间，概率测度 $P$ 通常定义在幂集 $2^\Omega$ 上。
若骰子均匀，单点集概率为：
$$ P(\{k\}) = \frac{1}{6}, \quad k=1, \dots, 6 $$
对于任意事件 $E \in 2^\Omega$，其概率为：
$$ P(E) = \sum_{k \in E} P(\{k\}) = \frac{|E|}{|\Omega|} $$
这符合拉普拉斯古典概型的定义。

### 答案

1.  $\{\Omega, A, \emptyset\}$ **不是**事件类。
2.  $\{\Omega, A, \bar{A}, \emptyset\}$ **是**事件类。
3.  $\mathcal{F} = 2^\Omega$ **是**事件类。
4.  若有限样本空间大小为 $|\Omega|$，共有 **$2^{|\Omega|}$** 个子集。

### 知识延伸

1.  **$\sigma$-代数（事件类）的重要性**：
    概率 $P$ 是一个集合函数，它并不是定义在 $\Omega$ 的所有子集上（尤其是在连续型样本空间中，如 $\mathbb{R}$，存在不可测集），而是定义在 $\sigma$-代数 $\mathcal{F}$ 上。三元组 $(\Omega, \mathcal{F}, P)$ 构成了严格的**概率空间**。

2.  **特殊的 $\sigma$-代数**：
    *   **平凡 $\sigma$-代数**：$\{\emptyset, \Omega\}$，是最小的事件类。
    *   **离散 $\sigma$-代数（幂集）**：$2^\Omega$，是最大的事件类。对于有限或可列样本空间，通常取此作为事件类。

3.  **易错点**：
    *   混淆“元素”与“子集”。例如 $1 \in \Omega$，但 $1 \notin \mathcal{F}$，应该是 $\{1\} \in \mathcal{F}$。事件类是“集合的集合”。
    *   在验证事件类时，忘记检查“对补集封闭”这一条件。如本题第(2)问，仅有 $A$ 而无 $\bar{A}$ 是不构成事件类的。

---

## 例题0.3

### 题目

一个箱子里有 $M$ 个可区分的球，从中取出 $n$ 个，分析如下试验的样本空间的大小。 （1）有放回：有序，无序； (2）不放回：有序，无序。

### 解答

**Analyzing Problem Components**

I've just finished the problem analysis stage. The goal is now to clarify the key symbols ($M$, $n$) and concepts (combinatorics, sampling, etc.). I'm solidifying the four distinct scenarios: ordered/unordered with/without replacement. The core focus remains on deriving the correct formulas for each situation, particularly the unordered with replacement case.


### 题目分析
本题考查概率论与组合数学中的**计数原理（Counting Principles）**，具体涉及排列（Permutation）、组合（Combination）以及多重集排列组合的问题。
题目中 $M$ 代表总体中可区分元素的个数，$n$ 代表抽取的样本量。
核心目标是计算在四种不同抽样机制（有放回/不放回、有序/无序）下的**样本空间大小**（即基本事件的总数 $|\Omega|$）。

### 解题过程

我们将样本空间记为 $\Omega$，其大小记为 $|\Omega|$。

#### （1）有放回情形

**情形 1.1：有放回、有序 (With replacement, Ordered)**
在此情形下，每次抽取都有 $M$ 种可能的结果，且先后抽取的顺序不同被视为不同的样本。
- 第 1 次抽取有 $M$ 种选择；
- 第 2 次抽取仍有 $M$ 种选择（因为是“有放回”）；
- $\dots$
- 第 $n$ 次抽取也有 $M$ 种选择。

根据乘法原理，样本空间的大小为：
$$ |\Omega| = \underbrace{M \times M \times \dots \times M}_{n \text{ times}} = M^n $$
这对应于物理统计中的麦克斯韦-玻尔兹曼统计（Maxwell-Boltzmann statistics）的基本计数模型。

**情形 1.2：有放回、无序 (With replacement, Unordered)**
在此情形下，我们只关心 $M$ 个球中每一个被抽到了多少次，而不关心抽取的先后顺序。
这等价于将 $n$ 个不可区分的球（代表 $n$ 次抽取机会）放入 $M$ 个可区分的盒子（代表 $M$ 个球）中。或者理解为求方程 $x_1 + x_2 + \dots + x_M = n$ 的非负整数解的个数，其中 $x_i$ 表示第 $i$ 个球被抽到的次数。

利用**隔板法（Stars and Bars）**：
我们需要在 $n$ 个“球”和 $M-1$ 个“隔板”之间进行排列。总位置数为 $n + (M-1)$，我们要从中选择 $n$ 个位置放球（或选择 $M-1$ 个位置放隔板）。
因此，样本空间的大小为：
$$ |\Omega| = \binom{n+M-1}{n} = \binom{n+M-1}{M-1} $$
这对应于物理统计中的玻色-爱因斯坦统计（Bose-Einstein statistics）的计数模型。

---

## 例题0.4

### 题目

把 n 个球扔进M 个箱子，分析如下试验的样本空间的大小。 （1）球可区分，箱子可区分，不限箱内球数； （2）球不可区分，箱子可区分，不限箱内球数； （3）球可区分，箱子可区分，箱内球数不超过 1； （4）球不可区分，箱子可区分，箱内球数不超过 1； （5）球不可区分，箱子不可区分，箱内球数不限。 0.1.2 概率的基本运算律 在概率中，我们有以下运算法则： 1. 加法：对于事件A，B， • P(A SB) = P(A) + P(B) − P(AB)；P(A SB) ≤ P(A) + P(B)。 • 若A 与 B 不相容，则 P(A SB) = P(A) + P(B)（分类） 。 • 容斥定理：若事件集 A1，· · ·，An 为有限集，则有 P{∪n i=1Ai} = nX i=1 P{Ai} − X 1≤i<j≤n P{Ai ∩ Aj} + X 1≤i<j<k≤n P{Ai ∩ Aj ∩ Ak} − · · · + (−1)n−1P{A1 ∩ · · · ∩ An}。 2. 乘法： • 条件概率定义：设A 与B 为样本空间Ω 中的两个事件，其中P{A} > 0。那么在事件A 发生的条件下，事 件B 发生的条件概率为： P(B|A) = P(AB) P(A) ； • P(AB) = P(B|A)P(A)；若 A 与B 相互独立，则 P(AB) = P(A)P(B)； • 链式法则：P(A1A2 . . . An) = P(A1)P(A2|A1) · · ·P(An|A1A2...An−1)； 2 0.1. 概率论的基本概念 • 多事件独立的定义：设有 A1，A2，. . . ，An(n ≥ 2) 个事件，如果其中任意 k (2 ≤ k ≤ n) 个事件的积事 件的概率都等于各事件概率之积，即 P(Ai1 Ai2 · · ·Aik ) = kY j=1 P(Aij )， 对于所有1 ≤ i1 < i2 < · · · < ik ≤ n 均成立，则称它们相互独立。 3. 减法： • P( ¯A) = 1 − P(A)； • P(A − B) = P(A) − P(AB)。 4. 全概率公式：若 Bi 是样本空间 Ω 的划分，则有P(A) = P i P(Bi)P(A|Bi)（分类、分步） 。 5. 贝叶斯公式：P(Bi|A) = P(Bi)P(A|Bi) P(A) = P(Bi)P(A|Bi)P j P(Bj)P(A|Bj) 。 贝叶斯分析在工程中有广泛的应用背景。我们可以把 Bi 视作引起 A 发生的原因，P(Bi) 称之为先验概率， 而 P(Bi|A) 是后验概率，反映事件 A 对于 Bi 概率的影响。条件概率 P(A|Bi) 通常称为似然概率，是“由因及 果” ，而贝叶斯公式计算的后验概率是“由果及因” ，是概率推断的重要工具。在实际应用中，我们通常可以借 助图模型来分析复杂的概率问题，概率树可以用来描述“分类”与“分步” ： “分类”用“分支” ， “分步”用“分 节” 。从根节点（全集，或者说“必然事件” ）开始做划分，随着树的生长，划分得越来越细。每个子节点是父节 点的子事件，所有子节点划分（不重不漏）它们的父节点。根节点的概率是1。每条分支有个权重，对应给定父 节点条件下相应子节点发生的概率。因此，从一个节点出发的各个分支的权重之和为1。某个节点的概率是从根 节点到达该节点的路径的概率，而该概率是边概率的连乘积。一个子树的概率是该子树所有叶子对应的概率的 和，这个子树的概率是等于该子树的根节点的概率的。在概率树中，一个节点的概率既可以由其父亲节点通过 一次乘法算得（由上至下） ，也可以由其儿子节点通过加法算得（由下至上） 。有些问题在题目中描述比较直接， 有些需要自己提炼，整理出“概率树” ，不同的“概率树”对应不同的概率模型。

### 解答

**Refining Case Five Logic**

\n\n

I've been meticulously re-evaluating Case 5, the most challenging scenario. Initially, I recognized it as an integer partition problem, but now I'm clarifying the explanation to emphasize the lack of a simple closed-form solution. I'll explicitly mention partitioning the integer *n* into at most *M* parts and using the appropriate notation, ensuring precision for my audience. This is vital given the context.


### 题目分析

本题考查概率论基础中的**样本空间计数问题**，本质上属于**组合数学（Combinatorics）**中的“球盒模型”（Balls and Bins）。题目要求计算在不同约束条件下，将 $n$ 个球放入 $M$ 个箱子中的所有可能结果的数量（即样本空间 $|\Omega|$ 的大小）。

核心概念涉及：
1.  **乘法原理**：分步计数。
2.  **排列（Permutation）**：有序选择。
3.  **组合（Combination）**：无序选择。
4.  **隔板法（Stars and Bars）**：处理不可区分物体的分配问题。
5.  **整数分拆（Integer Partition）**：处理全不可区分问题。

需要求解的目标是针对五种不同情况，给出样本空间大小的数学表达式。

---

## 例题0.5

### 题目

在0 到200 的整数中， 求A = {能被 3 整除的数}，B = {能被 5 整除的数}， 和C = {能被7 整除 的数} 并集的大小。

### 解答

**Reviewing Calculations**

\n\n

I've carefully re-examined the arithmetic within the Inclusion-Exclusion Principle application. Each set's cardinality has been verified, and the intersections appear correctly calculated. The final result of 109 seems sound, considering the range and divisibility criteria.


### 题目分析

本题要求计算在整数区间 $[0, 200]$ 内，能被 3、5 或 7 整除的数的集合并集的大小。这是一个典型的集合计数问题，考查的核心概念是**容斥原理（Inclusion-Exclusion Principle）**。虽然本题属于组合数学范畴，但它是概率论中计算事件并集概率 $P(A \cup B \cup C)$ 的基础模型。解题的关键在于正确计算各个集合及其交集的基数，并注意区间包含 $0$ 这一特殊情况。

### 解题过程

定义全集为 $\Omega = \{0, 1, 2, \dots, 200\}$。
设 $A$ 为能被 3 整除的数，$B$ 为能被 5 整除的数，$C$ 为能被 7 整除的数。
我们需要求解的是 $|A \cup B \cup C|$。

根据**容斥原理**，三个集合并集的大小计算公式为：
$$|A \cup B \cup C| = |A| + |B| + |C| - (|A \cap B| + |A \cap C| + |B \cap C|) + |A \cap B \cap C|$$

**第一步：计算单个集合的基数**

在区间 $[0, n]$ 中，能被 $k$ 整除的数的个数为 $\lfloor \frac{n}{k} \rfloor + 1$（加 1 是因为 0 也能被任何非零整数整除）。

1.  计算 $|A|$（能被 3 整除）：
    $$|A| = \left\lfloor \frac{200}{3} \right\rfloor + 1 = 66 + 1 = 67$$
2.  计算 $|B|$（能被 5 整除）：
    $$|B| = \left\lfloor \frac{200}{5} \right\rfloor + 1 = 40 + 1 = 41$$
3.  计算 $|C|$（能被 7 整除）：
    $$|C| = \left\lfloor \frac{200}{7} \right\rfloor + 1 = 28 + 1 = 29$$

**第二步：计算两两交集的基数**

两个集合的交集意味着数必须同时被两个数整除，即被它们的**最小公倍数（LCM）**整除。

1.  计算 $|A \cap B|$（能被 $\text{lcm}(3, 5) = 15$ 整除）：
    $$|A \cap B| = \left\lfloor \frac{200}{15} \right\rfloor + 1 = 13 + 1 = 14$$
2.  计算 $|A \cap C|$（能被 $\text{lcm}(3, 7) = 21$ 整除）：
    $$|A \cap C| = \left\lfloor \frac{200}{21} \right\rfloor + 1 = 9 + 1 = 10$$
3.  计算 $|B \cap C|$（能被 $\text{lcm}(5, 7) = 35$ 整除）：
    $$|B \cap C| = \left\lfloor \frac{200}{35} \right\rfloor + 1 = 5 + 1 = 6$$

**第三步：计算三个集合交集的基数**

三个集合的交集意味着数必须同时被 3、5、7 整除。

1.  计算 $|A \cap B \cap C|$（能被 $\text{lcm}(3, 5, 7) = 105$ 整除）：
    $$|A \cap B \cap C| = \left\lfloor \frac{200}{105} \right\rfloor + 1 = 1 + 1 = 2$$
    *(注：这两个数是 0 和 105)*

**第四步：代入容斥原理公式**

$$
\begin{aligned}
|A \cup B \cup C| &= (67 + 41 + 29) - (14 + 10 + 6) + 2 \\
&= 137 - 30 + 2 \\
&= 109
\end{aligned}
$$

### 答案

$$|A \cup B \cup C| = 109$$

### 知识延伸

本题主要涉及**容斥原理**，这是概率论中计算复合事件概率的重要工具。

**核心公式：**
对于 $n$ 个有限集合 $A_1, \dots, A_n$，其并集大小为：
$$ \left| \bigcup_{i=1}^n A_i \right| = \sum_{1 \le i \le n} |A_i| - \sum_{1 \le i < j \le n} |A_i \cap A_j| + \sum_{1 \le i < j < k \le n} |A_i \cap A_j \cap A_k| - \dots + (-1)^{n-1} |A_1 \cap \dots \cap A_n| $$

**在概率论中的对应形式：**
$$ P\left( \bigcup_{i=1}^n A_i \right) = \sum P(A_i) - \sum P(A_i \cap A_j) + \dots $$

**常见易错点：**
1.  **边界问题**：题目区间是 $[0, 200]$ 还是 $[1, 200]$。若从 0 开始，必须考虑 0 能被任意非零整数整除，计数时需 $+1$。
2.  **交集定义**：计算交集时应使用除数的**最小公倍数**，而非简单的乘积（尽管本题中 3, 5, 7 互质，乘积等于最小公倍数，但在非互质情况下直接相乘会导致错误）。

---

## 例题0.6

### 题目

设 P(A(B ∪ C)) = 0 .3，P( ¯A) = 0 .6，P( ¯A ¯B ¯C) = 0 .1，求 P(B ∪ C)。

### 解答

**Analyzing the Approach**

\n\n

I'm currently focused on structuring the solution using the provided steps. I've broken down the problem into key components, like understanding the givens and the target. I'm also preparing the format for the output, using sections to present the solution clearly. I'm about to begin drafting the content for each section, ensuring correct LaTeX formatting and adherence to the specified tone.


### 题目分析
本题考查概率论中的基本运算公式，主要涉及事件的并、交、补运算及其概率性质。题目中符号 $A(B \cup C)$ 表示事件 $A$ 与事件 $B \cup C$ 的交集，即 $A \cap (B \cup C)$；符号 $\bar{A}$ 表示 $A$ 的对立事件（补集）；符号 $\bar{A}\bar{B}\bar{C}$ 表示 $\bar{A} \cap \bar{B} \cap \bar{C}$。解题的核心在于利用德·摩根定律（De Morgan's laws）将交集的补转化为并集，以及利用概率加法公式建立已知量与未知量 $P(B \cup C)$ 之间的等量关系。

### 解题过程

**步骤 1：由已知条件计算 $P(A)$**

根据对立事件的概率性质 $P(A) = 1 - P(\bar{A})$，已知 $P(\bar{A}) = 0.6$，则：
$$P(A) = 1 - 0.6 = 0.4$$

**步骤 2：利用德·摩根定律计算 $P(A \cup B \cup C)$**

已知 $P(\bar{A}\bar{B}\bar{C}) = 0.1$。根据德·摩根定律，$\bar{A} \cap \bar{B} \cap \bar{C} = \overline{A \cup B \cup C}$。
因此：
$$P(\overline{A \cup B \cup C}) = 0.1$$
利用补集概率公式：
$$P(A \cup B \cup C) = 1 - P(\overline{A \cup B \cup C}) = 1 - 0.1 = 0.9$$

**步骤 3：利用概率加法公式求解 $P(B \cup C)$**

我们将 $B \cup C$ 视为一个整体事件，记为 $Y = B \cup C$。根据两个事件并集的概率加法公式：
$$P(A \cup Y) = P(A) + P(Y) - P(A \cap Y)$$
代回 $Y = B \cup C$，公式变为：
$$P(A \cup B \cup C) = P(A) + P(B \cup C) - P(A \cap (B \cup C))$$

**步骤 4：代入数值求解**

将已知和已求得的数值代入上述方程：
- $P(A \cup B \cup C) = 0.9$ （由步骤 2 得出）
- $P(A) = 0.4$ （由步骤 1 得出）
- $P(A \cap (B \cup C)) = 0.3$ （题目已知 $P(A(B \cup C)) = 0.3$）

方程如下：
$$0.9 = 0.4 + P(B \cup C) - 0.3$$

整理得：
$$0.9 = 0.1 + P(B \cup C)$$
$$P(B \cup C) = 0.9 - 0.1 = 0.8$$

### 答案
$$P(B \cup C) = 0.8$$

### 知识延伸

1.  **德·摩根定律 (De Morgan's Laws)**：
    这是集合论与概率论中处理复合事件补集的重要工具：
    $$ \overline{A \cup B} = \bar{A} \cap \bar{B} $$
    $$ \overline{A \cap B} = \bar{A} \cup \bar{B} $$
    推广到三个事件即本题用到的：$\overline{A \cup B \cup C} = \bar{A} \cap \bar{B} \cap \bar{C}$。

2.  **概率加法公式 (Addition Rule)**：
    对于任意两个事件 $A$ 和 $B$：
    $$ P(A \cup B) = P(A) + P(B) - P(AB) $$
    本题的技巧在于将 $B \cup C$ 看作一个整体，避免展开成三个事件的复杂加法公式。

3.  **解题技巧与易错点**：
    - **符号识别**：题目中常省略交集符号 $\cap$，如 $AB$ 或 $A(B \cup C)$，需正确识别为交集运算。
    - **整体思维**：当题目只要求求解 $P(B \cup C)$ 而未给出 $P(B)$ 或 $P(C)$ 的单独值时，不要试图拆解 $B$ 和 $C$，应将其视为一个整体变量求解。
    - **文氏图 (Venn Diagram)**：在处理此类集合运算题目时，画出文氏图可以直观地辅助验证各部分概率之间的关系。

---

## 例题0.7

### 题目

令 $A_{1}$ ， $A_{2}$ ，..和 $B_{1} , \ B_{2} , \ \dots$ 分别为两个事件序列。若 $\operatorname* {l i m}_{n \rightarrow\infty} P ( A_{n} )=1$ 且 $\operatorname* {l i m}_{n \rightarrow\infty} P ( B_{n} )$ $= 1$ ，证明： $\operatorname* {l i m}_{n \rightarrow\infty} P ( A_{n} \cap B_{n} )=1$ .

### 解答

**Examining Proof Strategy**

\n\nI'm now zeroing in on the solution process. My preference is for a direct approach utilizing the Bonferroni inequality to showcase the core concepts with minimal fuss. I've broken down the solution into clear steps, including the use of limits and the application of the squeeze theorem to reach a final concise answer. I will also incorporate an explanation on why independence is not required, as a knowledge extension.


### 题目分析
本题考查概率论中关于事件序列极限的基本性质，具体涉及概率测度的基本运算规则和极限的算术性质。题目给出了两个事件序列 $A_n$ 和 $B_n$，已知它们各自发生的概率在 $n$ 趋于无穷大时均趋近于 1，要求证明它们同时发生的概率（即交集的概率）在 $n$ 趋于无穷大时也趋近于 1。解题的核心在于利用概率加法公式（容斥原理）或邦费罗尼不等式（Bonferroni inequalities）建立交集概率与单个事件概率之间的不等式关系。

### 解题过程

我们需要证明 $\lim_{n\to\infty} P(A_n \cap B_n) = 1$。

**步骤 1：利用概率加法公式**
根据概率论的基本性质，对于任意两个事件 $A_n$ 和 $B_n$，其并集的概率满足以下加法公式：
$$P(A_n \cup B_n) = P(A_n) + P(B_n) - P(A_n \cap B_n)$$

**步骤 2：变换公式以表示交集概率**
将上述公式变形，将 $P(A_n \cap B_n)$ 移至等式左侧：
$$P(A_n \cap B_n) = P(A_n) + P(B_n) - P(A_n \cup B_n)$$

**步骤 3：利用概率的有界性建立不等式**
由于对于任意事件 $E$，其概率 $P(E) \le 1$，因此对于并集事件 $A_n \cup B_n$，必然有：
$$P(A_n \cup B_n) \le 1$$
将此不等式代入步骤 2 的等式中，由于我们要减去 $P(A_n \cup B_n)$，根据不等式性质，减去一个小于等于 1 的数，结果会大于等于减去 1。因此可得下界：
$$P(A_n \cap B_n) \ge P(A_n) + P(B_n) - 1$$
同时，概率本身总是小于等于 1 的，所以我们有如下双边不等式：
$$P(A_n) + P(B_n) - 1 \le P(A_n \cap B_n) \le 1$$

**步骤 4：取极限**
题目已知条件为：
$$\lim_{n\to\infty} P(A_n) = 1 \quad \text{且} \quad \lim_{n\to\infty} P(B_n) = 1$$

对步骤 3 中的不等式两边同时取 $n \to \infty$ 的极限。

考察不等式左侧的极限：
$$
\begin{aligned}
\lim_{n\to\infty} [P(A_n) + P(B_n) - 1] &= \lim_{n\to\infty} P(A_n) + \lim_{n\to\infty} P(B_n) - 1 \\
&= 1 + 1 - 1 \\
&= 1
\end{aligned}
$$

考察不等式右侧的极限：
$$\lim_{n\to\infty} 1 = 1$$

**步骤 5：应用夹逼定理（Squeeze Theorem）**
根据夹逼定理，由于 $P(A_n \cap B_n)$ 被夹在两个趋于 1 的极限之间，即：
$$1 \le \lim_{n\to\infty} P(A_n \cap B_n) \le 1$$

因此，极限存在且为 1。

### 答案
$$ \lim_{n\to\infty} P(A_n \cap B_n) = 1 $$

### 知识延伸

1.  **邦费罗尼不等式 (Bonferroni Inequalities)**：
    本题推导过程中使用的 $P(A \cap B) \ge P(A) + P(B) - 1$ 是邦费罗尼不等式的一阶形式。它给出了交集概率的一个重要下界。
    推广到 $m$ 个事件的情况：
    $$P\left(\bigcap_{i=1}^m A_i\right) \ge \sum_{i=1}^m P(A_i) - (m-1)$$

2.  **布尔不等式 (Boole's Inequality)**：
    本题也可以通过补集来证明。利用 $(A \cap B)^c = A^c \cup B^c$ 以及 $P(A \cup B) \le P(A) + P(B)$（次可加性）。
    若 $P(A_n) \to 1$，则 $P(A_n^c) \to 0$。
    $$P((A_n \cap B_n)^c) = P(A_n^c \cup B_n^c) \le P(A_n^c) + P(B_n^c) \to 0$$
    进而得出原命题成立。

3.  **易错点提示**：
    *   **独立性误区**：初学者容易误以为需要假设 $A_n$ 和 $B_n$ 相互独立（即 $P(AB)=P(A)P(B)$）才能解题。实际上，无论事件是否独立，该结论都成立。
    *   **概率有界性**：在使用夹逼定理时，不要忘记概率的上界是 1，这是构建不等式链条的关键一环。

---

## 例题0.8

### 题目

给定 $B , \, \, \, P ( B ) > 0$ ，则 $P ( * | B )$ 也是概率。 （1）特别地， $P ( A | B )+P ( \bar{A} | B )=1$ 。 (2) $P ( A | B )+P ( A | \bar{B} )$ 等于1吗？讨论 $P ( A | B )$ 与 $P ( A )$ 的大小关系。

### 解答

**Refining Solution Approach**

I'm currently focused on the "解题过程" section, aiming for a rigorous yet concise presentation. The proof of the complement rule within conditional probability is solidified. For part 2, I'm leaning toward a counterexample approach to showcase the non-equivalence of the probabilities. Part 3 will incorporate independence, positive and negative correlation, and a Bayesian perspective for deeper insight. I'm actively integrating LaTeX to render math expressions properly.


### 题目分析
本题主要考查**条件概率（Conditional Probability）**的定义及其基本性质。题目首先要求验证条件概率满足概率公理中的归一性（即互补事件概率之和为1），随后考察当条件发生变化时（$B$ 与 $\bar{B}$），事件 $A$ 的条件概率之和是否具有定值，最后讨论条件概率 $P(A|B)$ 与无条件概率 $P(A)$ 之间的大小关系，这涉及事件独立性与相关性的概念。

### 解题过程

**1. 证明 $P(A|B) + P(\bar{A}|B) = 1$**

根据条件概率的定义，对于任意事件 $A$ 和 $B$（其中 $P(B) > 0$），有：
$$P(A|B) = \frac{P(AB)}{P(B)}$$
$$P(\bar{A}|B) = \frac{P(\bar{A}B)}{P(B)}$$

将两式相加：
$$P(A|B) + P(\bar{A}|B) = \frac{P(AB)}{P(B)} + \frac{P(\bar{A}B)}{P(B)} = \frac{P(AB) + P(\bar{A}B)}{P(B)}$$

观察分子部分，事件 $AB$（$A$ 发生且 $B$ 发生）与 $\bar{A}B$（$A$ 不发生且 $B$ 发生）是互不相容的（互斥的），即 $(AB) \cap (\bar{A}B) = \emptyset$。
同时，根据全概率公式的分解思想，它们的并集构成了事件 $B$：
$$AB \cup \bar{A}B = (A \cup \bar{A}) \cap B = \Omega \cap B = B$$

根据概率的可列可加性：
$$P(AB) + P(\bar{A}B) = P(AB \cup \bar{A}B) = P(B)$$

代回原式：
$$P(A|B) + P(\bar{A}|B) = \frac{P(B)}{P(B)} = 1$$
得证。这说明在给定 $B$ 的条件下，样本空间缩减为 $B$，此时 $A$ 与 $\bar{A}$ 依然构成完备事件组。

---

## 例题0.9

### 题目

掷两个骰子，记：事件 $A=$ {第一个骰子点数为奇数}；事件 $B=\{$ 第二个骰子点数为奇数}； 事件 $C=$ {两个骰子点数和为奇数}；则 $A , \, \, B , \, \, C$ 两两独立，但不互相独立。

### 解答

**Refining the Solution**

\n\n

I've just finished verifying the calculations and LaTeX, and the logic seems sound. The tone is suitably professional and concise. I'm now polishing the answer section to present the final result in mathematical notation, including the formal statement of pairwise independence and the demonstration of non-mutual independence. After this, I'll move on to extending the knowledge section.


### 题目分析

本题考查概率论中**事件的独立性**这一核心概念，具体涉及**两两独立**（Pairwise Independence）与**相互独立**（Mutual Independence）的区别。题目设定在掷两个均匀骰子的样本空间下，定义了三个事件 $A, B, C$。解题目标是通过计算各事件的概率及其交事件的概率，验证 $A, B, C$ 满足两两独立的定义（即 $P(XY)=P(X)P(Y)$），但不满足相互独立的定义（即 $P(ABC) \neq P(A)P(B)P(C)$）。这是一个用于辨析独立性概念的经典反例。

### 解题过程

**1. 建立样本空间与计算基础概率**

设样本空间 $\Omega$ 为掷两个骰子所有可能结果的集合，其中 $(i, j)$ 表示第一个骰子点数为 $i$，第二个骰子点数为 $j$：
$$ \Omega = \{(i, j) \mid i, j \in \{1, 2, 3, 4, 5, 6\}\} $$
样本空间的总元素个数为 $|\Omega| = 6 \times 6 = 36$。

**分析事件 A：**
事件 $A$ 表示“第一个骰子点数为奇数”。
$$ A = \{(i, j) \mid i \in \{1, 3, 5\}, 1 \le j \le 6\} $$
包含的样本点数为 $3 \times 6 = 18$。
故其概率为：
$$ P(A) = \frac{18}{36} = \frac{1}{2} $$

**分析事件 B：**
事件 $B$ 表示“第二个骰子点数为奇数”。
$$ B = \{(i, j) \mid 1 \le i \le 6, j \in \{1, 3, 5\}\} $$
包含的样本点数为 $6 \times 3 = 18$。
故其概率为：
$$ P(B) = \frac{18}{36} = \frac{1}{2} $$

**分析事件 C：**
事件 $C$ 表示“两个骰子点数和为奇数”。
两个整数之和为奇数，当且仅当其中一个为奇数，另一个为偶数。
情况1：第一个奇数，第二个偶数（$3 \times 3 = 9$ 种）。
情况2：第一个偶数，第二个奇数（$3 \times 3 = 9$ 种）。
总样本点数为 $9 + 9 = 18$。
故其概率为：
$$ P(C) = \frac{18}{36} = \frac{1}{2} $$

**2. 验证两两独立性**

**验证 A 与 B：**
事件 $AB$（即 $A \cap B$）表示“第一个为奇数且第二个为奇数”。
样本点数为 $3 \times 3 = 9$。
$$ P(AB) = \frac{9}{36} = \frac{1}{4} $$
对比乘积：
$$ P(A)P(B) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} $$
因为 $P(AB) = P(A)P(B)$，所以 **$A$ 与 $B$ 独立**。

**验证 A 与 C：**
事件 $AC$ 表示“第一个为奇数且点数和为奇数”。
若第一个点数 $i$ 为奇数，要使 $i+j$ 为奇数，则第二个点数 $j$ 必须为偶数。
$i \in \{1, 3, 5\}$，$j \in \{2, 4, 6\}$，样本点数为 $3 \times 3 = 9$。
$$ P(AC) = \frac{9}{36} = \frac{1}{4} $$
对比乘积：
$$ P(A)P(C) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} $$
因为 $P(AC) = P(A)P(C)$，所以 **$A$ 与 $C$ 独立**。

**验证 B 与 C：**
事件 $BC$ 表示“第二个为奇数且点数和为奇数”。
若第二个点数 $j$ 为奇数，要使 $i+j$ 为奇数，则第一个点数 $i$ 必须为偶数。
$i \in \{2, 4, 6\}$，$j \in \{1, 3, 5\}$，样本点数为 $3 \times 3 = 9$。
$$ P(BC) = \frac{9}{36} = \frac{1}{4} $$
对比乘积：
$$ P(B)P(C) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} $$
因为 $P(BC) = P(B)P(C)$，所以 **$B$ 与 $C$ 独立**。

综上，事件 $A, B, C$ **两两独立**。

**3. 验证相互独立性**

事件 $ABC$（即 $A \cap B \cap C$）表示“第一个为奇数，且第二个为奇数，且点数和为奇数”。
若第一个点数 $i$ 为奇数，第二个点数 $j$ 为奇数，则它们的和 $i+j$ 必然为**偶数**。
因此，不可能存在点数和为奇数的情况。
该事件为空集：$ABC = \emptyset$。
$$ P(ABC) = 0 $$

计算三个事件概率的乘积：
$$ P(A)P(B)P(C) = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} = \frac{1}{8} $$

显然：
$$ P(ABC) = 0 \neq \frac{1}{8} = P(A)P(B)P(C) $$

因此，事件 $A, B, C$ **不相互独立**。

### 答案

经计算验证：
1.  $P(AB) = P(A)P(B) = \frac{1}{4}$
2.  $P(AC) = P(A)P(C) = \frac{1}{4}$
3.  $P(BC) = P(B)P(C) = \frac{1}{4}$
4.  $P(ABC) = 0 \neq P(A)P(B)P(C) = \frac{1}{8}$

故事件 $A, B, C$ 两两独立，但不相互独立。

### 知识延伸

1.  **两两独立与相互独立的关系**：
    *   **相互独立 $\implies$ 两两独立**：若 $n$ 个事件相互独立，则其中任意两个事件必然独立。
    *   **两两独立 $\nRightarrow$ 相互独立**：本题即为最经典的伯恩斯坦（Bernstein）反例，说明两两独立比相互独立的条件要弱。

2.  **相互独立的定义**：
    对于 $n$ 个事件 $A_1, A_2, \dots, A_n$，它们相互独立的充要条件是：对于任意 $k$ ($2 \le k \le n$) 和任意下标 $1 \le i_1 < i_2 < \dots < i_k \le n$，均满足：
    $$ P(A_{i_1} A_{i_2} \dots A_{i_k}) = P(A_{i_1}) P(A_{i_2}) \dots P(A_{i_k}) $$
    仅验证 $P(A_1 \dots A_n) = \prod P(A_i)$ 是不够的，必须验证所有子组合。

3.  **易错点**：
    在判断三个事件是否独立时，初学者常误以为只要两两独立就足够，或者只验证三个事件的积事件概率是否等于概率之积。必须同时满足所有子集的乘积性质才构成相互独立。

---

## 例题0.10

### 题目

考虑一个概率空间。记 $A$ ， $B$ ， $\mathit{C}$ 是三个事件。 （1）举例说明事件 $A$ 与（ $B$ 与 $\mathrm{C}$ 的和事件）独立，但 $A$ 与 $\mathit{B}$ 不独立，与 $\mathit{C}$ 也不独立。 (2）设 $A , \ B$ 两个事件的概率均大于3/4。证明 $A$ 与 $B$ 的积事件的概率大于 $1 / 2$ .

### 解答

**Reviewing Example Solution**

I'm now revisiting the construction of the counterexample, focusing on clarity and rigor. Ensuring the calculated probabilities align with the independence criteria for $A$ and $B \cup C$ remains paramount. I've also identified potential areas for simplification in the presentation, aiming for conciseness without sacrificing accuracy. The proof of the second part, using the inclusion-exclusion principle, seems straightforward.


### 题目分析

本题主要考查概率论基础中的**事件独立性**定义以及**概率加法公式**（或容斥原理）的应用。
1.  题目中涉及的符号 $A, B, C$ 为同一概率空间中的事件。
2.  第一问要求构造反例，旨在考察对独立性定义 $P(XY) = P(X)P(Y)$ 的深刻理解，说明事件与“和事件”的独立性并不蕴含与“子事件”的独立性。
3.  第二问考察概率的基本不等式性质，即如何通过边缘概率推导积事件（交事件）概率的下界，这与邦费罗尼不等式（Bonferroni inequalities）有关。

---

## 例题0.11

### 题目

现有三个袋子，编号为 $i=1 , 2 , 3$ ，已知第 $i$ 个袋子有 $b_{i}$ 个黑球和 $w_{i}$ 个白球，现在随机的选择一个袋子，然后在这个袋子中随机的选择一个球，选中黑球的概率是多少？若已知选取的是黑球，如何推断黑球的袋来源？

### 解答

**Examining Problem's Structure**

I'm currently focused on dissecting the problem's structure. I've broken it down into key stages and components. I'm focusing on identifying the core concepts such as the total probability formula and Bayes' theorem. I am now working on defining the symbols and formulating the goals, specifically calculating P(Black) and P(Bag i | Black).


### 题目分析

本题描述了一个典型的两阶段随机试验模型。第一阶段是从三个袋子中随机选择一个，第二阶段是从选中的袋子中随机抽取一个球。题目中涉及的数学符号包括袋子编号 $i=1,2,3$，以及第 $i$ 个袋子中黑球数量 $b_i$ 和白球数量 $w_i$。

本题考查的核心概念是概率论中的**全概率公式（Law of Total Probability）**和**贝叶斯公式（Bayes' Theorem）**。

需要求解的目标有两个：
1.  计算选中黑球的边缘概率（无条件概率）。
2.  在已知选中黑球的条件下，推断该球来自特定袋子的后验概率。

### 解题过程

**1. 定义事件与参数**

设 $A_i$ 表示“选中第 $i$ 个袋子”的事件，其中 $i = 1, 2, 3$。
设 $B$ 表示“选中黑球”的事件。

根据题目描述，“随机的选择一个袋子”意味着选择每个袋子的概率是相等的。因此，事件 $A_1, A_2, A_3$ 构成了样本空间的一个划分（完备事件组），且先验概率为：
$$P(A_1) = P(A_2) = P(A_3) = \frac{1}{3}$$

在已知选中第 $i$ 个袋子的情况下，选中黑球的条件概率 $P(B|A_i)$ 取决于该袋子中黑球数 $b_i$ 与总球数 $b_i + w_i$ 的比例：
$$P(B|A_i) = \frac{b_i}{b_i + w_i}, \quad i = 1, 2, 3$$

**2. 求解选中黑球的概率**

根据**全概率公式**，事件 $B$ 发生的概率等于在各个划分 $A_i$ 下 $B$ 发生概率的加权和：
$$P(B) = \sum_{i=1}^{3} P(A_i) P(B|A_i)$$

将已知数值代入公式：
$$P(B) = \frac{1}{3} \cdot \frac{b_1}{b_1 + w_1} + \frac{1}{3} \cdot \frac{b_2}{b_2 + w_2} + \frac{1}{3} \cdot \frac{b_3}{b_3 + w_3}$$

提取公因子 $\frac{1}{3}$，可得：
$$P(B) = \frac{1}{3} \sum_{i=1}^{3} \frac{b_i}{b_i + w_i}$$

**3. 推断黑球的袋来源**

题目要求“若已知选取的是黑球，如何推断黑球的袋来源”，即求解在事件 $B$ 发生的条件下，事件 $A_k$ ($k \in \{1, 2, 3\}$) 发生的条件概率 $P(A_k|B)$。

根据**贝叶斯公式**：
$$P(A_k|B) = \frac{P(A_k) P(B|A_k)}{P(B)}$$

将分子代入具体表达式：
$$P(A_k) P(B|A_k) = \frac{1}{3} \cdot \frac{b_k}{b_k + w_k}$$

将分母 $P(B)$ 代入（利用上一步的结果）：
$$P(B) = \frac{1}{3} \sum_{i=1}^{3} \frac{b_i}{b_i + w_i}$$

因此，后验概率为：
$$P(A_k|B) = \frac{\frac{1}{3} \cdot \frac{b_k}{b_k + w_k}}{\frac{1}{3} \sum_{i=1}^{3} \frac{b_i}{b_i + w_i}}$$

消去分子分母中的 $\frac{1}{3}$，得到最终表达式：
$$P(A_k|B) = \frac{\frac{b_k}{b_k + w_k}}{\sum_{i=1}^{3} \frac{b_i}{b_i + w_i}}$$

### 答案

1.  选中黑球的概率为：
$$P(B) = \frac{1}{3} \sum_{i=1}^{3} \frac{b_i}{b_i + w_i}$$

2.  已知选取的是黑球，该球来自第 $k$ 个袋子 ($k=1,2,3$) 的概率为：
$$P(A_k|B) = \frac{\frac{b_k}{b_k + w_k}}{\sum_{i=1}^{3} \frac{b_i}{b_i + w_i}}$$

### 知识延伸

**1. 核心定理总结**
*   **全概率公式**：用于计算复杂事件的概率，将其分解为若干个互不相容的简单情形下的概率之和。公式为 $P(B) = \sum P(A_i)P(B|A_i)$。
*   **贝叶斯公式**：用于“由果索因”，即在观察到结果（事件 $B$）后，更新对原因（事件 $A_i$）的概率估计（后验概率）。公式为 $P(A_k|B) = \frac{P(A_k)P(B|A_k)}{\sum P(A_i)P(B|A_i)}$。

**2. 解题技巧与易错点**
*   **完备性检查**：在使用全概率公式时，必须确保 $A_1, \dots, A_n$ 构成了样本空间的一个划分，即它们互不相容且和为全集 ($\sum P(A_i) = 1$)。
*   **先验概率**：题目中“随机选择一个袋子”隐含了均匀分布假设 ($P(A_i) = 1/n$)。如果题目改为“以概率 $p_i$ 选择第 $i$ 个袋子”，则公式中的 $1/3$ 需替换为对应的 $p_i$，且不能简单消去。
*   **符号区分**：注意区分 $P(B|A)$（似然度/条件概率）与 $P(A|B)$（后验概率）。前者是已知袋子找球的概率，后者是已知球找袋子的概率。

---

## 例题0.12

### 题目

Alice 向 Bob 发送字符 0 或 1，发送 0 的概率为 p0，传输过程中字符有概率 pe 被翻转，求当 Bob 收到的字符为1 时，Alice 发送字符为 0 的概率。

### 解答

**Reviewing Solution Steps**

I'm now deep into the "解题过程" section, meticulously writing out each step in a precise manner. I've switched to a more direct notation, using standard probability symbols, which should significantly improve readability. I've also incorporated intermediate steps to break down the Bayes' Theorem application, and I'm very aware of the need to ensure LaTeX formatting throughout for professional presentation.


### 题目分析
本题描述了一个经典的二进制对称信道（Binary Symmetric Channel, BSC）通信模型。题目中给出的符号 $p_0$ 代表发送端发送字符 0 的先验概率，$p_e$ 代表传输过程中的误码率（Error Probability），即字符发生翻转的条件概率。本题考查的核心概念是**条件概率**、**全概率公式**以及**贝叶斯公式（Bayes' Theorem）**。解题目标是求解在接收端收到字符 1 的条件下，发送端实际发送字符为 0 的后验概率。

### 解题过程

**1. 定义随机事件**

设随机变量 $X$ 表示 Alice 发送的字符，随机变量 $Y$ 表示 Bob 收到的字符。
根据题目描述，定义以下事件及其概率：
*   Alice 发送字符 0：$P(X=0) = p_0$
*   Alice 发送字符 1：$P(X=1) = 1 - p_0$

**2. 分析条件概率（信道特性）**

题目指出传输过程中字符有概率 $p_e$ 被翻转。这意味着：
*   发送 0 翻转为 1（收到 1）：$P(Y=1 | X=0) = p_e$
*   发送 1 翻转为 0（收到 0）：$P(Y=0 | X=1) = p_e$

由此可推导出未发生翻转（正确传输）的概率：
*   发送 1 收到 1：$P(Y=1 | X=1) = 1 - P(Y=0 | X=1) = 1 - p_e$

**3. 明确求解目标**

我们需要求解的是当 Bob 收到的字符为 1 时，Alice 发送字符为 0 的概率，即条件概率 $P(X=0 | Y=1)$。

**4. 应用贝叶斯公式**

根据贝叶斯公式：
$$P(X=0 | Y=1) = \frac{P(Y=1 | X=0)P(X=0)}{P(Y=1)}$$

**5. 利用全概率公式计算分母**

分母 $P(Y=1)$ 表示 Bob 收到字符 1 的总概率。根据全概率公式，这由两种情况组成：
1.  Alice 发送 0，且发生了翻转。
2.  Alice 发送 1，且未发生翻转。

公式如下：
$$P(Y=1) = P(Y=1 | X=0)P(X=0) + P(Y=1 | X=1)P(X=1)$$

代入已知变量：
$$P(Y=1) = p_e \cdot p_0 + (1 - p_e) \cdot (1 - p_0)$$

**6. 计算最终结果**

将分子和分母代回贝叶斯公式中：
分子为：
$$P(Y=1 | X=0)P(X=0) = p_e \cdot p_0$$

故最终表达式为：
$$P(X=0 | Y=1) = \frac{p_0 p_e}{p_0 p_e + (1 - p_0)(1 - p_e)}$$

### 答案
$$P(\text{发送0} | \text{收到1}) = \frac{p_0 p_e}{p_0 p_e + (1 - p_0)(1 - p_e)}$$

### 知识延伸
1.  **核心定理**：本题是**贝叶斯公式**的典型应用场景，即由“果”（收到1）推导“因”（发送0）的概率。同时结合了**全概率公式**来计算边缘概率。
2.  **重要公式**：
    *   贝叶斯公式：$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
    *   全概率公式：$P(B) = \sum_{i} P(B|A_i)P(A_i)$
3.  **模型背景**：在信息论中，这是最基础的**二进制对称信道（BSC）**模型。该结果反映了在有噪声干扰的情况下，接收端对发送端信号的后验判断。
4.  **易错点**：
    *   混淆 $P(Y=1|X=0)$（误码率）与 $P(X=0|Y=1)$（后验概率）。
    *   忘记计算发送 1 的概率 $P(X=1) = 1 - p_0$。
    *   在计算全概率分母时，忽略了“发送1且正确传输”这一项，误以为 $P(Y=1)$ 仅由 $p_e$ 决定。

---

## 例题0.13

### 题目

老师在三个信封中分别装入三道数学测试题，一道是线性代数的，而另两道是随机过程的。为 方便起见，学生甲随机选定一个信封，不妨记作A。然后，助教打开剩下两个信封中的一个，记作B。拆开后发 现其中是随机过程的试题。此时助教问同学甲：要不要选做另一个信封（不妨记作 C）的题目？ 3 0.2. 随机变量及其分布函数

### 解答

**Formulating the Solution**

The problem has been modeled as a conditional probability problem, analogous to the Monty Hall problem. I'm focusing on defining events precisely and applying Bayes' theorem to calculate the probabilities, making key assumptions about the TA's actions. I've also incorporated the understanding that the question's intention is to test the switching strategy for the student.


### 题目分析

本题是一个经典的**条件概率**问题，其数学模型等同于著名的**三门问题（Monty Hall Problem）**。
题目中包含三个基本对象（信封），其中一个是唯一的（线性代数题，记为 $L$），另外两个是相同的（随机过程题，记为 $S$）。
核心考察点是**贝叶斯公式（Bayes' Theorem）**的应用，以及在获取新信息（助教打开信封 B 并展示 $S$）后，对先验概率进行修正得到后验概率的过程。
我们需要求解的目标是：在已知助教打开信封 B 且其中为随机过程试题的条件下，信封 A 和信封 C 中含有线性代数试题的后验概率，并据此判断是否应当交换。

### 解题过程

**1. 定义事件与先验概率**

设事件 $L_A, L_B, L_C$ 分别表示线性代数试题（$L$）在信封 A、B、C 中。
由于试题是随机装入的，且甲随机选择了 A，因此先验概率为：
$$P(L_A) = P(L_B) = P(L_C) = \frac{1}{3}$$

设事件 $O_B$ 表示“助教打开了信封 B，并展示了随机过程试题”。

**2. 确定条件概率（似然度）**

我们需要分析在 $L$ 位于不同信封的情况下，助教做出 $O_B$ 这一行为的概率。基于三门问题的标准设定（助教知道信封内容，且必须打开一个未被选中且不含 $L$ 的信封）：

*   **情形 1：若 $L$ 在 A 中 ($L_A$ 发生)**
    此时 B 和 C 中都是随机过程试题 ($S$)。助教可以在 B 和 C 中随机选择一个打开。
    $$P(O_B | L_A) = \frac{1}{2}$$

*   **情形 2：若 $L$ 在 B 中 ($L_B$ 发生)**
    此时助教不能打开 B（因为 B 中是 $L$，助教必须展示 $S$）。因此助教打开 B 的概率为 0。
    $$P(O_B | L_B) = 0$$

*   **情形 3：若 $L$ 在 C 中 ($L_C$ 发生)**
    此时 A 中是 $S$（已被甲选），C 中是 $L$。助教剩下的选择只有 B 和 C，但不能打开 C（因为含 $L$）。因此助教必须打开 B。
    $$P(O_B | L_C) = 1$$

**3. 利用全概率公式计算边缘概率**

计算助教打开信封 B 的总概率 $P(O_B)$：
$$
\begin{aligned}
P(O_B) &= P(O_B | L_A)P(L_A) + P(O_B | L_B)P(L_B) + P(O_B | L_C)P(L_C) \\
&= \frac{1}{2} \times \frac{1}{3} + 0 \times \frac{1}{3} + 1 \times \frac{1}{3} \\
&= \frac{1}{6} + 0 + \frac{1}{3} \\
&= \frac{1}{2}
\end{aligned}
$$

**4. 利用贝叶斯公式计算后验概率**

我们需要比较甲当前手中的信封 A 含有 $L$ 的概率 $P(L_A | O_B)$ 与另一信封 C 含有 $L$ 的概率 $P(L_C | O_B)$。

*   **信封 A 含有线性代数题的后验概率：**
$$
P(L_A | O_B) = \frac{P(O_B | L_A)P(L_A)}{P(O_B)} = \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}
$$

*   **信封 C 含有线性代数题的后验概率：**
$$
P(L_C | O_B) = \frac{P(O_B | L_C)P(L_C)}{P(O_B)} = \frac{1 \times \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}
$$

**5. 结果分析**

如果不交换，甲获得线性代数题的概率为 $\frac{1}{3}$。
如果交换选 C，甲获得线性代数题的概率为 $\frac{2}{3}$。

### 答案

根据计算结果：
$$P(L_C | O_B) = \frac{2}{3} > P(L_A | O_B) = \frac{1}{3}$$
因此，**甲应当选择交换信封 C**，这样选中线性代数试题的概率将翻倍。

### 知识延伸

1.  **核心定理**：本题是**贝叶斯公式 (Bayes' Formula)** $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ 的典型应用。它展示了如何根据新观测到的信息（助教排除了一个错误选项）来更新对假设的置信度。
2.  **直观理解**：
    *   甲最初选择 A 时，$L$ 在 A 中的概率是 $1/3$，而在 {B, C} 集合中的概率是 $2/3$。
    *   助教打开 B 这一行为并没有改变 $L$ 在 A 中的概率（因为无论 $L$ 在哪里，助教总能打开一个空信封），但助教的行为将 {B, C} 集合中原本 $2/3$ 的概率全部“浓缩”到了 C 身上（因为 B 被排除了）。
3.  **易错点**：
    *   **均等谬误**：很多人直觉上认为剩下两个信封，概率应该是 $1/2$ 对 $1/2$。这是错误的，因为它忽略了助教选择打开 B 并不是随机的，而是受限于“不能打开有 $L$ 的信封”这一规则，从而引入了信息量。
    *   **题目条件**：如果助教是**随机**打开 B，且**碰巧**发现了随机过程题，那么交换与否概率确实都是 $1/2$。但在标准的数学例题（如本题语境）中，通常默认助教知情且有目的性地排除干扰项。

---

## 例题0.14

### 题目

老师准备从五个学生A、B、C、D、E中选三个代表班级参加随机过程课程竞赛。 (1) $\mathrm{A}$ 被选中的概率是多少？ (2）老师公布了其中一个参赛同学：E。此时 $\mathrm{A}$ 被选中的概率是多少？ （3）在老师未公布之前， $\mathrm{A}$ 向助教打听自己有无入选。助教按照纪律要求不能告知 $\mathrm{A}$ 是否入选，但告知日入选了。此时， $\mathrm{A}$ 入选的概率是多少？ -S0.2 随机变量及其分布函数 0.2.1 基本概念 【定义0.15】随机变量 $\mathit{X}$ 不是（自）变量，而是样本空间 $\Omega$ 到实数集R的...

### 解答

**Problem's Scope Narrowed**

I've clarified the core distinction between the teacher's announcement (Q2) and the TA's statement (Q3). I've meticulously refined the conditional probability calculations for both scenarios. I'm now formulating the final output, focusing on precision, particularly the crucial assumption about the TA's random selection process.


### 题目分析

本题主要考查**古典概型**、**条件概率**以及**贝叶斯公式**的应用。
1.  题目涉及从有限集合中进行无放回抽样（组合问题）。
2.  第（2）问考察当样本空间缩减时的条件概率。
3.  第（3）问是经典的“三囚犯问题”或“蒙提霍尔问题”的变体，考察在特定交互协议下（助教不能告知 A 的情况，只能告知其他人），信息的获取如何影响后验概率。关键在于区分“E 入选”这一事实与“助教告知 E 入选”这一行为的区别。

---

## 例题0.16

### 题目

掷一枚均匀的骰子，定义 X 为骰子面上的点数。此时，样本空间 Ω = {1，2，3，4，5，6}。 回答如下问题。 （1）取A = {1，3，5}。F0 = {Ω，A，∅} 是事件类吗？F1 = {Ω，A， ¯A，∅} 是事件类吗？F = 2 Ω 是 事件类吗？ （2）若我们考察的概率模型以 F1 为事件类，X 是随机变量吗？若我们考察的概率模型以 F 为事件类， X 是随机变量吗？ （3）定义 Y = X mod 2，讨论上一个问题。 注：以 F1 为事件类，X 不是随机变量。因为存在 x0 使得 {ω ∈ Ω|X(ω) ≤ x0} /∈ F 1。如 x0 = 2 ，{X ≤ 2} = {1，2} /∈ F1。以 F 为事件类，X 是随机变量。而不论以 F1 还是 F 为事件类，Y 都是随机变量。另外也 要注意，上面的讨论与骰子是否均匀关系不大。换句话讲，一个从样本空间Ω 到实数轴R 的映射X 是否是随机 变量，仅与事件类 F 的定义有关。 设 X 是一个随机变量。由于 {X ≤ x} 是事件，我们可以定义 FX(x) = P(X ≤ x), x ∈ R, 称之为随机变量 X 的累积分布函数（cumulative distribution function ，CDF） 。当x 在 (−∞，∞) 变化时，累积分布函数反映 了随机变量取值在实数轴上的分布情况。如果X 取值有限或者可列，则称为离散型随机变量，我们可以定义概 率质量函数 PX(x) = P(X = x)，x ∈ X （R 的有限子集或可列子集）满足 PX(xi) ⩾ 0 且 P i PX(xi) = 1 。如 果 FX(x) 连续且几乎处处可微，我们称 X 为连续型随机变量，并定义概率密度函数 fX(x)，x ∈ X （R 上的区 间 X）满足 fX(x) ⩾ 0 和 R R fX(x)dx = 1 。图2、图3与图4分别给出了离散型随机变量的 PMF、连续型随机变 量的 PDF 与一般随机变量的 CDF 的示意图。 x ) (x P 1x 2x nxo 图 2: 离散型随机变量的 PMF (又称分布律)，“谱线” 4 0.2. 随机变量及其分布函数 o ) (x f x1x 2x 图3: 连续型随机变量的PDF， “谱密度” 一个随机变量 X 的累积分布函数 FX(x) ≜ P(X ⩽ x)，x ∈ R 满足如下性质： 1. 对离散型随机变量X，FX(x) = P xi⩽x PX(xi) 是阶梯函数，其中PX(x)，x ∈ X 是X 的概率质量函数。 2. 对连续型随机变量X，FX(x) = R y⩽x fX(y)dy 是连续函数，其中fX(x)，x ∈ X 是X 的是概率密度函 数。 3. FX(x) 是单调递增（未必严格单调）函数，且 lim x→−∞ FX(x) = 0 ， lim x→+∞ FX(x) = 1 。 4. FX(x) 的间断点至多可列个；若在 x1 处有跳跃，则跳跃的高度恰好是概率 PX(x1)。 o ( ) XF x x  1x 1( ) XP x ^ 图4: 随机变量的 CDF 以上讨论的只限于一个随机变量的情况，如果随机试验的结果需要同时考虑两个或两个以上的随机变量的 情况，则可以定义复随机变量，随机向量，随机序列，随机过程等。 0.2.2 常见的随机变量 常见的离散随机变量的概率质量函数分述如下，其中 0 < p < 1，λ > 0 是参数。 1. 两点分布：随机变量X 取0 与 1 两个值，其分布律是PX(1) = p，PX(0) = 1 − p。 2. 二项分布：设n 是正整数，记B(n; p) 为二项分布，其分布律是PX(m) =  n m  pm(1 − p)n−m，0 ≤ m ≤ n。 3. 几何分布：随机变量X 取正整数，其分布律是 PX(m) = p(1 − p)m−1，m ≥ 1。 4. 泊松分布：随机变量X 取非负整数，其分布律是 PX(n) = λn exp(−λ) n! ，n ≥ 0。 常见的连续随机变量的概率密度函数分述如下，其中 λ > 0、σ > 0、µ 是参数。 1. 指数分布：随机变量X 的密度函数 fX(x) = λ exp(−λx)，x ≥ 0。 2. 高斯分布：随机变量X 的密度函数是 fX(x) = 1√ 2πσ exp(−(x−µ)2 2σ2 )，x ∈ R。 3. 均匀分布：给定 a > 0。随机变量X 的密度函数是fX(x) = 1 a ，0 ≤ x ≤ a。 注：密度函数的定义域与累积分布函数的定义域一致，均是整个实数轴。在给出密度函数的时候，通常只给 出函数的支撑集（support set), 即函数值不为 0 的自变量所构成的子集合。在支撑集之外，函数值默认为 0。 我们称 FX

### 解答

**Evaluating Solution Progress**

I've finalized the problem decomposition and solution strategy. The analysis of $\sigma$-algebras is complete, along with measurability checks for both random variables, $X$ and $Y$. I'm now drafting the content, meticulously incorporating LaTeX, and refining the tone to meet the given constraints. A final review will ensure the output structure is adhered to and a concise "知识延伸" section can be formulated.


### 题目分析

本题基于掷骰子的随机试验，主要考查概率论公理化体系中的两个核心概念：**$\sigma$-代数（事件类）的定义**与**随机变量的本质（可测函数）**。
1.  **样本空间**：$\Omega = \{1, 2, 3, 4, 5, 6\}$。
2.  **核心考点**：
    *   验证给定的集合族是否构成 $\sigma$-代数（事件类）。
    *   判断从样本空间到实数轴的映射是否构成特定概率空间上的随机变量。
3.  **求解目标**：
    *   判断 $\mathcal{F}_0, \mathcal{F}_1, \mathcal{F}$ 是否为事件类。
    *   判断 $X$ 在 $\mathcal{F}_1$ 和 $\mathcal{F}$ 下是否为随机变量。
    *   判断 $Y = X \pmod 2$ 在 $\mathcal{F}_1$ 和 $\mathcal{F}$ 下是否为随机变量。

---

## 例题0.18

### 题目

投掷两个均匀的骰子，随机变量 $X_{i} , \, \, i \in\{1 , \, \, 2 \}$ 表示第 $i$ 个骰子的点数， $X_{i} \in\{1 , \ 2 , \ 3 , \ 4 , \ 5 \}$ ， 6}。令 $X=X_{1}+X_{2}$ ，求： (1) $P ( X=2 | X_{1}=6 )$ : (2) $P ( X_{1}=2 | X_{2}=2 )$ : (3) $P ( X_{1}=1 | X=2 )$ .

### 解答

**Analyzing Conditional Probabilities**

I'm currently focused on the problem's core, meticulously defining the variables and sample spaces. I'm carefully formulating each step, ensuring the logic and mathematical notation are flawless. The application of conditional probability and independence is being rigorously tested, with the aim of presenting a complete, mathematically sound solution. I'm also ensuring the final answer adheres to all stated constraints.


### 题目分析
本题考查概率论中的基础概念，主要涉及**样本空间**的构建、**随机变量**的定义、**事件的独立性**以及**条件概率**的计算。题目设定为投掷两个均匀骰子，这是一个典型的离散型概率模型。核心在于理解条件概率的定义 $P(A|B) = \frac{P(AB)}{P(B)}$，以及如何根据已知条件缩小样本空间来求解概率。

### 解题过程

首先定义样本空间 $\Omega$。投掷两个均匀骰子，每个骰子的结果 $X_i \in \{1, 2, 3, 4, 5, 6\}$。
样本空间由所有可能的有序对 $(x_1, x_2)$ 组成，总数为：
$$|\Omega| = 6 \times 6 = 36$$
由于骰子是均匀的，每个基本事件发生的概率为 $\frac{1}{36}$。
随机变量 $X_1$ 和 $X_2$ 相互独立。

#### （1）求解 $P(X = 2 |X_1 = 6)$

根据条件概率公式：
$$P(X = 2 |X_1 = 6) = \frac{P(X = 2 \cap X_1 = 6)}{P(X_1 = 6)}$$

**分母分析**：
$X_1 = 6$ 的事件包含 6 个基本事件：$\{(6,1), (6,2), (6,3), (6,4), (6,5), (6,6)\}$。
$$P(X_1 = 6) = \frac{6}{36} = \frac{1}{6}$$

**分子分析**：
事件 $\{X = 2 \cap X_1 = 6\}$ 意味着 $X_1 + X_2 = 2$ 且 $X_1 = 6$。
代入 $X_1 = 6$，得到 $6 + X_2 = 2$，即 $X_2 = -4$。
然而 $X_2 \in \{1, 2, 3, 4, 5, 6\}$，不可能取值为 -4。
因此，该事件为空集，概率为 0。
$$P(X = 2 \cap X_1 = 6) = 0$$

**计算结果**：
$$P(X = 2 |X_1 = 6) = \frac{0}{1/6} = 0$$

---

## 例题0.19

### 题目

甲乙两人网聊。甲向乙随机发送一个数字 $X \in\{0 , ~ 1 , ~ 2 \}$ ，概率依次为 $\frac{1} {2} , ~ \frac{1} {4} , ~ \frac{1} {4}$ 。 $\mathbf{z}$ 收到数字 $Y \in\{0 , ~ 1 , ~ 2 \}$ 。但传输过程中有可能出错。设错误模型是 $Y=X+Z \bmod3$ ，其中 $Z \in\{0 , ~ 1 , ~ 2 \}$ 的分布律是 $\frac{7} {8} , \ \frac{1} {1 6} , \ \frac{1} {1 6}$ 。讨论乙如何根据接收的 $\mathit{Y}$ 推断发送...

### 解答

**Outlining the Solution**

\n\n

I've constructed a basic outline: problem analysis, a step-by-step solution using Bayes' theorem, the final answer in a concise format, and a section for extended knowledge. I'm focusing on the critical probabilities and how the modulo arithmetic plays a role.  The calculations seem sound at this stage.


### 题目分析

本题描述了一个离散信道通信模型，其中输入信号 $X$ 和噪声 $Z$ 相互独立，输出信号 $Y$ 由 $X$ 和 $Z$ 模 3 加法生成。题目给出了 $X$ 的先验分布和 $Z$ 的分布律。考查的核心概念是**贝叶斯推断（Bayesian Inference）**和**最大后验概率估计（Maximum A Posteriori estimation, MAP）**。求解的目标是找到一个决策规则（或准则），使得乙在观测到 $Y$ 的具体取值后，推断出的 $\hat{X}$ 犯错的概率最小，即最大化后验概率 $P(X=x|Y=y)$。

### 解题过程

乙根据接收到的 $Y$ 推断 $X$，最佳准则是选择使得后验概率 $P(X=x|Y=y)$ 最大的那个 $x$ 值。这就是最大后验概率（MAP）准则。

根据贝叶斯公式，后验概率为：
$$P(X=x | Y=y) = \frac{P(Y=y | X=x) P(X=x)}{P(Y=y)}$$

对于给定的观测值 $y$，分母 $P(Y=y)$ 是一个常数（归一化因子），不依赖于 $x$。因此，最大化后验概率等价于最大化分子，即联合概率：
$$\hat{x}_{MAP}(y) = \arg\max_{x \in \{0,1,2\}} P(Y=y | X=x) P(X=x)$$

已知信道模型为 $Y = (X + Z) \pmod 3$，这意味着 $Z = (Y - X) \pmod 3$。由于 $X$ 和 $Z$ 独立，条件概率（似然函数）为：
$$P(Y=y | X=x) = P(Z = (y-x) \pmod 3)$$

已知先验概率 $P(X)$ 分布为：
$$P(X=0)=\frac{1}{2}, \quad P(X=1)=\frac{1}{4}, \quad P(X=2)=\frac{1}{4}$$

已知噪声 $Z$ 分布为：
$$P(Z=0)=\frac{7}{8}, \quad P(Z=1)=\frac{1}{16}, \quad P(Z=2)=\frac{1}{16}$$

下面针对乙收到的每一个可能的 $Y$ 值（0, 1, 2），分别计算不同 $X$ 对应的联合概率 $S(x, y) = P(Y=y | X=x) P(X=x)$ 并进行比较。

**情形 1：乙收到 $Y=0$**

*   假设 $X=0$：需要 $Z = (0-0) \pmod 3 = 0$。
    $$S(0, 0) = P(Z=0)P(X=0) = \frac{7}{8} \times \frac{1}{2} = \frac{7}{16}$$
*   假设 $X=1$：需要 $Z = (0-1) \pmod 3 = 2$。
    $$S(1, 0) = P(Z=2)P(X=1) = \frac{1}{16} \times \frac{1}{4} = \frac{1}{64}$$
*   假设 $X=2$：需要 $Z = (0-2) \pmod 3 = 1$。
    $$S(2, 0) = P(Z=1)P(X=2) = \frac{1}{16} \times \frac{1}{4} = \frac{1}{64}$$

比较可知：$\frac{7}{16} > \frac{1}{64}$。因此，当 $Y=0$ 时，推断 $\hat{X}=0$。

**情形 2：乙收到 $Y=1$**

*   假设 $X=0$：需要 $Z = (1-0) \pmod 3 = 1$。
    $$S(0, 1) = P(Z=1)P(X=0) = \frac{1}{16} \times \frac{1}{2} = \frac{1}{32}$$
*   假设 $X=1$：需要 $Z = (1-1) \pmod 3 = 0$。
    $$S(1, 1) = P(Z=0)P(X=1) = \frac{7}{8} \times \frac{1}{4} = \frac{7}{32}$$
*   假设 $X=2$：需要 $Z = (1-2) \pmod 3 = 2$。
    $$S(2, 1) = P(Z=2)P(X=2) = \frac{1}{16} \times \frac{1}{4} = \frac{1}{64}$$

比较可知：$\frac{7}{32} > \frac{1}{32} > \frac{1}{64}$。因此，当 $Y=1$ 时，推断 $\hat{X}=1$。

**情形 3：乙收到 $Y=2$**

*   假设 $X=0$：需要 $Z = (2-0) \pmod 3 = 2$。
    $$S(0, 2) = P(Z=2)P(X=0) = \frac{1}{16} \times \frac{1}{2} = \frac{1}{32}$$
*   假设 $X=1$：需要 $Z = (2-1) \pmod 3 = 1$。
    $$S(1, 2) = P(Z=1)P(X=1) = \frac{1}{16} \times \frac{1}{4} = \frac{1}{64}$$
*   假设 $X=2$：需要 $Z = (2-2) \pmod 3 = 0$。
    $$S(2, 2) = P(Z=0)P(X=2) = \frac{7}{8} \times \frac{1}{4} = \frac{7}{32}$$

比较可知：$\frac{7}{32} > \frac{1}{32} > \frac{1}{64}$。因此，当 $Y=2$ 时，推断 $\hat{X}=2$。

### 答案

乙推断 $X$ 的最佳准则是**最大后验概率准则（MAP）**。具体的判决规则为：
$$
\hat{X} = Y
$$
即：
- 若收到 $Y=0$，推断 $X=0$；
- 若收到 $Y=1$，推断 $X=1$；
- 若收到 $Y=2$，推断 $X=2$。

### 知识延伸

1.  **最大后验概率估计 (MAP) vs 最大似然估计 (ML)**：
    *   **MAP**: $\arg\max_x P(Y|X)P(X)$。考虑了先验概率 $P(X)$，适用于先验分布已知且非均匀的情况。本题中 $P(X)$ 不均匀，必须使用 MAP。
    *   **ML**: $\arg\max_x P(Y|X)$。假设先验概率是均匀的（即所有 $X$ 出现概率相等）。如果本题中 $X$ 是均匀分布，MAP 将退化为 ML。

2.  **贝叶斯公式的核心地位**：
    在通信和检测理论中，贝叶斯公式用于将观测数据（$Y$）反演为信源状态（$X$）的概率分布。
    $$P(H|E) = \frac{P(E|H)P(H)}{P(E)}$$
    其中 $H$ 是假设（Hypothesis，这里是 $X$ 的取值），$E$ 是证据（Evidence，这里是 $Y$ 的观测值）。

3.  **易错点提示**：
    *   **忽略先验概率**：很多学生容易直接比较 $P(Y|X)$（似然度）而忽略 $P(X)$。例如在 $Y=1$ 的情况下，虽然 $X=0$ 产生 $Y=1$ 的概率很小，但如果 $X=0$ 的先验概率极大，结果可能会改变。在本题中，尽管 $P(X=0)$ 较大，但由于噪声 $Z=0$ 的概率占绝对优势（$7/8$），最终结果依然是 $\hat{X}=Y$。
    *   **模运算混淆**：在计算 $Z = Y - X$ 时，要注意负数取模的问题，例如 $0-1 \equiv 2 \pmod 3$。

---

## 例题0.20

### 题目

设 $\mathit{X}$ 是随机变量，则 $Y=X^{2} , \, \, \, Z=P_{X} ( X ) , \, \, \, L=-\operatorname{l o g} P_{X} ( X )$ 均是随机变量。他们的分布律如下所示。洼：随机变量“被函数”之后，“谱线”的数目可能减少，高度可能增加。 $$ \begin{array} {c | c c c c} {X} & {-1} & {0} & {1} & {2} \\ \hline{P_{X} ( x )} & {0 . 2} & {0 . 3} & {0 . 2} & {0 . 3} \\ {Y=X^{2}} & {0} &...

### 解答

**Reviewing Solution Steps**

I'm now revisiting the "Solution Process" section. I need to make the steps mathematically rigorous, paying close attention to notation. I will also incorporate comments explaining the rationale behind each step for clarity. Specifically, I'm focusing on the derivation of $Y$, $Z$, and $L$'s distributions, making sure the explanation is accessible to the target audience. After that, I will review the knowledge extension section.


### 题目分析

本题考查**离散型随机变量函数的分布**。给定随机变量 $X$ 的分布律，要求推导其变换后的随机变量 $Y = X^2$、$Z = P_X(X)$ 以及 $L = -\log P_X(X)$ 的分布律。

核心概念涉及：
1.  **随机变量函数的分布**：对于离散型随机变量 $X$ 和函数 $g(\cdot)$，若 $W = g(X)$，则 $P(W=w) = \sum_{x: g(x)=w} P(X=x)$。即寻找所有映射到 $w$ 的 $x$ 值，将其概率相加。
2.  **自信息量与熵**：$L = -\log P_X(X)$ 是信息论中的自信息量（Self-Information），其期望即为熵。
3.  **概率质量函数的变换**：题目中提到的“谱线数目可能减少，高度可能增加”，形象地描述了多对一映射（Many-to-one mapping）导致样本空间缩并、概率累积的过程。

---

## 例题0.21

### 题目

已知 $\mathit{X}$ 的密度函数， $\mathit{Y}$ 的密度函数。若 $\mathit{X}$ ， $Y$ 独立，求 $Z=X+Y$ 的密度函数。

### 解答

**Calculating**

\n\nI've now finalized the structure and steps for solving the problem. The core will involve deriving the convolution formula, as specific density functions aren't provided. I'm focusing on rigorous mathematical presentation and ensuring LaTeX is used correctly. The emphasis remains on a clear, direct derivation of the formula and avoiding any unnecessary introduction or explanation.


### 题目分析

本题考查的核心概念是**连续型随机变量函数的分布**，具体为**独立随机变量之和的概率密度函数**。

*   **符号识别**：
    *   $X, Y$：连续型随机变量。
    *   $f_X(x), f_Y(y)$：分别为 $X$ 和 $Y$ 的概率密度函数（PDF）。
    *   $Z = X + Y$：目标随机变量。
    *   $f_Z(z)$：待求的 $Z$ 的概率密度函数。
*   **核心考点**：
    *   利用分布函数法（CDF）求解随机变量函数的分布。
    *   二重积分的计算与求导。
    *   **卷积公式（Convolution Formula）**的推导与应用。
*   **求解目标**：推导并给出 $Z = X + Y$ 的概率密度函数 $f_Z(z)$ 的一般表达式。

### 解题过程

设 $Z = X + Y$，首先求 $Z$ 的分布函数 $F_Z(z)$。根据定义：
$$F_Z(z) = P(Z \le z) = P(X + Y \le z)$$

由于 $X$ 和 $Y$ 相互独立，其联合概率密度函数 $f(x, y)$ 等于边缘密度函数的乘积：
$$f(x, y) = f_X(x) f_Y(y)$$

根据连续型随机变量概率的计算公式，上述概率可以表示为积分区域 $D = \{(x, y) \mid x + y \le z\}$ 上的二重积分：
$$F_Z(z) = \iint_{x+y \le z} f_X(x) f_Y(y) \, dx \, dy$$

我们将积分化为累次积分。固定 $x$，则 $y$ 的范围是 $(-\infty, z-x]$；$x$ 的范围是 $(-\infty, +\infty)$：
$$F_Z(z) = \int_{-\infty}^{+\infty} \left[ \int_{-\infty}^{z-x} f_Y(y) \, dy \right] f_X(x) \, dx$$

为了求概率密度函数 $f_Z(z)$，我们需要对分布函数 $F_Z(z)$ 关于 $z$ 求导：
$$f_Z(z) = \frac{d}{dz} F_Z(z) = \frac{d}{dz} \int_{-\infty}^{+\infty} f_X(x) \left[ \int_{-\infty}^{z-x} f_Y(y) \, dy \right] dx$$

假设积分与求导运算可以交换顺序（满足莱布尼茨积分法则的正则性条件），我们将求导符号移入积分号内部：
$$f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) \left[ \frac{d}{dz} \int_{-\infty}^{z-x} f_Y(y) \, dy \right] dx$$

观察中括号内的部分，令 $G(u) = \int_{-\infty}^{u} f_Y(y) dy$，则中括号内为 $\frac{d}{dz} G(z-x)$。根据链式法则：
$$\frac{d}{dz} \int_{-\infty}^{z-x} f_Y(y) \, dy = f_Y(z-x) \cdot \frac{d}{dz}(z-x) = f_Y(z-x) \cdot 1 = f_Y(z-x)$$

将此结果代回原积分式，得到：
$$f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z-x) \, dx$$

同理，若交换 $X$ 和 $Y$ 的积分次序，也可得到对称形式：
$$f_Z(z) = \int_{-\infty}^{+\infty} f_Y(y) f_X(z-y) \, dy$$

### 答案

若 $X$ 和 $Y$ 独立，且具有密度函数 $f_X$ 和 $f_Y$，则 $Z = X + Y$ 的密度函数为：
$$f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z-x) \, dx$$
或者
$$f_Z(z) = \int_{-\infty}^{+\infty} f_Y(y) f_X(z-y) \, dy$$
此公式称为 $f_X$ 与 $f_Y$ 的**卷积**，记作 $f_Z = f_X * f_Y$。

### 知识延伸

1.  **卷积定理**：
    两个独立连续随机变量之和的密度函数是它们各自密度函数的卷积。
    公式：$f_{X+Y}(z) = (f_X * f_Y)(z)$。

2.  **离散情况**：
    若 $X, Y$ 为独立离散随机变量，其和的分布律为卷积和：
    $$P(Z=k) = \sum_{i} P(X=i) P(Y=k-i)$$

3.  **重要性质**：
    *   **交换律**：$f_X * f_Y = f_Y * f_X$。
    *   **正态分布的可加性**：若 $X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$ 且独立，则 $X+Y \sim N(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)$。直接利用卷积公式计算较为繁琐，通常利用特征函数或矩母函数证明。
    *   **伽玛分布的可加性**：若 $X \sim \Gamma(\alpha_1, \lambda), Y \sim \Gamma(\alpha_2, \lambda)$ 且独立，则 $X+Y \sim \Gamma(\alpha_1+\alpha_2, \lambda)$。

4.  **解题技巧与易错点**：
    *   **确定积分限**：这是应用卷积公式最关键的一步。虽然公式写的是 $(-\infty, +\infty)$，但实际计算时必须根据 $f_X(x)$ 和 $f_Y(z-x)$ 的非零区间确定有效的积分范围（即 $f_X(x) > 0$ 且 $f_Y(z-x) > 0$ 的交集）。
    *   **特征函数法**：对于某些复杂的密度函数，先求特征函数（或矩母函数），利用 $\varphi_{X+Y}(t) = \varphi_X(t)\varphi_Y(t)$，再进行逆变换往往更简单。

---

## 例题0.22

### 题目

随机变量 $\mathit{X}$ ，分布函数为 $F_{X} ( x )$ 。求以下随机变量的分布函数。 （1）分布函数为 $F_{X} ( x )$ 的 $n$ 个独立同分布随机变量中的最大值。 （2）分布函数为 $F_{X} ( x )$ 的 $n$ 个独立同分布随机变量中的最小值。 （3）以上(1)和(2)中定义的随机变量之差。假设 $\mathit{X}$ 的密度函数为 $f_{X} ( x )$ . -g0.3 数字特征 对收敛，则 $\mathit{X}$ 【定义0.23】设离散型随机变量 $\mathit{X}$ $$ \mathbf{E} ( X )=\sum_{k=1}^{...

### 解答

**Revising Solution Approach**

I've been meticulously revisiting the range calculation.  I'm now focusing on the integration limits and ensuring the joint PDF of the minimum and maximum are properly handled.  The boundary condition checks also require careful scrutiny to guarantee the correctness of the final CDF. The integral simplification is still under review.


### 题目分析

本题考查概率论中**顺序统计量（Order Statistics）**的分布理论。
题目给出了一个总体 $X$ 的分布函数 $F_X(x)$ 和密度函数 $f_X(x)$，以及 $n$ 个独立同分布（i.i.d.）的样本 $X_1, X_2, \dots, X_n$。
我们需要求解的目标是：
1.  最大值 $X_{(n)} = \max(X_1, \dots, X_n)$ 的分布函数。
2.  最小值 $X_{(1)} = \min(X_1, \dots, X_n)$ 的分布函数。
3.  极差 $R = X_{(n)} - X_{(1)}$ 的分布函数。

解题的核心在于利用独立性将联合概率转化为边缘概率的乘积，以及利用顺序统计量的联合密度函数求解函数的分布。

---

## 例题0.29

### 题目

概率模型中的事件 $A$ 可以定义一个二元随机变量 $\mathbb{I}_{A}$ ，称之为 $A$ 的示性变量，满足 $\mathbb{I}_{A} ( \omega)=1$ . $\omega\in A$ ，而 $\mathbb{I}_{A} ( \omega)=0$ , wf $A$ 。反之，一个二元随机变量必定是某个事件 $A$ 的示性变量。我们有 $$ P_{\mathbb{I}_{A}} ( 0 )=1-P ( A ) ; \ \ P_{\mathbb{I}_{A}} ( 1 )=P ( A )_{\circ} . $$ $$ \mathbf{E} [ \mathbb{I}_{A...

### 解答

**Defining the Variable**

I've just finished the analysis, identifying the key components of Example 0.29: the indicator variable, its distribution, and its moments. My next step involves constructing each section, starting with a formal definition of $I_A$, then moving onto a detailed step-by-step solution process, complete with LaTeX-formatted equations. I'll pay close attention to clarifying the core concepts, especially the connection between probability, expectation, and the underlying measure-theoretic principles.


### 题目分析

本题主要阐述了**示性随机变量（Indicator Random Variable）**的定义及其基本统计性质。
*   **符号识别**：
    *   $A$：样本空间 $\Omega$ 中的一个事件。
    *   $I_A$（或 $\mathbb{1}_A$）：事件 $A$ 的示性变量。
    *   $\omega$：样本点。
    *   $P(A)$：事件 $A$ 发生的概率。
    *   $E[I_A]$：示性变量的数学期望。
    *   $\sigma_{I_A}$：示性变量的标准差。
*   **核心概念**：
    *   **伯努利分布（Bernoulli Distribution）**：示性变量本质上服从参数为 $p=P(A)$ 的伯努利分布（0-1分布）。
    *   **概率与期望的转换**：题目强调了概率论中一个深刻的观点，即概率可以看作示性函数的期望，这在测度论中对应于勒贝格积分的概念。
*   **求解目标**：推导并验证示性变量 $I_A$ 的分布律、期望、方差及标准差公式。

### 解题过程

**1. 示性变量的定义与分布律**

根据定义，随机变量 $I_A$ 是一个定义在样本空间 $\Omega$ 上的函数：
$$
I_A(\omega) = \begin{cases} 
1, & \text{若 } \omega \in A \\
0, & \text{若 } \omega \notin A 
\end{cases}
$$

这是一个离散型随机变量，仅取 $0$ 和 $1$ 两个值。其概率质量函数（PMF）如下：

当 $I_A = 1$ 时，对应事件 $A$ 发生：
$$P(I_A = 1) = P(\{\omega \mid \omega \in A\}) = P(A)$$

当 $I_A = 0$ 时，对应事件 $A$ 不发生（即 $A^c$）：
$$P(I_A = 0) = P(\{\omega \mid \omega \notin A\}) = P(A^c) = 1 - P(A)$$

这表明 $I_A$ 服从参数为 $p = P(A)$ 的伯努利分布。

**2. 数学期望的推导**

根据离散型随机变量期望的定义 $E[X] = \sum x_i P(X=x_i)$：
$$
\begin{aligned}
E[I_A] &= 1 \cdot P(I_A = 1) + 0 \cdot P(I_A = 0) \\
&= 1 \cdot P(A) + 0 \cdot (1 - P(A)) \\
&= P(A)
\end{aligned}
$$
这验证了题目中提到的“概率是期望”：事件 $A$ 的概率等于其示性变量 $I_A$ 的期望。

**3. 方差与标准差的推导**

首先计算 $I_A$ 的二阶原点矩 $E[I_A^2]$。由于 $I_A$ 只能取 0 或 1，且 $0^2=0, 1^2=1$，因此 $I_A^2 = I_A$。
$$
E[I_A^2] = E[I_A] = P(A)
$$
或者通过定义计算：
$$
E[I_A^2] = 1^2 \cdot P(A) + 0^2 \cdot (1 - P(A)) = P(A)
$$

利用方差公式 $Var(X) = E[X^2] - (E[X])^2$：
$$
\begin{aligned}
Var(I_A) &= E[I_A^2] - (E[I_A])^2 \\
&= P(A) - (P(A))^2 \\
&= P(A)(1 - P(A))
\end{aligned}
$$

标准差 $\sigma_{I_A}$ 是方差的算术平方根：
$$
\sigma_{I_A} = \sqrt{Var(I_A)} = \sqrt{P(A)(1 - P(A))}
$$

**4. 关于“积分是期望”的注记**

在现代概率论（基于测度论）中，数学期望被定义为关于概率测度 $P$ 的勒贝格积分。对于事件 $A$：
$$
P(A) = \int_{\Omega} I_A(\omega) \, dP(\omega) = E[I_A]
$$
这解释了题目结尾注记的数学本质。

### 答案

示性变量 $I_A$ 的性质如下：

1.  **分布律**：
    $$P(I_A=1) = P(A), \quad P(I_A=0) = 1 - P(A)$$
2.  **期望**：
    $$E[I_A] = P(A)$$
3.  **标准差**：
    $$\sigma_{I_A} = \sqrt{P(A)(1 - P(A))}$$

*(注：题目原文中的 $\sigma I_A = p P(A)(1 - P(A))$ 这里的 $p$ 应理解为根号 $\sqrt{\cdot}$ 的排版错误或特定记号，正确形式应包含根号)*

### 知识延伸

1.  **示性变量法（Method of Indicators）**：
    这是解决复杂计数和概率问题（特别是涉及“匹配”或“乱序”问题）的强力工具。利用期望的线性性质（Linearity of Expectation）：
    $$E\left[\sum_{i} I_{A_i}\right] = \sum_{i} E[I_{A_i}] = \sum_{i} P(A_i)$$
    即使事件 $A_i$ 之间**不独立**，该公式依然成立。

2.  **重要性质**：
    *   **积的示性变量**：$I_{A \cap B} = I_A \cdot I_B$。
    *   **和的示性变量**：$I_{A \cup B} = I_A + I_B - I_A I_B$（对应容斥原理）。
    *   **幂等性**：$(I_A)^k = I_A$，对于任意 $k \ge 1$。

3.  **常见易错点**：
    *   计算方差时，$Var(\sum I_{A_i}) \neq \sum Var(I_{A_i})$，除非 $A_i$ 相互独立。若不独立，必须考虑协方差项：
        $$Var\left(\sum_{i} X_i\right) = \sum_{i} Var(X_i) + \sum_{i \neq j} Cov(X_i, X_j)$$
    *   混淆方差与标准差，忘记开根号。

---

## 例题0.30

### 题目

期末某校评优，甲、乙、丙、丁、戊5名同学分别获得“德、智、体、美、劳”单项奖状。 不巧的是，辅导员发放奖状时完全随机放乱了。请问，5名同学中平均意义上有几人拿到了与自己匹配的奖状？ 7 0.4.尾部概率 -S0.4 尾部概率 在实际应用中，一方面，事件的概率未必有简易的计算方法；另一方面，很多情况下非平凡的概率界也可以用来指导系统的设计。下面介绍一些重要的不等式。 1. Markov（马尔科夫）不等式（若均值有限，则尾部概率线性递减趋于0） 萨非鱼成机密司 $\mathit{X}$ 的期望 $\mathbf{E} ( X )$ 的值有限，则对于 $y > 0 , \ \ P ( X \geq ...

### 解答

**Refining the Solution**

\n\n

I've finalized the solution steps and am now focused on the LaTeX formatting for maximum clarity. My current focus is ensuring all mathematical notation is perfectly rendered. I'm also proofreading the Chinese translation to ensure accuracy and a professional tone. The knowledge extension section is being expanded to give the expert a deep understanding of the problem.


### 题目分析
本题描述了一个经典的**配对问题**（Matching Problem），也被称为“乱序排列”或“帽子检查问题”的变体。题目中共有 $n=5$ 个不同的元素（奖状）和 $n=5$ 个不同的位置（学生）。“完全随机放乱”意味着所有 $5!$ 种排列出现的概率是相等的。题目要求求解“平均意义上有几人拿到与自己匹配的奖状”，即求解匹配人数这一随机变量的**数学期望**。本题考查的核心概念是**随机变量的期望**以及**期望的线性性质**（Linearity of Expectation），同时也涉及指示随机变量（Indicator Random Variable）的应用。题目末尾的“7 0.4. 尾部概率”疑似为原书排版残留或章节索引，与求解期望的主题无关，解题时予以忽略。

### 解题过程

设 $n=5$ 为学生人数（也是奖状数量）。
定义随机变量 $X$ 表示拿到自己匹配奖状的学生总人数。我们需要求解 $E[X]$。

**步骤 1：引入指示随机变量**

为了简化计算，我们引入指示随机变量 $X_i$ ($i=1, 2, 3, 4, 5$)。
定义 $X_i$ 如下：
$$
X_i = \begin{cases} 
1, & \text{如果第 } i \text{ 名学生拿到了自己的奖状} \\
0, & \text{如果第 } i \text{ 名学生没有拿到自己的奖状}
\end{cases}
$$

显然，总的匹配人数 $X$ 可以表示为所有指示变量的和：
$$
X = \sum_{i=1}^{5} X_i = X_1 + X_2 + X_3 + X_4 + X_5
$$

**步骤 2：计算单个指示变量的期望**

根据期望的定义，对于任意的 $i$：
$$
E[X_i] = 1 \cdot P(X_i=1) + 0 \cdot P(X_i=0) = P(X_i=1)
$$

我们需要计算第 $i$ 名学生拿到自己奖状的概率 $P(X_i=1)$。
- 样本空间的总数是 $n$ 个奖状的全排列，即 $n!$。
- 有利事件是第 $i$ 名学生拿到了自己的奖状。固定第 $i$ 名学生的奖状为正确奖状后，其余 $n-1$ 名学生的奖状可以任意排列。
- 因此，有利事件的排列数为 $(n-1)!$。

所以：
$$
P(X_i=1) = \frac{(n-1)!}{n!} = \frac{1}{n}
$$

在本题中，$n=5$，故：
$$
E[X_i] = \frac{1}{5}
$$

**步骤 3：利用期望的线性性质求解 $E[X]$**

数学期望具有线性性质：**和的期望等于期望的和**。这一性质无论随机变量之间是否独立都成立（注意：本题中 $X_i$ 之间并不是相互独立的，例如若前 4 人拿对，第 5 人必然拿对，但线性性质依然适用）。

$$
\begin{aligned}
E[X] &= E\left[ \sum_{i=1}^{5} X_i \right] \\
&= \sum_{i=1}^{5} E[X_i] \\
&= \sum_{i=1}^{5} \frac{1}{5} \\
&= 5 \times \frac{1}{5} \\
&= 1
\end{aligned}
$$

### 答案
$$ E[X] = 1 $$

### 知识延伸

1.  **期望的线性性质（Linearity of Expectation）**：
    公式为 $E[\sum X_i] = \sum E[X_i]$。这是概率论中最强大的工具之一，因为它不需要考虑变量 $X_i$ 之间的独立性。在处理复杂的组合计数或几何概率问题时，通过引入指示变量并将大问题拆解为小问题求和，往往能极大地简化计算。

2.  **配对问题的普适性**：
    无论 $n$ 是多少（只要 $n \ge 1$），平均匹配人数的期望永远是 **1**。
    $$ E[X] = n \cdot \frac{1}{n} = 1 $$

3.  **方差与分布（进阶）**：
    - 对于配对问题，匹配人数 $X$ 的方差也是 1，即 $Var(X) = 1$。
    - 当 $n \to \infty$ 时，随机变量 $X$ 的分布收敛于参数为 $\lambda=1$ 的**泊松分布**（Poisson Distribution）。
    - 没有任何人拿到自己奖状（即 $X=0$）的概率称为错排概率，由错排公式给出，当 $n$ 很大时，$P(X=0) \approx \frac{1}{e}$。

4.  **易错点提示**：
    初学者容易试图列出 $X$ 的分布列（即计算 $P(X=0), P(X=1), \dots$）然后按定义求期望。虽然可行，但计算量巨大且容易出错（特别是计算错排数 $D_n$ 时）。使用指示随机变量法是此类问题的标准解法。

---

## 例题0.31

### 题目

生日悖论：把 $n$ 个球放进 $M$ 个箱子，有一个箱子内球数超过1的概率。

### 解答

**Defining the Approach**

\n\n

I'm currently focused on precisely defining the problem's parameters: $n$ balls, $M$ bins, seeking the probability of a "collision." My analysis now includes clear symbolic representations and the core concepts, specifically combinatorics, and the complementary event. I'm crafting the "Problem Analysis" section now. I am also planning the detailed mathematical derivation.


### 题目分析
本题考查的是经典的**古典概型**与**组合数学**问题。题目中的 $n$ 代表球的数量（对应生日问题中的人数），$M$ 代表箱子的数量（对应一年中的天数，通常为365）。核心概念是利用**补集思想**（Complementary Counting）来计算概率。直接计算“至少有一个箱子球数超过1”的情况非常复杂，因为这包含了恰好有一个箱子有2个球、有3个球、或者多个箱子有多个球等多种互斥情况。因此，解题的关键在于先计算对立事件——“所有箱子内的球数都不超过1”（即所有球都在不同的箱子内）的概率，然后用1减去该概率。

### 解题过程

我们将问题转化为：将 $n$ 个可区分的球放入 $M$ 个可区分的箱子中，求至少有一个箱子中球的数量大于等于2的概率 $P$。

**步骤 1：确定样本空间的总数**
每个球都有 $M$ 种选择（可以放入 $M$ 个箱子中的任意一个）。由于共有 $n$ 个球，且球的放置是相互独立的，根据乘法原理，样本空间 $\Omega$ 的样本点总数为：
$$|\Omega| = M \times M \times \cdots \times M = M^n$$

**步骤 2：定义事件与对立事件**
设事件 $A$ 为“有一个箱子内球数超过1”（即发生碰撞）。
设事件 $A^c$ 为“所有箱子内球数都不超过1”（即没有发生碰撞，所有球所在的箱子互不相同）。

根据概率的性质：
$$P(A) = 1 - P(A^c)$$

**步骤 3：计算对立事件的样本点数**
事件 $A^c$ 意味着 $n$ 个球必须放入 $M$ 个不同的箱子中。这要求 $n \le M$（如果 $n > M$，根据鸽巢原理，必然有箱子球数超过1，此时 $P(A)=1$）。
假设 $n \le M$，我们可以按顺序放置球：
- 第1个球有 $M$ 种选择；
- 第2个球为了不与第1个球同箱，有 $M-1$ 种选择；
- 第3个球为了不与前2个球同箱，有 $M-2$ 种选择；
- $\cdots$
- 第 $n$ 个球有 $M-(n-1)$ 种选择。

因此，事件 $A^c$ 包含的样本点数为从 $M$ 个元素中取 $n$ 个元素的排列数 $P_M^n$（或记为 $A_M^n$）：
$$|A^c| = M \times (M-1) \times (M-2) \times \cdots \times (M-n+1) = \frac{M!}{(M-n)!}$$

**步骤 4：计算概率**
对立事件的概率为：
$$P(A^c) = \frac{|A^c|}{|\Omega|} = \frac{M(M-1)\cdots(M-n+1)}{M^n} = \frac{M!}{M^n (M-n)!}$$

也可以写成乘积形式：
$$P(A^c) = \frac{M}{M} \times \frac{M-1}{M} \times \cdots \times \frac{M-n+1}{M} = \prod_{i=0}^{n-1} \left(1 - \frac{i}{M}\right)$$

**步骤 5：得出最终结果**
所求事件 $A$ 的概率为：
$$P(A) = 1 - \frac{M!}{M^n (M-n)!}$$

### 答案
设 $n$ 个球放进 $M$ 个箱子，有一个箱子内球数超过 1 的概率为 $p$，则：
$$p = \begin{cases} 1 - \frac{M!}{M^n(M-n)!} & \text{若 } n \le M \\ 1 & \text{若 } n > M \end{cases}$$

### 知识延伸
1.  **近似公式**：
    当 $n$ 远小于 $M$ 时，利用泰勒展开近似 $e^{-x} \approx 1-x$，可以将上述概率近似为：
    $$P(A^c) = \prod_{i=0}^{n-1} \left(1 - \frac{i}{M}\right) \approx \prod_{i=0}^{n-1} e^{-\frac{i}{M}} = e^{-\sum_{i=0}^{n-1} \frac{i}{M}} = e^{-\frac{n(n-1)}{2M}}$$
    因此，碰撞概率近似为：
    $$P(A) \approx 1 - e^{-\frac{n(n-1)}{2M}}$$
    这一近似公式常用于快速估算。例如在 $M=365$ 时，当 $n=23$，概率约为 $0.5$。

2.  **应用领域**：
    -   **哈希碰撞（Hash Collision）**：在计算机科学中，评估哈希函数发生冲突的概率。
    -   **生日攻击（Birthday Attack）**：密码学中利用该原理寻找哈希碰撞，从而降低暴力破解的复杂度（将复杂度从 $2^k$ 降低到 $2^{k/2}$）。

3.  **易错点**：
    -   混淆“恰好有一个箱子有2个球”与“至少有一个箱子有2个球以上”。本题是后者，必须使用补集法。
    -   误认为球或箱子不可区分。在标准的生日悖论模型中，默认样本空间是基于有序元组的（即球是不同的，箱子也是不同的），这符合麦克斯韦-玻尔兹曼统计。

---

## 例题0.32

### 题目

中奖概率： $M$ 个球，其中 $n$ 个球有奖，从中取 $n$ 个球，求中奖概率。

### 解答

**Calculating Probabilities**

My current thinking centers on the ambiguity of the phrase "winning probability." I've decided to interpret it as the probability of drawing all $n$ prize balls (the jackpot), but I am also including the general Hypergeometric distribution formula, with a clarification of the interpretation for "winning" in this specific problem setup. I'm focusing on providing a comprehensive, precise, and unambiguous answer within the requested format. This ensures a complete solution for any interpretation of the word winning.


### 题目分析

本题属于典型的**古典概型**（Classical Probability）问题，涉及**组合数学**与**超几何分布**（Hypergeometric Distribution）的核心概念。

*   **已知条件**：
    *   总体容量：$M$ 个球。
    *   目标特征数（有奖球）：$n$ 个。
    *   样本容量（抽取数）：$n$ 个。
    *   抽取方式：不放回抽取（由“从中取 $n$ 个球”暗示）。
*   **核心概念**：排列组合、古典概型定义。
*   **求解目标**：计算“中奖”的概率。
    *   *注*：在概率论经典例题中，当样本容量与奖品数量相等（均为 $n$）时，“中奖概率”通常指**头奖**（Jackpot），即抽出的 $n$ 个球恰好全部是那 $n$ 个有奖球。

### 解题过程

设随机变量 $X$ 表示抽出的 $n$ 个球中含有奖球的个数。

**1. 确定样本空间**
从 $M$ 个球中不放回地抽取 $n$ 个球，所有可能的组合总数（样本空间 $\Omega$ 的基数）为从 $M$ 个元素中取 $n$ 个元素的组合数：
$$ |\Omega| = \binom{M}{n} = C_M^n = \frac{M!}{n!(M-n)!} $$

**2. 分析“中奖”事件**
设事件 $A$ 为“中奖”（即抽出的 $n$ 个球全部为有奖球）。
要发生事件 $A$，必须满足以下两个条件：
*   从 $n$ 个有奖球中选出 $n$ 个。
*   从 $M-n$ 个无奖球中选出 $0$ 个。

该事件包含的基本结果数（有利场合数）为：
$$ |A| = \binom{n}{n} \cdot \binom{M-n}{0} $$

根据组合数性质 $\binom{n}{n} = 1$ 且 $\binom{k}{0} = 1$，可得：
$$ |A| = 1 \cdot 1 = 1 $$

**3. 计算概率**
根据古典概型的概率定义 $P(A) = \frac{|A|}{|\Omega|}$，中奖概率为：
$$ P(A) = \frac{\binom{n}{n}\binom{M-n}{0}}{\binom{M}{n}} = \frac{1}{\binom{M}{n}} $$

**4. 化简表达式**
将组合数公式代入化简：
$$ P(A) = \frac{1}{\frac{M!}{n!(M-n)!}} = \frac{n!(M-n)!}{M!} $$

或者写成连乘形式：
$$ P(A) = \frac{n}{M} \cdot \frac{n-1}{M-1} \cdot \cdots \cdot \frac{1}{M-n+1} $$

### 答案

中奖概率（指抽中全部 $n$ 个奖球）为：
$$ P = \frac{1}{\binom{M}{n}} = \frac{n!(M-n)!}{M!} $$

### 知识延伸

1.  **超几何分布 (Hypergeometric Distribution)**
    本题是超几何分布的特例。若从 $M$ 个球（其中 $n$ 个有奖）中抽取 $k$ 个球（本题中 $k=n$），其中恰好有 $x$ 个有奖球的概率分布律为：
    $$ P(X=x) = \frac{\binom{n}{x}\binom{M-n}{k-x}}{\binom{M}{k}} $$
    本题即求解 $k=n, x=n$ 时的概率。

2.  **期望与方差**
    服从超几何分布 $X \sim H(M, n, k)$ 的随机变量，其数学期望为：
    $$ E[X] = k \cdot \frac{n}{M} $$
    在本题情境下（抽取 $n$ 个），期望抽中的奖球数为 $E[X] = n \cdot \frac{n}{M} = \frac{n^2}{M}$。

3.  **二项分布近似**
    当总球数 $M$ 远大于抽取数 $n$ ($M \gg n$) 时，不放回抽取近似于放回抽取，超几何分布可近似为二项分布 $B(n, p)$，其中 $p = \frac{n}{M}$。

4.  **易错点提示**
    *   **混淆“恰好”与“至少”**：若题目问“至少中一个奖”，则应用 $1 - P(X=0)$ 求解。本题语境下通常指特定组合的中奖（即全部匹配）。
    *   **符号计算**：在处理阶乘化简时，注意 $0! = 1$ 以及组合数的上下标限制条件。

---

## 例题0.33

### 题目

$P o l y a$ 的传染病罐子模型：罐子里有 $b$ 个黑球， $w$ 个白球 $( n=b+w )$ 。均匀随机取一个球，然后放回 $a$ 个同色球。记 $B_{k}$ 是第 $k$ 次取到黑球的事件， $W_{k}$ 是第 $k$ 次取到白球的事件。求： (1) $P ( B_{2} | W_{1} )$ ; (2) $P ( B_{1} B_{2} )$ 。 0.5.2独立掷硬币多次，正面朝上的概率为 $p$

### 解答

**Outlining a Response**

I'm now structuring the answer into sections: 题目分析, 解题过程, 答案, and 知识延伸, per the plan.  I'm also paying close attention to the use of LaTeX for mathematical notation and the required level of detail, to provide a proper proof. Further, I have explicitly stated to ignore the extraneous coin toss text.


### 题目分析
本题考查的是经典的 **Polya 罐子模型（Polya's Urn Model）**。题目给出了初始状态：罐中有 $b$ 个黑球和 $w$ 个白球，总数 $n=b+w$。规则是“随机取球，放回原球并加进 $a$ 个同色球”。这是一种**随机过程**，具体表现为**依赖过去的随机试验**（非独立同分布），涉及的核心概念包括**条件概率**和**乘法公式**。题目末尾关于“独立掷硬币”的文字属于无关信息（可能是下一题的题干），在此忽略。本题的目标是求解特定条件下的概率 $P(B_2|W_1)$ 以及联合概率 $P(B_1B_2)$。

### 解题过程

令 $b$ 表示初始黑球数，$w$ 表示初始白球数，初始总球数为 $n = b + w$。
每次取球后，罐中的球总数会增加 $a$ 个。

#### 第一部分：求解 $P(B_2|W_1)$

该式表示在**第1次取到白球**（事件 $W_1$ 发生）的条件下，**第2次取到黑球**（事件 $B_2$ 发生）的概率。

1.  **分析第1次取球后的状态变化：**
    *   已知第1次取到了白球（$W_1$）。
    *   根据规则，我们需要将取出的白球放回，并额外放入 $a$ 个白球。
    *   此时罐中球的数量变化如下：
        *   黑球数量：保持不变，仍为 $b$。
        *   白球数量：变为 $w + a$。
        *   此时罐中球的总数变为：$n + a = b + w + a$。

2.  **计算第2次取到黑球的概率：**
    *   在上述新的状态下，从罐中随机取出一个球。
    *   取到黑球的概率等于黑球数量除以总球数。
    $$
    P(B_2 | W_1) = \frac{\text{当前黑球数}}{\text{当前总球数}} = \frac{b}{b + w + a}
    $$

#### 第二部分：求解 $P(B_1B_2)$

该式表示**第1次取到黑球且第2次也取到黑球**的联合概率。根据概率的乘法公式：
$$
P(B_1 B_2) = P(B_1) \cdot P(B_2 | B_1)
$$

1.  **计算 $P(B_1)$：**
    *   初始状态下，罐中有 $b$ 个黑球，$w$ 个白球，总数 $b+w$。
    *   第1次取到黑球的概率为：
    $$
    P(B_1) = \frac{b}{b + w}
    $$

2.  **计算 $P(B_2 | B_1)$：**
    *   假设第1次取到了黑球（$B_1$ 发生）。
    *   根据规则，放回原黑球并放入 $a$ 个黑球。
    *   此时罐中球的数量变化如下：
        *   黑球数量：变为 $b + a$。
        *   白球数量：保持不变，仍为 $w$。
        *   此时罐中球的总数变为：$b + a + w$。
    *   在此条件下，第2次取到黑球的概率为：
    $$
    P(B_2 | B_1) = \frac{b + a}{b + w + a}
    $$

3.  **计算联合概率：**
    将上述两步结果代入乘法公式：
    $$
    \begin{aligned}
    P(B_1 B_2) &= P(B_1) \cdot P(B_2 | B_1) \\
    &= \frac{b}{b + w} \cdot \frac{b + a}{b + w + a}
    \end{aligned}
    $$

### 答案

(1) $P(B_2|W_1) = \frac{b}{b + w + a}$

(2) $P(B_1B_2) = \frac{b(b + a)}{(b + w)(b + w + a)}$

### 知识延伸

1.  **可交换性 (Exchangeability)**：
    Polya 罐子模型的一个重要性质是序列的可交换性。即只要取出的黑球和白球总数相同，其发生的概率与顺序无关。例如：
    $$P(B_1 W_2) = \frac{b}{b+w} \cdot \frac{w}{b+w+a} = P(W_1 B_2)$$
    这也意味着第 $k$ 次取到黑球的边缘概率与第 1 次相同，即 $P(B_k) = P(B_1) = \frac{b}{b+w}$。

2.  **相关公式**：
    *   **条件概率定义**：$P(A|B) = \frac{P(AB)}{P(B)}$。
    *   **乘法公式**：$P(AB) = P(A)P(B|A)$。

3.  **模型特例**：
    *   当 $a=0$ 时，为**有放回抽样**（独立同分布），$P(B_2|B_1) = P(B_1)$。
    *   当 $a=-1$ 时，为**无放回抽样**（超几何分布模型）。

4.  **易错点**：
    *   混淆总球数的变化。务必注意分母是随着抽取次数增加而增加的（每次增加 $a$）。
    *   混淆联合概率 $P(B_1 B_2)$ 与条件概率 $P(B_2 | B_1)$。联合概率是两个事件同时发生的概率，而条件概率是已知前一个事件发生后，后一个事件发生的概率。

---

## 例题0.34

### 题目

用语言描述两点分布、二项分布、几何分布。给出它们的概率质量函数、数学期望与矩生成 函数。

### 解答

**Refining Derivations**

I'm currently focused on the "解题过程" section, specifically the derivations. I've drafted the core definitions and PMFs for all three distributions. Now, I'm carefully checking the expectations and MGFs, ensuring the derivation steps, especially the summation in the Geometric MGF, are crystal clear and mathematically sound. I'm also ensuring all LaTeX is correctly formatted.


### 题目分析

本题要求对概率论中三个最基础的离散型随机变量——两点分布（伯努利分布）、二项分布和几何分布进行描述，并给出它们的核心数学性质：概率质量函数（PMF）、数学期望（Expectation）以及矩生成函数（MGF）。这是掌握离散概率模型的基础，也是后续学习随机过程（如泊松过程、马尔可夫链）的基石。

### 解题过程

我们将依次对这三种分布进行详细推导和描述。为简化记号，设单次试验成功的概率为 $p$ ($0 < p < 1$)，失败的概率为 $q = 1 - p$。

#### 1. 两点分布 (Two-point Distribution / Bernoulli Distribution)

**语言描述**：
两点分布，也称为伯努利分布，描述的是一次随机试验（称为伯努利试验），该试验只有两种可能的结果：成功（记为1）或失败（记为0）。

**概率质量函数 (PMF)**：
设随机变量 $X$ 服从参数为 $p$ 的两点分布，记为 $X \sim B(1, p)$。其取值为 $k \in \{0, 1\}$。
$$P(X=k) = p^k (1-p)^{1-k}, \quad k=0, 1$$
或者写为分段形式：
$$P(X=k) = \begin{cases} p, & k=1 \\ 1-p, & k=0 \end{cases}$$

**数学期望**：
根据期望的定义：
$$E[X] = \sum_{k=0}^{1} k \cdot P(X=k) = 0 \cdot (1-p) + 1 \cdot p = p$$

**矩生成函数 (MGF)**：
根据矩生成函数的定义 $M_X(t) = E[e^{tX}]$：
$$M_X(t) = e^{t \cdot 0} P(X=0) + e^{t \cdot 1} P(X=1) = 1 \cdot (1-p) + e^t \cdot p = q + pe^t$$

---

## 例题0.35

### 题目

观察到 $n$ 次正面朝上时，共掷了多少次？

### 解答

**Reviewing Problem Interpretation**

I'm now revisiting the problem's core to solidify the approach. I am confident it's a Negative Binomial scenario, but I want to be *absolutely* clear on the interpretation of "how many tosses". The phrasing strongly suggests an expected value, not a specific value given some fixed total tosses. Therefore, I will define N as the total number of tosses and find its expectation. The variance could be a bonus, but not expected.


### 题目分析

本题考查的是**伯努利试验（Bernoulli Trials）**序列中的等待时间问题。具体而言，题目询问的是在独立重复试验中，累积获得 $n$ 次成功（正面朝上）时，所需的总试验次数。这对应于**负二项分布（Negative Binomial Distribution）**的概念。我们需要将总试验次数视为一个随机变量，并求解其**数学期望**。

### 解题过程

设每次掷硬币正面朝上的概率为 $p$（$0 < p \le 1$）。
设随机变量 $N$ 表示观察到第 $n$ 次正面朝上时，总共进行的掷硬币次数。

我们可以将 $N$ 分解为 $n$ 个相互独立的阶段。
设 $X_1$ 为第一次掷出正面朝上所需的次数；
设 $X_2$ 为在第一次正面朝上之后，直到第二次正面朝上所需的额外次数；
...
设 $X_i$ 为在第 $i-1$ 次正面朝上之后，直到第 $i$ 次正面朝上所需的额外次数。

显然，总次数 $N$ 可以表示为：
$$N = X_1 + X_2 + \dots + X_n = \sum_{i=1}^{n} X_i$$

对于每一个 $X_i$，其代表的是在独立重复伯努利试验中，直到获得一次成功所需的试验次数。因此，$X_i$ 服从**几何分布（Geometric Distribution）**。
记为 $X_i \sim Ge(p)$。

根据几何分布的性质，其概率质量函数为：
$$P(X_i = k) = (1-p)^{k-1}p, \quad k=1, 2, \dots$$

几何分布的数学期望为：
$$E[X_i] = \frac{1}{p}$$

利用数学期望的线性性质（Linearity of Expectation），总次数 $N$ 的期望为：
$$
\begin{aligned}
E[N] &= E\left[ \sum_{i=1}^{n} X_i \right] \\
&= \sum_{i=1}^{n} E[X_i] \\
&= \sum_{i=1}^{n} \frac{1}{p} \\
&= n \cdot \frac{1}{p} \\
&= \frac{n}{p}
\end{aligned}
$$

此外，随机变量 $N$ 服从参数为 $(n, p)$ 的负二项分布，其概率质量函数（PMF）为：
$$P(N=k) = \binom{k-1}{n-1} p^n (1-p)^{k-n}, \quad k=n, n+1, \dots$$
这表示在第 $k$ 次试验时恰好出现第 $n$ 次正面，意味着前 $k-1$ 次中有 $n-1$ 次正面，且第 $k$ 次一定是正面。

### 答案

若硬币正面朝上的概率为 $p$，则观察到 $n$ 次正面朝上时，共掷次数的数学期望为：
$$E[N] = \frac{n}{p}$$

（特别地，如果是公平硬币，即 $p=0.5$，则期望次数为 $2n$。）

### 知识延伸

1.  **负二项分布（Negative Binomial Distribution）**：
    本题中的随机变量 $N$ 服从负二项分布。它描述了在伯努利试验序列中，达到指定数量的“成功”所需的试验次数。
    -   期望：$E[N] = \frac{n}{p}$
    -   方差：$Var(N) = \frac{n(1-p)}{p^2}$

2.  **几何分布与无记忆性**：
    负二项分布可以看作是 $n$ 个独立同分布的几何分布随机变量之和。几何分布具有无记忆性，即之前的失败不会影响下一次成功的概率，这是将 $N$ 分解为 $X_i$ 求和的关键依据。

3.  **易错点提示**：
    在处理几何分布和负二项分布时，需注意定义的差异。
    -   **定义一**：计算总试验次数（本题情况），支持集为 $\{n, n+1, \dots\}$，期望为 $n/p$。
    -   **定义二**：计算失败次数（即拿到 $n$ 次成功前的失败次数），支持集为 $\{0, 1, \dots\}$，期望为 $n(1-p)/p$。
    解题时务必根据“共掷了多少次”这一描述选择定义一。

---

## 例题0.36

### 题目

集优惠券问题：有 n 类不同的优惠券，买 1 件商品可以随机获得其中某类 1 张，问：平均需 要购买多少商品才能集齐？ 0.5.3 概率树，概率转移图，贝叶斯推断

### 解答

**Refining the Solution**

The decomposition into geometric phases is key, and I'm finalizing the LaTeX formatting. I'm ensuring clarity in the definition of $X_i$ and its geometric distribution. The "Markov Chain View" will be a brief footnote, emphasizing the state transitions implied by the coupon collection process. Then, I'll state the answer in both summation and harmonic number forms.


### 题目分析
本题是经典的**集优惠券问题（Coupon Collector's Problem）**。题目中给定 $n$ 种不同的优惠券，每次获取一种，且获取每种优惠券的概率相等（均为 $1/n$）。我们需要求解的是集齐所有 $n$ 种优惠券所需购买商品数量的**数学期望**。本题考查的核心概念包括**几何分布（Geometric Distribution）**的期望、**随机变量的分解**以及**期望的线性性质（Linearity of Expectation）**。虽然题目元数据提及概率树和贝叶斯推断，但解决此题最标准且严谨的方法是利用马尔可夫链的状态转移思想，将总过程分解为若干个独立的几何分布阶段。

### 解题过程

令随机变量 $X$ 表示集齐 $n$ 种优惠券所需购买的商品总数。我们需要求解 $E[X]$。

为了求解 $E[X]$，我们将整个收集过程分解为 $n$ 个阶段。
令 $X_i$ 表示在已经收集了 $i-1$ 种不同优惠券的情况下，为了收集到第 $i$ 种**新**的优惠券所需要购买的商品数量（其中 $i = 1, 2, \dots, n$）。

显然，总购买数量 $X$ 是各阶段购买数量之和：
$$X = X_1 + X_2 + \dots + X_n = \sum_{i=1}^n X_i$$

根据期望的线性性质，总期望等于各阶段期望之和：
$$E[X] = \sum_{i=1}^n E[X_i]$$

接下来分析每一个随机变量 $X_i$ 的分布：

1.  **阶段 1 ($X_1$)**：
    当我们手里有 $0$ 张优惠券时，购买第 1 件商品必然会得到一张新的优惠券（因为任何一张都是新的）。
    获得新优惠券的概率 $p_1 = \frac{n-0}{n} = 1$。
    因此，$X_1 = 1$，且 $E[X_1] = 1$。

2.  **阶段 $i$ ($X_i$)**：
    假设我们已经收集了 $i-1$ 种不同的优惠券。此时，剩余未收集的优惠券种类为 $n - (i-1)$。
    每次购买获得一张**新**优惠券（即属于未收集的那 $n-i+1$ 种）的概率为 $p_i$：
    $$p_i = \frac{n - (i-1)}{n}$$
    
    如果买到的优惠券是已经拥有的（概率为 $1-p_i$），则我们需要继续购买。这是一个典型的**伯努利试验**序列，直到成功为止。因此，$X_i$ 服从参数为 $p_i$ 的**几何分布**，记为 $X_i \sim G(p_i)$。
    
    几何分布的期望公式为 $E[Y] = \frac{1}{p}$，所以：
    $$E[X_i] = \frac{1}{p_i} = \frac{n}{n - i + 1}$$

3.  **求和**：
    将所有阶段的期望相加：
    $$E[X] = \sum_{i=1}^n \frac{n}{n - i + 1}$$
    
    展开该求和公式：
    $$E[X] = \frac{n}{n} + \frac{n}{n-1} + \dots + \frac{n}{2} + \frac{n}{1}$$
    
    提取公因子 $n$，并令求和索引 $j = n - i + 1$（当 $i=1$ 时 $j=n$，当 $i=n$ 时 $j=1$），我们可以将求和重写为调和级数的形式：
    $$E[X] = n \left( \frac{1}{n} + \frac{1}{n-1} + \dots + \frac{1}{1} \right) = n \sum_{k=1}^n \frac{1}{k}$$

### 答案
平均需要购买的商品数量为：
$$ n \sum_{k=1}^n \frac{1}{k} $$

### 知识延伸
1.  **调和级数与渐近估计**：
    上述答案中 $\sum_{k=1}^n \frac{1}{k}$ 被称为第 $n$ 个调和数，记为 $H_n$。当 $n$ 很大时，根据欧拉定理，有如下近似：
    $$H_n \approx \ln n + \gamma$$
    其中 $\gamma \approx 0.577$ 是欧拉-马歇罗尼常数（Euler-Mascheroni constant）。
    因此，集齐 $n$ 种优惠券平均需要的购买次数约为 $n \ln n$。

2.  **几何分布的性质**：
    本题的关键在于识别出从“拥有 $k$ 种”状态转移到“拥有 $k+1$ 种”状态所需的步数服从几何分布。这是马尔可夫链中状态转移时间的典型应用。

3.  **方差计算**：
    由于各阶段 $X_i$ 相互独立，总方差也等于各阶段方差之和：
    $$Var(X) = \sum_{i=1}^n Var(X_i) = \sum_{i=1}^n \frac{1-p_i}{p_i^2} < \frac{\pi^2}{6}n^2$$

4.  **易错点**：
    不要直接用总概率 $1/n$ 去计算期望，必须分阶段考虑，因为随着手中优惠券种类的增加，获得新优惠券的概率在不断降低（难度在增加）。

---

## 例题0.37

### 题目

）一个家庭有两个孩子，设男孩、女孩的机会均等。问： （1）已知老大是女孩，老二也是女孩的概率； (2）已知有一个是女孩，有两个女孩的概率。

### 解答

**Calculating Probabilities**

\n\n

I've now fully formalized the "解题过程" section, defining events, calculating probabilities for each part using conditional probability formulas, and including an intuitive check using independent events for part (1). I am also working on "知识延伸" by adding potential pitfalls of this kind of problem. I'm focusing on ensuring accuracy in the LaTeX formatting before generating the final output.


### 题目分析
本题考查概率论中的**条件概率**这一核心概念。题目描述了一个典型的样本空间模型（两个孩子的性别组合），要求在不同的已知条件下计算特定事件发生的概率。解题的关键在于正确构建样本空间，明确“已知条件”所对应的事件集合，以及“目标事件”与已知条件的交集，最后利用条件概率定义公式进行求解。

### 解题过程

首先，我们需要定义样本空间和基本事件。
设 $b$ 代表男孩（boy），$g$ 代表女孩（girl）。
样本空间 $\Omega$ 由两个孩子的性别组合构成，考虑到出生顺序（老大，老二），样本空间为：
$$ \Omega = \{(b, b), (b, g), (g, b), (g, g)\} $$
其中 $(x, y)$ 表示老大是 $x$，老二是 $y$。
根据题目“机会均等”的假设，每个基本事件发生的概率均为 $\frac{1}{4}$。

#### （1）求解：已知老大是女孩，老二也是女孩的概率

设事件 $A$ 为“老大是女孩”，事件 $B$ 为“老二也是女孩”。
我们需要求解的是条件概率 $P(B|A)$。

1.  **确定事件 $A$ 的集合**：
    老大是女孩的情况包括 $(g, b)$ 和 $(g, g)$。
    $$ A = \{(g, b), (g, g)\} $$
    故其概率为：
    $$ P(A) = \frac{2}{4} = \frac{1}{2} $$

2.  **确定事件 $AB$（即 $A \cap B$）的集合**：
    事件 $AB$ 表示“老大是女孩 且 老二也是女孩”，即两个都是女孩。
    $$ AB = \{(g, g)\} $$
    故其概率为：
    $$ P(AB) = \frac{1}{4} $$

3.  **利用条件概率公式计算**：
    $$ P(B|A) = \frac{P(AB)}{P(A)} = \frac{1/4}{1/2} = \frac{1}{2} $$

*注：从独立性角度看，老二的性别与老大无关，故 $P(B|A) = P(B) = 1/2$。*

#### （2）求解：已知有一个是女孩，有两个女孩的概率

设事件 $C$ 为“至少有一个是女孩”，事件 $D$ 为“有两个女孩”。
我们需要求解的是条件概率 $P(D|C)$。

1.  **确定事件 $C$ 的集合**：
    至少有一个女孩的情况排除了 $(b, b)$，剩下的情况为：
    $$ C = \{(b, g), (g, b), (g, g)\} $$
    故其概率为：
    $$ P(C) = \frac{3}{4} $$

2.  **确定事件 $DC$（即 $D \cap C$）的集合**：
    事件 $DC$ 表示“有两个女孩 且 至少有一个女孩”，这等同于“有两个女孩”。
    $$ DC = \{(g, g)\} $$
    故其概率为：
    $$ P(DC) = \frac{1}{4} $$

3.  **利用条件概率公式计算**：
    $$ P(D|C) = \frac{P(DC)}{P(C)} = \frac{1/4}{3/4} = \frac{1}{3} $$

### 答案
（1）已知老大是女孩，老二也是女孩的概率为 $\frac{1}{2}$。
（2）已知有一个是女孩，有两个女孩的概率为 $\frac{1}{3}$。

### 知识延伸

1.  **核心公式**：
    本题主要应用了条件概率的定义式：
    $$ P(A|B) = \frac{P(AB)}{P(B)}, \quad \text{其中 } P(B) > 0 $$

2.  **易错点分析（男孩女孩悖论）**：
    *   **指定个体 vs 存在个体**：第一问中指定了“老大”这一特定个体的性别，这排除了 $(b, g)$ 和 $(b, b)$，样本空间缩减为 2 个元素。第二问仅指出了“存在”女孩，没有指定是哪一个，只排除了 $(b, b)$，样本空间缩减为 3 个元素。
    *   **直觉误区**：初学者常误认为第二问答案也是 $1/2$，忽略了 $(b, g)$ 和 $(g, b)$ 是两个不同的样本点，导致分母计算错误。

3.  **相关性质**：
    *   **独立性**：在第一问中，如果孩子的性别是相互独立的，则一个孩子的性别信息不会改变另一个孩子性别的概率分布。
    *   **信息量的差异**：信息的具体程度（如“老大是女孩”比“有一个是女孩”更具体）会直接影响条件概率的分母，从而改变最终结果。

---

## 例题0.38

### 题目

有三个文件柜，一个文件有可能放在某个文件柜里。设快速翻阅一个会有该文件的文件柜，发现该文件的概率为 $\alpha$ 。现翻阅文件柜 $A$ 后，未发现。问文件在 $A$ 中的概率。

### 解答

**Refining the Approach**

Now I'm focusing on the actual output. I've finalized the notation, ensuring it aligns precisely with the problem's phrasing. Currently I'm translating the mathematical solution into the required format, paying close attention to the LaTeX syntax and section headings. I'm also preparing the intuitive checks for the "知识延伸" (Knowledge Extension) section, ensuring they demonstrate a solid understanding of the concepts.


### 题目分析

本题描述了一个典型的贝叶斯推断问题。题目中涉及的数学符号 $\alpha$ 代表在特定条件下（文件确实在柜中）发生某一事件（发现文件）的条件概率。本题考查的核心概念是**条件概率**、**全概率公式**以及**贝叶斯公式（Bayes' Theorem）**。我们需要求解的目标是在“翻阅文件柜 A 后未发现文件”这一新的信息（证据）下，文件实际上仍然在文件柜 A 中的**后验概率**。

### 解题过程

首先，我们需要定义相关的随机事件。

设事件 $A$、$B$、$C$ 分别表示“文件在文件柜 A、B、C 中”。
根据题意，文件在三个文件柜中的可能性是均等的（先验概率），即：
$$P(A) = P(B) = P(C) = \frac{1}{3}$$

设事件 $E$ 表示“在文件柜 A 中未发现该文件”。

我们需要根据题目给出的条件概率 $\alpha$ 来确定相关的概率值：
1.  如果文件在 A 中，发现它的概率是 $\alpha$，那么未发现它的概率是 $1 - \alpha$。即：
    $$P(E | A) = 1 - \alpha$$
2.  如果文件不在 A 中（即在 B 或 C 中），那么在 A 中肯定无法发现该文件，即未发现的概率为 1。即：
    $$P(E | B) = 1, \quad P(E | C) = 1$$

我们的目标是求解在事件 $E$ 发生的条件下，事件 $A$ 发生的概率，即 $P(A | E)$。

根据**贝叶斯公式**：
$$P(A | E) = \frac{P(E | A)P(A)}{P(E)}$$

其中，分母 $P(E)$ 可以通过**全概率公式**展开计算：
$$P(E) = P(E | A)P(A) + P(E | B)P(B) + P(E | C)P(C)$$

将已知数值代入全概率公式：
$$
\begin{aligned}
P(E) &= (1 - \alpha) \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} \\
&= \frac{1}{3} [(1 - \alpha) + 1 + 1] \\
&= \frac{1}{3} (3 - \alpha)
\end{aligned}
$$

接下来，计算分子 $P(E | A)P(A)$：
$$P(E | A)P(A) = (1 - \alpha) \cdot \frac{1}{3}$$

最后，代入贝叶斯公式求解 $P(A | E)$：
$$
\begin{aligned}
P(A | E) &= \frac{(1 - \alpha) \cdot \frac{1}{3}}{\frac{1}{3} (3 - \alpha)} \\
&= \frac{1 - \alpha}{3 - \alpha}
\end{aligned}
$$

### 答案

$$P(A | \text{未发现}) = \frac{1 - \alpha}{3 - \alpha}$$

### 知识延伸

1.  **贝叶斯定理（Bayes' Theorem）**：本题是贝叶斯定理的直接应用，展示了如何根据新的观测结果（未发现文件）更新对假设（文件在 A 中）的信念。
    *   公式：$P(H|E) = \frac{P(E|H)P(H)}{P(E)}$，其中 $H$ 是假设，$E$ 是证据。
    *   在本题中，随着 $\alpha$（搜索效率）的增加，后验概率 $\frac{1-\alpha}{3-\alpha}$ 会减小。如果 $\alpha=1$（搜索绝对可靠），则概率降为 0；如果 $\alpha=0$（搜索无效），则概率保持先验的 $1/3$。

2.  **全概率公式**：用于计算复杂事件（如“未发现文件”）的边缘概率，通过将样本空间划分为互斥的完备事件组（文件在 A、B 或 C）来分情况讨论。
    *   公式：$P(E) = \sum_{i} P(E|H_i)P(H_i)$

3.  **常见易错点**：
    *   混淆 $P(E|A)$ 和 $P(A|E)$。
    *   忽略文件不在 A 中时，$P(E|A^c)=1$ 这一隐含条件。
    *   错误地认为未在 A 中发现文件就意味着文件一定不在 A 中（除非 $\alpha=1$）。

---

## 例题0.39

### 题目

单项选择题设几个选项合适？

### 解答

**Defining the Variables**

I've formalized the mathematical framework. I'm now explicitly defining the variables involved: 'x' representing the number of options, 'n' representing the number of questions, and 'C' as the total capacity or cost, representing the total number of options presented to the student. I'm also ready to formulate the objective function to maximize the discriminating power of the test, and will apply calculus to find the critical point.


### 题目分析
本题表面上是一个教学设计问题，实则是一个经典的**最优化问题**，常出现在概率论、信息论或组合数学的引论中。题目考查的核心概念是**在总成本（总选项数或阅读量）固定的约束下，如何最大化信息容量（或区分度）**。我们需要建立一个数学模型，将“合适”量化为“区分度最大”或“信息熵最大”，利用微积分求极值的方法求解。

### 解题过程

为了从数学角度解答“设几个选项合适”，我们需要建立以下模型：

**1. 设定变量与约束条件**
假设一份试卷中：
- 每道单项选择题有 $x$ 个选项（$x$ 为正整数，且 $x \ge 2$）。
- 试卷共有 $n$ 道题目。
- 考生在答题过程中能处理的**总选项数**（代表阅读量、时间成本或试卷篇幅）是固定的，记为常数 $C$。

则我们有约束条件：
$$ n \cdot x = C \implies n = \frac{C}{x} $$

**2. 设定目标函数**
试卷的区分度取决于所有题目能产生的不同答案组合的总数（即样本空间的大小）。组合数越多，试卷区分不同水平考生的能力（理论上的信息容量）越强。
设试卷所有可能的答案组合数为 $W$，则：
$$ W = x^n $$

将约束条件 $n = \frac{C}{x}$ 代入上式，得到关于 $x$ 的函数：
$$ W(x) = x^{\frac{C}{x}} $$

我们的目标是求整数 $x$，使得 $W(x)$ 取得最大值。

**3. 求解极值**
由于 $W(x) > 0$，且对数函数 $y = \ln u$ 是单调递增函数，最大化 $W(x)$ 等价于最大化 $\ln W(x)$。
令 $f(x) = \ln W(x)$，则：
$$ f(x) = \ln \left( x^{\frac{C}{x}} \right) = \frac{C}{x} \ln x $$

对 $x$ 求导（将 $x$ 视为连续实数）：
$$ f'(x) = C \cdot \left( \frac{\ln x}{x} \right)' = C \cdot \frac{\frac{1}{x} \cdot x - \ln x \cdot 1}{x^2} = \frac{C(1 - \ln x)}{x^2} $$

令 $f'(x) = 0$，解得驻点：
$$ 1 - \ln x = 0 \implies \ln x = 1 \implies x = e $$
其中 $e \approx 2.71828$ 是自然对数的底。

当 $x < e$ 时，$f'(x) > 0$，函数单调递增；
当 $x > e$ 时，$f'(x) < 0$，函数单调递减。
因此，在实数范围内，$x = e$ 时函数取得最大值。

**4. 确定最优整数解**
由于选项数 $x$ 必须为整数，我们需要比较 $e$ 两侧最近的整数，即 $x=2$ 和 $x=3$。

我们需要比较 $x^{\frac{1}{x}}$ 在 $x=2$ 和 $x=3$ 处的大小（忽略常数 $C$，因为 $W(x) = (x^{\frac{1}{x}})^C$）：
- 当 $x=2$ 时：$2^{1/2} = \sqrt{2} \approx 1.414$
- 当 $x=3$ 时：$3^{1/3} = \sqrt[3]{3} \approx 1.442$

显然：
$$ 3^{1/3} > 2^{1/2} $$

此外，如果考虑 $x=4$：
$$ 4^{1/4} = (2^2)^{1/4} = 2^{1/2} \approx 1.414 $$
可见 $x=3$ 优于 $x=2$ 和 $x=4$。

### 答案
从数学优化的角度来看，单项选择题设 **3** 个选项最合适。

$$ \text{最优选项数} = 3 $$

### 知识延伸
1.  **数学原理**：本题本质上是考察函数 $y = x^{1/x}$ 的最大值问题。该函数的最大值点是自然常数 $e$。这解释了为什么在计算机科学中三进制（接近 $e$）在理论上比二进制更高效，尽管工程实现上二进制更方便。
2.  **信息论视角**：这与香农信息熵的概念有关。在总资源受限的情况下，底数为 $e$ 时单位成本所能携带的信息量最大。
3.  **实际应用与权衡**：
    *   虽然数学推导结果是 3，但在实际考试中，为了降低**猜测概率**（Guessing Probability），常采用 4 个选项。
    *   3 个选项的猜测概率是 $1/3 \approx 33.3\%$。
    *   4 个选项的猜测概率是 $1/4 = 25\%$。
    *   命题者通常在“区分度效率”（数学最优为3）和“防猜测干扰”（选项越多越好）之间做权衡，因此 4 选项也是常见选择，但纯数学效率上 3 最佳。

---

## 例题0.40

### 题目

试着解释临床检查中的“假阳性与假阴性” 。一个阳性者真有病的概率受哪些因素影响? 9 0.5. 几种典型概率问题分析 0.5.4 数学期望

### 解答

**Refining and Formatting**

The problem has been deconstructed, and I've successfully drafted the core definitions, formulas, and identified the influencing factors. I'm satisfied with the logic and clarity so far. I am focusing now on the formatting requirements, especially the LaTeX rendering. I've re-examined the prompt to make certain I haven't missed any details. My next steps involve generating the final answer.


### 题目分析
本题考查概率论中**贝叶斯公式（Bayes' Theorem）**与**全概率公式（Law of Total Probability）**的实际应用。题目要求解释医学诊断中的“假阳性”与“假阴性”概念，并推导在已知检测结果为阳性的条件下，受试者真正患病的后验概率（即阳性预测值）。尽管题目末尾出现了“数学期望”字样，但根据问题描述，核心考点在于条件概率的计算及先验概率（患病率）对后验概率的影响，属于典型的逆概问题。

### 解题过程

#### 1. 定义事件与符号
为了清晰地描述问题，我们定义以下随机事件：
- 设 $A$ 表示“受试者真正患病”（True State is Disease）。
- 设 $\bar{A}$ 表示“受试者未患病”（True State is Healthy）。
- 设 $B$ 表示“化验结果呈阳性”（Test Result is Positive）。
- 设 $\bar{B}$ 表示“化验结果呈阴性”（Test Result is Negative）。

#### 2. 解释“假阳性”与“假阴性”
在临床检查中，理想的检测应当是 $A$ 发生时 $B$ 必发生，$\bar{A}$ 发生时 $\bar{B}$ 必发生，但在现实中存在误差：

*   **假阳性 (False Positive)**：
    指受试者实际**未患病**，但检查结果呈**阳性**。
    在概率论中，这对应于条件概率 $P(B|\bar{A})$。
    这也被称为**误诊**，对应假设检验中的**第一类错误（Type I Error）**。

*   **假阴性 (False Negative)**：
    指受试者实际**患病**，但检查结果呈**阴性**。
    在概率论中，这对应于条件概率 $P(\bar{B}|A)$。
    这也被称为**漏诊**，对应假设检验中的**第二类错误（Type II Error）**。

#### 3. 推导阳性者真有病的概率
我们需要求解的是：已知化验结果为阳性（$B$），受试者真正患病（$A$）的概率，即后验概率 $P(A|B)$。

根据**贝叶斯公式**：
$$P(A|B) = \frac{P(AB)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$$

其中分母 $P(B)$ 表示人群中检测出阳性的总概率。根据**全概率公式**，可以将 $P(B)$ 分解为“真阳性”和“假阳性”两部分之和：
$$P(B) = P(B|A)P(A) + P(B|\bar{A})P(\bar{A})$$

将分母代入贝叶斯公式，得到最终表达式：
$$P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\bar{A})P(\bar{A})}$$

#### 4. 分析影响因素
为了更直观地分析，我们引入医学统计常用指标：
- **患病率 (Prevalence)**：$p = P(A)$。则健康率为 $P(\bar{A}) = 1-p$。
- **敏感度 (Sensitivity)**：$Se = P(B|A)$。即真阳性率，也就是 $1 - \text{假阴性率}$。
- **特异度 (Specificity)**：$Sp = P(\bar{B}|\bar{A})$。即真阴性率。
- **假阳性率 (False Positive Rate)**：$\alpha = P(B|\bar{A}) = 1 - Sp$。

将上述符号代入公式：
$$P(A|B) = \frac{Se \cdot p}{Se \cdot p + (1-Sp) \cdot (1-p)} = \frac{Se \cdot p}{Se \cdot p + \alpha \cdot (1-p)}$$

通过观察该公式，可以得出 $P(A|B)$ 受以下三个主要因素影响：

1.  **患病率 ($p$)**：这是先验概率。如果某种疾病在人群中极其罕见（$p$ 非常小），即使检测手段很准确，$(1-p)$ 项也会很大，导致分母中假阳性部分权重增加，从而显著拉低 $P(A|B)$。
2.  **敏感度 ($Se$)**：即检测手段识别病人的能力。$Se$ 越高，分子越大，确诊概率越高。
3.  **特异度 ($Sp$) 或 假阳性率 ($\alpha$)**：即检测手段排除非病人的能力。$\alpha$ 越低（即特异度越高），分母中的干扰项越小，确诊概率越高。

### 答案

**1. 概念解释：**
*   **假阳性**：$P(B|\bar{A})$，即没病的人被检测出阳性（误诊）。
*   **假阴性**：$P(\bar{B}|A)$，即有病的人被检测出阴性（漏诊）。

**2. 阳性者真有病的概率公式：**
$$P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|\bar{A})[1-P(A)]}$$

**3. 影响因素：**
一个阳性者真有病的概率主要受以下三个因素影响：
1.  **人群患病率（先验概率）** $P(A)$；
2.  **检测的敏感度（真阳性率）** $P(B|A)$；
3.  **检测的特异度（或假阳性率）** $P(B|\bar{A})$。

### 知识延伸

1.  **贝叶斯定理的核心**：
    $$P(\text{因}|\text{果}) = \frac{P(\text{果}|\text{因}) \times P(\text{因})}{P(\text{果})}$$
    本题展示了贝叶斯定理在修正概率中的作用：先验概率（患病率）通过新的信息（检测阳性）被修正为后验概率。

2.  **基本比率谬误 (Base Rate Fallacy)**：
    这是本题最常见的直觉陷阱。人们往往忽略 $P(A)$（基础比率）。例如，对于一种发病率仅为 0.1% 的疾病，即使检测准确率高达 99%（假阳性率 1%），检测结果为阳性的人中，真正患病的概率也可能不足 10%。这是因为在庞大的健康人群基数下，假阳性的人数绝对值可能远超真阳性的人数。

3.  **假设检验联系**：
    *   假阳性 = 第一类错误 (Type I Error, $\alpha$)，即“弃真”（原假设为没病）。
    *   假阴性 = 第二类错误 (Type II Error, $\beta$)，即“取伪”（原假设为没病）。
    *   功效 (Power) = $1 - \beta = P(B|A)$，即敏感度。

---

## 例题0.41

### 题目

非负随机变量的数学期望的几何意义： (1) $X \geq0 , ~ \mathrm{E} X$ ; (2) $Y=X^{2} , ~ \mathbf{E} Y$ : (3) $X \geq0 , \, \, \, Y=\operatorname* {m i n} ( 1 , \, \, \, X )$ ，说明 $\mathbf{E} Y=P ( X \geq U )$ ，其中 $U$ 是独立于 $\mathit{X}$ ,取值于 $[ 0 , ~ 1 ]$ 区间的均匀随机变量； (4）非负整数变量 $\mathit{X}$ ， $\mathbf E X=\underset{i=1} {\ov...

### 解答

**Elaborating the Solution**

I'm now fully immersed in the problem, focusing on constructing the LaTeX-formatted solutions for each part. I'm paying close attention to the geometric interpretations for continuous RVs, particularly the use of the area under the survival function. Also ensuring a clean, rigorous, and direct style in the explanations. The discrete case is also being worked on now.


### 题目分析
本题主要考查**非负随机变量数学期望的尾概率公式**（Tail Sum Formula）及其几何意义。题目涉及数学符号包括数学期望 $E[\cdot]$、概率 $P(\cdot)$、均匀分布随机变量 $U$ 以及求和符号 $\sum$。
核心概念利用了积分与求和交换次序（Fubini定理在概率论中的应用）以及指示函数法。
题目要求：
1.  给出非负随机变量 $X$ 的期望 $E[X]$ 的积分表达（几何意义）。
2.  给出 $Y=X^2$ 的期望 $E[Y]$ 的积分表达。
3.  证明截断变量 $Y=\min(1, X)$ 的期望等于 $P(X \ge U)$。
4.  证明非负整数变量 $X$ 的期望等于其尾概率之和。

---

## 例题0.42

### 题目

条件期望： (1) $\mathbf{E} ( X | Y )$ 的意义； (2) $\mathbf{E} X=\mathbf{E} ( \mathbf{E} ( X | Y ) )$ ; (3）掷一个均匀骰子，点数记为 $N$ ，再掷 $N$ 次，点数记为 $X_{1} , \, \, X_{2} , \, \, . . . . , \, \, X_{N}$ ，求 $\mathbf{E} ( \sum X_{i} )$ 。 (1) $\mathit{Y}$ $\mathit{Y}$ $F_{Y} \left( y \right)=1-\frac{2} {\left( y+1 \right) ...

### 解答

**Revising Solution and Formatting**

I've finalized the solution, ensuring the distinction between conditional expectation and its realization is crystal clear. The LaTeX formatting has been double-checked for accuracy. Further, I've confirmed the "Sum" interpretation, as the most logical and standard for this textbook example, with a note regarding the typo.


### 题目分析

本题主要考查概率论中的**条件期望**（Conditional Expectation）及其核心性质。
1.  题目中符号 $E(X|Y)$ 表示给定随机变量 $Y$ 条件下 $X$ 的条件期望，这是一个随机变量。
2.  $EX = E(E(X|Y))$ 是全期望公式（Law of Total Expectation），也称为平滑引理或塔式性质（Tower Property）。
3.  第三问是全期望公式的典型应用场景：**随机变量和的期望**（Random Sum of Random Variables）。题目中的“PXi”根据上下文推断为排版错误，应指代求和符号 $\sum X_i$（即求 $S_N = X_1 + \dots + X_N$ 的期望）。如果是指乘积，计算复杂度远超常规例题范畴，故按随机和处理。

### 解题过程

#### （1）$E(X|Y)$ 的意义

条件期望 $E(X|Y)$ 本质上是一个**随机变量**，它是 $Y$ 的函数。
设 $g(y) = E(X|Y=y)$，其中 $E(X|Y=y)$ 是一个实数，表示当 $Y$ 取特定值 $y$ 时 $X$ 的期望值。
那么，$E(X|Y) = g(Y)$。

其具体含义如下：
1.  **作为估计量**：$E(X|Y)$ 是在已知 $Y$ 的信息下，对 $X$ 的最佳均方预测（即 $E[(X - f(Y))^2]$ 达到最小的函数 $f(Y)$ 就是 $E(X|Y)$）。
2.  **取值特性**：当随机变量 $Y$ 取值为 $y$ 时，随机变量 $E(X|Y)$ 的取值等于普通期望 $E(X|Y=y)$。

#### （2）证明 $EX = E(E(X|Y))$

此性质称为**全期望公式**。以下以离散型随机变量为例进行推导（连续型同理，只需将求和改为积分）：

根据期望的定义，对于随机变量 $Z = E(X|Y)$，其期望为：
$$
E[E(X|Y)] = \sum_{y} E(X|Y=y) \cdot P(Y=y)
$$
将条件期望 $E(X|Y=y)$ 展开为条件概率的加权和：
$$
E(X|Y=y) = \sum_{x} x \cdot P(X=x | Y=y)
$$
代入上式：
$$
\begin{aligned}
E[E(X|Y)] &= \sum_{y} \left( \sum_{x} x \cdot P(X=x | Y=y) \right) P(Y=y) \\
&= \sum_{y} \sum_{x} x \cdot P(X=x | Y=y) P(Y=y)
\end{aligned}
$$
利用乘法公式 $P(X=x | Y=y) P(Y=y) = P(X=x, Y=y)$：
$$
\begin{aligned}
E[E(X|Y)] &= \sum_{x} x \sum_{y} P(X=x, Y=y) \\
&= \sum_{x} x \cdot P(X=x) \quad (\text{利用边缘概率公式}) \\
&= E[X]
\end{aligned}
$$
证毕。

#### （3）掷骰子问题

**设定变量：**
*   设 $N$ 为第一次掷骰子的点数，$N \sim U\{1, 2, 3, 4, 5, 6\}$。
*   设 $X_i$ 为第 $i$ 次掷骰子的点数（$i=1, \dots, N$），$X_i \sim U\{1, 2, 3, 4, 5, 6\}$。
*   假设各次掷骰子相互独立，即 $X_i$ 相互独立且与 $N$ 独立。
*   令 $S = \sum_{i=1}^{N} X_i$ （即题目中的 $PXi$）。

**目标：** 求解 $E[S]$。

**步骤 1：计算 $N$ 和 $X_i$ 的期望**
由于是均匀骰子：
$$
E[N] = \frac{1+2+3+4+5+6}{6} = 3.5
$$
同理，对于任意 $i$：
$$
E[X_i] = 3.5
$$

**步骤 2：利用全期望公式**
根据第（2）问的结论：
$$
E[S] = E[E(S|N)]
$$

**步骤 3：计算内部条件期望 $E(S|N)$**
当 $N=n$ 时，$S$ 是 $n$ 个独立同分布随机变量的和：
$$
E(S|N=n) = E\left[ \sum_{i=1}^{n} X_i \right] = \sum_{i=1}^{n} E[X_i] = n \cdot E[X_1]
$$
因此，作为一个随机变量（关于 $N$ 的函数）：
$$
E(S|N) = N \cdot E[X_1]
$$

**步骤 4：计算最终期望**
$$
\begin{aligned}
E[S] &= E[N \cdot E[X_1]] \\
&= E[X_1] \cdot E[N] \quad (\text{因为 } E[X_1] \text{ 是常数}) \\
&= 3.5 \times 3.5 \\
&= 12.25
\end{aligned}
$$

### 答案

1.  $E(X|Y)$ 是一个随机变量，它是 $Y$ 的函数，表示在给定 $Y$ 的条件下 $X$ 的平均取值。
2.  $EX = E(E(X|Y))$。
3.  $E\left(\sum_{i=1}^N X_i\right) = 12.25$。

### 知识延伸

1.  **瓦尔德等式 (Wald's Equation)**：
    本题第（3）问推导出的结论 $E[\sum_{i=1}^N X_i] = E[N] \cdot E[X]$ 即为著名的瓦尔德等式。
    *   **适用条件**：$X_i$ 独立同分布，$N$ 是取正整数值的停时（Stopping Time）或与 $X_i$ 序列独立。

2.  **随机和的方差 (Eve's Law / Law of Total Variance)**：
    如果要求方差，需使用公式：
    $$
    Var(S) = E[N]Var(X) + (E[X])^2 Var(N)
    $$
    这体现了随机和的波动来源两部分：各项数值本身的波动（第一项）和项数 $N$ 的波动（第二项）。

3.  **几何意义**：
    在希尔伯特空间 $L^2$ 中，条件期望 $E(X|Y)$ 可以看作是随机变量 $X$ 在由 $Y$ 生成的子空间上的**正交投影**。全期望公式 $E[X] = E[E(X|Y)]$ 对应于投影的平均性质。

---

## 例题0.43

### 题目

考虑一个正整数随机变量Y ，其累积分布函数 FY (y) = 1 − 2 (y+1)(y+2) ，求： （1）Y 的质量分布函数； （2）EY ； （3）设X 是正整数随机变量，且PX|Y (x|y) = 1 y ，1 ≤ x ≤ y，求(a) E(X|Y = y)；(b) X 的概率质量函 数PX(x)，EX。 0.5.5 对称性的应用

### 解答

### 题目分析

本题涉及离散随机变量的累积分布函数（CDF）与概率质量函数（PMF）的转换，条件概率分布，以及条件期望和全期望公式的应用。给定Y的累积分布函数 $F_Y(y) = 1 - \frac{2}{(y+1)(y+2)}$，需要求Y的PMF、期望，以及在给定条件分布下求X的相关性质。核心考查离散随机变量的基本性质和条件期望的计算。

### 解题过程

**（1）求Y的概率质量函数**

对于离散随机变量，概率质量函数为：
$$P_Y(y) = F_Y(y) - F_Y(y-1)$$

首先计算 $F_Y(y)$ 和 $F_Y(y-1)$：

$$F_Y(y) = 1 - \frac{2}{(y+1)(y+2)}$$

$$F_Y(y-1) = 1 - \frac{2}{y(y+1)}$$

因此：
$$P_Y(y) = \left(1 - \frac{2}{(y+1)(y+2)}\right) - \left(1 - \frac{2}{y(y+1)}\right)$$

$$= \frac{2}{y(y+1)} - \frac{2}{(y+1)(y+2)}$$

$$= \frac{2}{y+1}\left(\frac{1}{y} - \frac{1}{y+2}\right)$$

$$= \frac{2}{y+1} \cdot \frac{y+2-y}{y(y+2)}$$

$$= \frac{2}{y+1} \cdot \frac{2}{y(y+2)}$$

$$= \frac{4}{y(y+1)(y+2)}$$

因此，$P_Y(y) = \frac{4}{y(y+1)(y+2)}$，$y = 1, 2, 3, \ldots$

**（2）求 $E[Y]$**

$$E[Y] = \sum_{y=1}^{\infty} y \cdot P_Y(y) = \sum_{y=1}^{\infty} \frac{4y}{y(y+1)(y+2)}$$

$$= \sum_{y=1}^{\infty} \frac{4}{(y+1)(y+2)}$$

利用部分分式分解：
$$\frac{4}{(y+1)(y+2)} = 4\left(\frac{1}{y+1} - \frac{1}{y+2}\right)$$

因此：
$$E[Y] = 4\sum_{y=1}^{\infty}\left(\frac{1}{y+1} - \frac{1}{y+2}\right)$$

这是一个伸缩级数：
$$= 4\left[\left(\frac{1}{2} - \frac{1}{3}\right) + \left(\frac{1}{3} - \frac{1}{4}\right) + \left(\frac{1}{4} - \frac{1}{5}\right) + \cdots\right]$$

$$= 4 \cdot \frac{1}{2} = 2$$

**（3a）求 $E(X|Y=y)$**

给定 $Y=y$，$X$ 在 $\{1, 2, \ldots, y\}$ 上均匀分布，即 $P_{X|Y}(x|y) = \frac{1}{y}$，$1 \leq x \leq y$。

$$E(X|Y=y) = \sum_{x=1}^{y} x \cdot \frac{1}{y} = \frac{1}{y} \sum_{x=1}^{y} x = \frac{1}{y} \cdot \frac{y(y+1)}{2} = \frac{y+1}{2}$$

**（3b）求 $P_X(x)$ 和 $E[X]$**

利用全概率公式：
$$P_X(x) = \sum_{y=x}^{\infty} P_{X|Y}(x|y) P_Y(y) = \sum_{y=x}^{\infty} \frac{1}{y} \cdot \frac{4}{y(y+1)(y+2)}$$

$$= \sum_{y=x}^{\infty} \frac{4}{y^2(y+1)(y+2)}$$

利用全期望公式求 $E[X]$：
$$E[X] = E[E(X|Y)] = E\left[\frac{Y+1}{2}\right] = \frac{1}{2}(E[Y] + 1) = \frac{1}{2}(2 + 1) = \frac{3}{2}$$

### 答案

（1）$P_Y(y) = \frac{4}{y(y+1)(y+2)}$，$y = 1, 2, 3, \ldots$

（2）$E[Y] = 2$

（3a）$E(X|Y=y) = \frac{y+1}{2}$

（3b）$P_X(x) = \sum_{y=x}^{\infty} \frac{4}{y^2(y+1)(y+2)}$，$E[X] = \frac{3}{2}$

### 知识延伸

**涉及的定理和性质：**
- 离散随机变量的PMF与CDF关系：$P(X=k) = F(k) - F(k-1)$
- 全概率公式：$P(A) = \sum_i P(A|B_i)P(B_i)$
- 全期望公式：$E[X] = E[E(X|Y)]$
- 条件期望的计算：$E(X|Y=y) = \sum_x x P_{X|Y}(x|y)$

**重要技巧：**
- 部分分式分解处理有理函数求和
- 伸缩级数求和技巧
- 利用全期望公式简化期望计算，避免直接求边缘分布

**易错点：**
- 注意Y是正整数，求和从1开始
- 条件概率中X的取值范围依赖于Y的值
- 求边缘分布时求和下限为x而非1

---

## 例题0.44

### 题目

箱子里最初有1 个白球，1 个黑球。每次取出一个，然后放回，同时添1 个同色球。n 次试验 后，猜测箱内白球数目的分布并证明。

### 解答

**Reviewing Induction Logic**

I'm now revisiting the induction step, carefully examining the boundary conditions. It's crucial to ensure the formula holds for $k=1$ and $k=n+1$, which I believe it does. However, I want to double-check these edge cases to eliminate any potential errors before moving forward with the final LaTeX rendering.


### 题目分析

本题描述的是著名的 **波利亚罐子模型（Polya's Urn Scheme）** 的一个特例。
题目中初始状态为 1 白 1 黑，总数为 2。每次试验后总球数增加 1。
本题考查的核心概念包括：
1.  **离散型随机变量的概率分布**：需要确定 $n$ 次试验后白球数量的所有可能取值及其对应的概率。
2.  **条件概率与全概率公式**：用于推导状态转移。
3.  **数学归纳法**：用于严格证明猜测的分布规律。

我们需要求解的目标是 $n$ 次试验后箱内白球数目 $X_n$ 的概率分布律 $P(X_n=k)$。

### 解题过程

**1. 符号定义与初步分析**

设 $X_n$ 表示 $n$ 次试验后箱子中白球的个数。
初始状态：$n=0$ 时，白球数 $X_0=1$，黑球数 $1$，总球数 $2$。
每次试验取出一球放回并添加一只同色球，因此每次试验后箱子中的总球数增加 1。
经过 $n$ 次试验后，箱子中的总球数为 $2+n$。
因为初始有 1 个白球，且每次最多增加 1 个白球，所以 $X_n$ 的可能取值为 $\{1, 2, \dots, n+1\}$。

**2. 寻找规律（归纳猜想）**

我们先计算 $n=1$ 和 $n=2$ 的情况。

*   **当 $n=1$ 时**：
    箱中有 2 个球（1白1黑）。
    *   若取出白球（概率 $1/2$），放回并加 1 白，则白球数变为 2。
    *   若取出黑球（概率 $1/2$），放回并加 1 黑，则白球数仍为 1。
    分布为：
    $$P(X_1=1) = \frac{1}{2}, \quad P(X_1=2) = \frac{1}{2}$$

*   **当 $n=2$ 时**：
    *   若 $X_1=1$（概率 $1/2$，此时总球数 3，1白2黑）：
        *   取黑（概率 $2/3$）$\to X_2=1$。
        *   取白（概率 $1/3$）$\to X_2=2$。
    *   若 $X_1=2$（概率 $1/2$，此时总球数 3，2白1黑）：
        *   取黑（概率 $1/3$）$\to X_2=2$。
        *   取白（概率 $2/3$）$\to X_2=3$。
    
    综合计算 $X_2$ 的分布：
    $$P(X_2=1) = P(X_2=1|X_1=1)P(X_1=1) = \frac{2}{3} \times \frac{1}{2} = \frac{1}{3}$$
    $$P(X_2=2) = P(X_2=2|X_1=1)P(X_1=1) + P(X_2=2|X_1=2)P(X_1=2) = \frac{1}{3} \times \frac{1}{2} + \frac{1}{3} \times \frac{1}{2} = \frac{1}{3}$$
    $$P(X_2=3) = P(X_2=3|X_1=2)P(X_1=2) = \frac{2}{3} \times \frac{1}{2} = \frac{1}{3}$$

**猜想**：$X_n$ 服从离散均匀分布，即对于 $k \in \{1, 2, \dots, n+1\}$，有：
$$P(X_n = k) = \frac{1}{n+1}$$

**3. 数学归纳法证明**

**第一步：奠基**
当 $n=1$ 时，由上述计算知 $P(X_1=1)=P(X_1=2)=\frac{1}{2}$，猜想成立。

**第二步：归纳假设**
假设当 $n=m$ 时，猜想成立，即箱内白球数 $X_m$ 服从 $\{1, 2, \dots, m+1\}$ 上的均匀分布：
$$P(X_m = k) = \frac{1}{m+1}, \quad k=1, 2, \dots, m+1$$

**第三步：归纳递推**
考虑 $n=m+1$ 的情况。
在第 $m+1$ 次试验前（即 $m$ 次试验后），箱内总球数为 $m+2$。
设 $m$ 次试验后白球数为 $X_m$。进行第 $m+1$ 次试验时：
*   若取出白球（概率为 $\frac{X_m}{m+2}$），则 $X_{m+1} = X_m + 1$。
*   若取出黑球（概率为 $\frac{(m+2)-X_m}{m+2}$），则 $X_{m+1} = X_m$。

我们要计算 $P(X_{m+1} = k)$，其中 $k \in \{1, 2, \dots, m+2\}$。
根据全概率公式：
$$P(X_{m+1} = k) = P(X_{m+1}=k | X_m=k)P(X_m=k) + P(X_{m+1}=k | X_m=k-1)P(X_m=k-1)$$

分析两项条件概率：
1.  **从 $X_m=k$ 转移到 $X_{m+1}=k$**：
    这意味着第 $m+1$ 次取出了黑球。此时箱内有 $k$ 个白球，总球数 $m+2$。
    $$P(X_{m+1}=k | X_m=k) = \frac{(m+2)-k}{m+2}$$
    
2.  **从 $X_m=k-1$ 转移到 $X_{m+1}=k$**：
    这意味着第 $m+1$ 次取出了白球。此时箱内有 $k-1$ 个白球，总球数 $m+2$。
    $$P(X_{m+1}=k | X_m=k-1) = \frac{k-1}{m+2}$$

代入归纳假设 $P(X_m=j) = \frac{1}{m+1}$：
$$P(X_{m+1} = k) = \frac{m+2-k}{m+2} \cdot \frac{1}{m+1} + \frac{k-1}{m+2} \cdot \frac{1}{m+1}$$

提取公因式 $\frac{1}{(m+1)(m+2)}$：
$$P(X_{m+1} = k) = \frac{1}{(m+1)(m+2)} \left[ (m+2-k) + (k-1) \right]$$
$$P(X_{m+1} = k) = \frac{1}{(m+1)(m+2)} \cdot (m+1)$$
$$P(X_{m+1} = k) = \frac{1}{m+2}$$

*注：对于边界情况 $k=1$（只能由 $X_m=1$ 取黑球得到）和 $k=m+2$（只能由 $X_m=m+1$ 取白球得到），上述公式中 $P(X_m=0)$ 和 $P(X_m=m+2)$ 均为 0，公式依然成立。*

由此可知，当 $n=m+1$ 时，白球数 $X_{m+1}$ 在 $\{1, 2, \dots, m+2\}$ 上服从均匀分布。

**结论**
由数学归纳法原理，对于任意 $n \ge 1$，箱内白球数目 $X_n$ 服从离散均匀分布。

### 答案

$n$ 次试验后，箱内白球数目的分布为 **离散均匀分布**。
设 $X_n$ 为 $n$ 次试验后的白球数，其概率分布律为：
$$P(X_n = k) = \frac{1}{n+1}, \quad k = 1, 2, \dots, n+1$$

### 知识延伸

1.  **波利亚罐子模型 (Polya's Urn Model)**：
    本题是波利亚罐子模型的一个特例（初始参数 $c=1$）。该模型具有**可交换性 (Exchangeability)**，即只要抽出的白球和黑球总数固定，无论抽取的顺序如何，其概率都是相同的。

2.  **贝塔-二项分布 (Beta-Binomial Distribution)**：
    一般情况下，若初始有 $a$ 个白球，$b$ 个黑球，每次添加 $c$ 个同色球，则 $n$ 次试验后白球数量服从贝塔-二项分布。本题中 $a=1, b=1, c=1$，该分布退化为离散均匀分布。

3.  **极限分布与鞅 (Martingale)**：
    设 $M_n = \frac{X_n}{n+2}$ 为 $n$ 次试验后白球的比例。序列 $\{M_n\}$ 构成一个**鞅**。
    根据鞅收敛定理，当 $n \to \infty$ 时，白球的比例 $M_n$ 依概率收敛于一个随机变量 $Z$。在本题（初始1白1黑）的情况下，极限分布 $Z$ 服从标准均匀分布 $U(0,1)$（即 Beta(1,1) 分布）。

4.  **解题技巧**：
    处理此类“状态依赖路径”的概率问题时，先计算前几步（$n=1, 2, 3$）寻找规律，然后使用**数学归纳法**配合**全概率公式**进行证明是标准且严谨的方法。

---

## 例题0.45

### 题目

掷均匀硬币10 次，求下面事件的概率： （1）正面朝上的次数 = 反面朝上的次数； （2）正面朝上的次数多于反面朝上的次数； （3）至少有连续 4 次正面朝上。

### 解答

**Analyzing Recursive Solutions**

I am now focusing on the nuances of explaining the recurrence relation used to solve the consecutive heads problem. It is crucial to justify the recurrence clearly, so I've decided to explicitly detail the logic behind it, ensuring the final explanation is rigorous and easy to follow. Specifically, detailing *why* the sequence can only end with T, HT, HHT, or HHHT, so the professor can be assured of the solution's validity.


### 题目分析

本题主要考查**古典概型**、**二项分布**以及**组合数学中的递推关系**。
1.  题目背景为掷均匀硬币 10 次，这是一个典型的 $n$ 重伯努利试验（Bernoulli trials），样本空间的总容量为 $2^{10}$。
2.  第一问考查二项分布中特定点概率的计算。
3.  第二问利用二项分布的对称性进行简化计算。
4.  第三问涉及“连续出现”即“游程”（Run）的问题，直接枚举较为繁琐，核心考点在于利用**补集思想**结合**线性递推数列**求解。

### 解题过程

设掷硬币的总次数为 $n=10$。由于硬币是均匀的，每次掷硬币正面（记为 H）和反面（记为 T）的概率均为 $0.5$。样本空间 $\Omega$ 的总元素个数为：
$$|\Omega| = 2^{10} = 1024$$

#### （1）求正面朝上的次数等于反面朝上的次数的概率

设随机变量 $X$ 表示正面朝上的次数。则 $X$ 服从二项分布 $X \sim B(10, 0.5)$。
正面次数等于反面次数，即 $X = 5$。
根据二项分布概率公式：
$$P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$$
代入 $n=10, k=5, p=0.5$：
$$P(X=5) = \binom{10}{5} \left(\frac{1}{2}\right)^5 \left(\frac{1}{2}\right)^5 = \binom{10}{5} \left(\frac{1}{2}\right)^{10}$$
计算组合数 $\binom{10}{5}$：
$$\binom{10}{5} = \frac{10 \times 9 \times 8 \times 7 \times 6}{5 \times 4 \times 3 \times 2 \times 1} = 2 \times 3 \times 2 \times 7 \times 3 = 252$$
所以：
$$P(X=5) = \frac{252}{1024} = \frac{63}{256}$$

#### （2）求正面朝上的次数多于反面朝上的次数的概率

我们需要求 $P(X > 5)$。
利用二项分布的对称性，当 $p=0.5$ 时，正面次数多于反面次数的概率与反面次数多于正面次数的概率是相等的，即：
$$P(X > 5) = P(X < 5)$$
又因为所有可能情况的概率之和为 1：
$$P(X > 5) + P(X < 5) + P(X = 5) = 1$$
代入对称性关系：
$$2P(X > 5) + P(X = 5) = 1$$
解得：
$$P(X > 5) = \frac{1 - P(X=5)}{2}$$
将第一问的结果 $P(X=5) = \frac{63}{256}$ 代入：
$$P(X > 5) = \frac{1 - \frac{63}{256}}{2} = \frac{\frac{193}{256}}{2} = \frac{193}{512}$$

#### （3）求至少有连续 4 次正面朝上的概率

设事件 $A$ 为“至少有连续 4 次正面朝上”。直接计算 $A$ 较为复杂，我们采用**补集法**。
设 $\bar{A}$ 为“没有连续 4 次正面朝上”的事件。
我们需要计算长度为 10 的 0-1 序列中，不包含子串 "1111"（代表 HHHH）的序列个数，记为 $a_{10}$。

定义 $a_n$ 为长度为 $n$ 且不包含连续 4 个正面的序列个数。
我们可以根据序列的**结尾情况**建立递推关系。为了不出现 4 个连续正面，合法的序列必须以 T, HT, HHT, 或 HHHT 结尾（其中 T 代表反面，H 代表正面）。
- 如果序列以 T 结尾，则前 $n-1$ 位可以是任意合法序列，个数为 $a_{n-1}$。
- 如果序列以 HT 结尾，则前 $n-2$ 位可以是任意合法序列，个数为 $a_{n-2}$。
- 如果序列以 HHT 结尾，则前 $n-3$ 位可以是任意合法序列，个数为 $a_{n-3}$。
- 如果序列以 HHHT 结尾，则前 $n-4$ 位可以是任意合法序列，个数为 $a_{n-4}$。

因此，递推公式为：
$$a_n = a_{n-1} + a_{n-2} + a_{n-3} + a_{n-4}, \quad \text{对于 } n \geq 4$$
初始条件（直接枚举）：
- $n=0$: $a_0 = 1$ (空集，视为合法)
- $n=1$: $a_1 = 2$ (H, T)
- $n=2$: $a_2 = 4$ (HH, HT, TH, TT)
- $n=3$: $a_3 = 8$ (所有情况均合法)
- $n=4$: $a_4 = 2^4 - 1 = 15$ (只有 HHHH 不合法)

开始递推计算 $a_5$ 到 $a_{10}$：
$$a_5 = a_4 + a_3 + a_2 + a_1 = 15 + 8 + 4 + 2 = 29$$
$$a_6 = a_5 + a_4 + a_3 + a_2 = 29 + 15 + 8 + 4 = 56$$
$$a_7 = a_6 + a_5 + a_4 + a_3 = 56 + 29 + 15 + 8 = 108$$
$$a_8 = a_7 + a_6 + a_5 + a_4 = 108 + 56 + 29 + 15 = 208$$
$$a_9 = a_8 + a_7 + a_6 + a_5 = 208 + 108 + 56 + 29 = 401$$
$$a_{10} = a_9 + a_8 + a_7 + a_6 = 401 + 208 + 108 + 56 = 773$$

所以，不包含连续 4 次正面的序列个数为 773。
则 $\bar{A}$ 的概率为：
$$P(\bar{A}) = \frac{773}{1024}$$
最终求得事件 $A$ 的概率：
$$P(A) = 1 - P(\bar{A}) = 1 - \frac{773}{1024} = \frac{251}{1024}$$

### 答案

（1）正面朝上的次数等于反面朝上的次数的概率为：
$$\frac{63}{256}$$

（2）正面朝上的次数多于反面朝上的次数的概率为：
$$\frac{193}{512}$$

（3）至少有连续 4 次正面朝上的概率为：
$$\frac{251}{1024}$$

### 知识延伸

1.  **二项分布的对称性**：当成功概率 $p=0.5$ 时，二项分布关于均值 $n/2$ 对称。利用 $P(X>k) = P(X<n-k)$ 可以极大地简化计算。
2.  **游程问题（Runs Problem）与递推法**：处理“连续出现 $k$ 次”这类问题时，直接排列组合容易出错（需使用容斥原理）。构建线性递推关系（类似于斐波那契数列的推广）是解决此类计数问题的通用且严谨的方法。
    *   通用公式：若求长度为 $n$ 的序列中不出现连续 $k$ 个 1 的个数 $a_n$，递推式为 $a_n = \sum_{i=1}^{k} a_{n-i}$。
3.  **补集思想**：当题目出现“至少”、“至多”等词汇时，优先考虑计算其对立事件的概率，通常会更简单。

---

## 例题0.46

### 题目

有 $r$ 个参赛人，其中参赛人 $i ( i=1 , \, \, \, . . . , \, \, \, r )$ 在开始时有 $n_{i} ( n_{i} > 0 )$ 个单位（财富)。在每一阶段参赛人中的两个被选中比赛，赢者从输者那里得到一个单位。任何参赛人，当他的财富减少到О时就退出， 如此继续，直至某个参赛人占有所有单位的 $n=\sum_{i=1}^{r} n_{i}$ 个单位为止，此参赛人就是胜利者。假定相继比赛的结果是独立的，而且在每次比赛中两个参赛人等可能地获胜，求参赛人 $i$ 是胜利者的概率。 0.5.6 $M o n t e \, C a r l o$ 仿真

### 解答

**Reviewing the Approach**

I'm now revisiting the overall approach. While the core martingale argument is sound, I will ensure each step is rigorously justified in the LaTeX solution. My focus is on concise, step-by-step reasoning for the "解题过程" section, making sure the notation and definitions are clear. Then, I will focus on the LaTeX implementation.


### 题目分析

本题描述了一个多人的赌徒破产问题（Gambler's Ruin Problem）的推广情形。题目中给出了 $r$ 个参赛人，初始财富分别为 $n_i$，总财富为 $n = \sum_{i=1}^r n_i$。游戏规则的核心在于“公平性”：每次比赛两人对决，胜负概率各为 $0.5$，且赌注为 1 单位。这是一个典型的**随机游走（Random Walk）**问题，且具有**鞅（Martingale）**性质。我们需要求解的是某个特定参赛人最终赢得所有财富的概率。利用公平博弈下财富期望不变的性质（即鞅的停时定理）是解决此题最直接且严谨的方法。

### 解题过程

**1. 建立数学模型**

设 $X_{i,k}$ 表示第 $i$ 个参赛人在第 $k$ 次比赛结束后的财富值，其中 $i = 1, 2, \dots, r$，且 $k = 0, 1, 2, \dots$。
已知初始状态为：
$$X_{i,0} = n_i, \quad \forall i \in \{1, \dots, r\}$$
系统的总财富守恒，即对于任意时刻 $k$：
$$\sum_{j=1}^r X_{j,k} = n = \sum_{j=1}^r n_j$$

**2. 分析随机过程的性质（鞅性质）**

考虑第 $k+1$ 次比赛。假设被选中的两个参赛人为 $a$ 和 $b$。根据题目，两人获胜的概率均为 $0.5$。
对于任意参赛人 $i$，在第 $k+1$ 步的财富变化情况如下：
*   若参赛人 $i$ 未被选中参加比赛，则 $X_{i, k+1} = X_{i, k}$。
*   若参赛人 $i$ 被选中参加比赛，则：
    $$X_{i, k+1} = \begin{cases} X_{i, k} + 1, & \text{概率为 } 0.5 \\ X_{i, k} - 1, & \text{概率为 } 0.5 \end{cases}$$

我们需要计算在已知当前所有信息（记为 $\mathcal{F}_k$）的情况下，参赛人 $i$ 下一步财富的条件期望 $E[X_{i, k+1} | \mathcal{F}_k]$。
无论下一轮比赛选中哪两个人，对于特定的参赛人 $i$：
$$E[X_{i, k+1} - X_{i, k} | \mathcal{F}_k] = P(i \text{被选中}) \cdot [0.5(1) + 0.5(-1)] + P(i \text{未被选中}) \cdot 0 = 0$$

因此，
$$E[X_{i, k+1} | \mathcal{F}_k] = X_{i, k}$$
这表明随机序列 $\{X_{i, k}\}_{k \ge 0}$ 是一个**鞅（Martingale）**。

**3. 引入停时与边界条件**

设 $T$ 为游戏结束的时刻（停时）。游戏结束的条件是某一个参赛人拥有了所有 $n$ 个单位的财富，而其他人的财富均为 $0$。
由于状态空间是有限的，且 $0$ 和 $n$ 是吸收壁（对于单个玩家而言，一旦财富归零即退出，一旦达到 $n$ 游戏结束），可以证明游戏以概率 1 在有限时间内结束，即 $P(T < \infty) = 1$。

在时刻 $T$，对于参赛人 $i$，其最终财富 $X_{i, T}$ 只有两种可能：
1.  参赛人 $i$ 获胜，此时 $X_{i, T} = n$。
2.  参赛人 $i$ 失败（即其他人获胜），此时 $X_{i, T} = 0$。

设 $P_i$ 为参赛人 $i$ 最终获胜的概率。则 $X_{i, T}$ 的分布律为：
$$P(X_{i, T} = n) = P_i$$
$$P(X_{i, T} = 0) = 1 - P_i$$

**4. 应用鞅的停时定理**

根据鞅的性质（或有界收敛定理/可选停时定理），期望在停时处保持不变：
$$E[X_{i, T}] = E[X_{i, 0}]$$

代入初始值和终态分布：
$$E[X_{i, 0}] = n_i$$
$$E[X_{i, T}] = n \cdot P_i + 0 \cdot (1 - P_i) = n P_i$$

联立上述两式，得到方程：
$$n P_i = n_i$$

**5. 求解概率**

解上述方程得：
$$P_i = \frac{n_i}{n}$$

即参赛人 $i$ 获胜的概率等于其初始财富占总财富的比例。

### 答案

参赛人 $i$ 是胜利者的概率为：
$$P(\text{参赛人 } i \text{ 获胜}) = \frac{n_i}{\sum_{j=1}^r n_j}$$

### 知识延伸

1.  **鞅（Martingale）与公平博弈**：本题的核心在于识别出这是一个公平博弈模型。在公平博弈中，玩家未来的期望财富等于当前财富。利用这一性质（$E[X_T] = E[X_0]$）可以极大地简化关于“最终获胜概率”或“平均退出时间”的计算，避免了复杂的递推方程求解。
2.  **赌徒破产问题（Gambler's Ruin）**：这是经典的赌徒破产问题的推广。在经典的两人对决（$r=2$）中，若 A 有 $a$ 元，B 有 $b$ 元，A 破产 B 的概率为 $b/(a+b)$，A 赢光 B 的概率为 $a/(a+b)$。本题结论与此一致。
3.  **无关性**：值得注意的是，最终的获胜概率只与**初始财富**有关，而与每一轮**如何选择比赛对手**（只要选择机制不依赖于未来的输赢结果）无关。
4.  **蒙特卡洛仿真（Monte Carlo Simulation）**：题目末尾提到的仿真方法是另一种数值求解手段。当问题变得复杂（例如胜率不是 0.5，或者多人同时博弈规则改变）导致解析解难以推导时，可以通过计算机模拟成千上万次游戏过程，统计某人获胜的频率来近似概率。但在本题的理想数学模型下，解析解是精确的。

---

## 例题0.47

### 题目

讨论积分、事件概率与数学期望的关系，并给出仿真计算方法。

### 解答

**Revising and Refining**

I've been meticulously refining the structure of the solution. The plan now focuses on the core equivalence of probability, integrals, and expectations, framing it through the lens of measure theory and Monte Carlo methods, making sure to show how these concepts are used for simulation. The LaTeX syntax is all checked, too. I'm focusing on clarity and conciseness, ready to produce a response that directly addresses the problem.


### 题目分析
本题旨在探讨概率论与数值计算中的核心联系，具体考察积分（Integral）、事件概率（Probability of Event）与数学期望（Mathematical Expectation）三者在理论上的等价性，以及基于此原理的蒙特卡洛（Monte Carlo）仿真计算方法。核心概念涉及**示性函数（Indicator Function）**、**大数定律（Law of Large Numbers）**以及**均匀分布（Uniform Distribution）**。求解目标是建立三者之间的数学转化公式，并给出基于随机抽样的数值估计算法。

### 解题过程

#### 1. 事件概率与数学期望的关系

设 $(\Omega, \mathcal{F}, P)$ 为概率空间，考虑事件 $A \in \mathcal{F}$。引入事件 $A$ 的**示性函数** $\mathbb{I}_A(\omega)$，定义如下：
$$
\mathbb{I}_A(\omega) = \begin{cases} 
1, & \text{若 } \omega \in A \\
0, & \text{若 } \omega \notin A 
\end{cases}
$$
示性函数 $\mathbb{I}_A$ 是一个离散型随机变量，其分布律为：
$$P(\mathbb{I}_A = 1) = P(A), \quad P(\mathbb{I}_A = 0) = 1 - P(A)$$
根据数学期望的定义，$\mathbb{I}_A$ 的期望为：
$$
E[\mathbb{I}_A] = 1 \cdot P(\mathbb{I}_A = 1) + 0 \cdot P(\mathbb{I}_A = 0) = P(A)
$$
**结论**：事件 $A$ 发生的概率等价于其示性函数的数学期望。

#### 2. 积分与数学期望的关系

考虑计算定积分 $I = \int_a^b g(x) \, dx$。
为了将其转化为数学期望，引入服从区间 $[a, b]$ 上均匀分布的随机变量 $X \sim U(a, b)$，其概率密度函数（PDF）为：
$$
f_X(x) = \begin{cases} 
\frac{1}{b-a}, & a \leq x \leq b \\
0, & \text{其他}
\end{cases}
$$
我们可以将积分重写为：
$$
I = \int_a^b g(x) \, dx = (b-a) \int_a^b g(x) \cdot \frac{1}{b-a} \, dx
$$
观察上式，积分部分正是函数 $g(X)$ 关于概率密度 $f_X(x)$ 的加权平均。根据连续型随机变量期望的定义 $E[Y] = \int y f_Y(y) dy$（或 $E[g(X)] = \int g(x)f_X(x)dx$），可得：
$$
I = (b-a) E[g(X)]
$$
**结论**：定积分可以转化为某个随机变量函数的数学期望乘以积分区间的测度（长度、面积或体积）。

#### 3. 仿真计算方法（蒙特卡洛方法）

基于上述关系，利用**强大数定律（Strong Law of Large Numbers, SLLN）**，我们可以通过计算机仿真生成大量样本，用**样本均值**依概率收敛于**总体期望**的性质来近似计算概率和积分。

**算法步骤：**

1.  **生成样本**：
    在区间 $[a, b]$ 上独立同分布地（i.i.d.）生成 $N$ 个服从均匀分布的随机数 $X_1, X_2, \dots, X_N \sim U(a, b)$。

2.  **计算概率 $P(A)$ 的仿真估计**：
    设事件 $A$ 对应于区间 $[a, b]$ 的子集（或满足特定条件 $X \in A$）。
    构造统计量（频率）：
    $$
    \hat{P}_N(A) = \frac{1}{N} \sum_{i=1}^N \mathbb{I}_A(X_i) = \frac{\text{满足条件 } A \text{ 的样本数}}{N}
    $$
    当 $N \to \infty$ 时，$\hat{P}_N(A) \xrightarrow{a.s.} E[\mathbb{I}_A] = P(A)$。

3.  **计算积分 $I = \int_a^b g(x) dx$ 的仿真估计**：
    构造统计量（样本均值）：
    $$
    \hat{I}_N = (b-a) \cdot \frac{1}{N} \sum_{i=1}^N g(X_i)
    $$
    当 $N \to \infty$ 时，$\hat{I}_N \xrightarrow{a.s.} (b-a)E[g(X)] = I$。

### 答案

1.  **事件概率与期望的关系**：
    $$P(A) = E[\mathbb{I}_A]$$
2.  **积分与期望的关系**（设 $X \sim U(a,b)$）：
    $$\int_a^b g(x) dx = (b-a)E[g(X)]$$
3.  **仿真计算公式**（蒙特卡洛估计量）：
    $$\hat{I} \approx \frac{b-a}{N} \sum_{i=1}^N g(X_i)$$

### 知识延伸

1.  **核心定理**：本题的理论基础是**辛钦大数定律（Khinchin's Law of Large Numbers）**或**柯尔莫哥洛夫强大数定律**，保证了样本均值收敛于数学期望。
2.  **误差分析**：根据**中心极限定理（CLT）**，蒙特卡洛方法的收敛速度为 $O(1/\sqrt{N})$。这意味着要使精度提高 10 倍，样本量 $N$ 需要增加 100 倍。
3.  **高维优势**：传统的数值积分方法（如梯形法则、辛普森法则）在处理高维积分时会遇到“维数灾难”，而蒙特卡洛方法的收敛速度与维数无关，因此特别适合计算高维积分。
4.  **方差缩减技术**：为了提高仿真效率（即在相同 $N$ 下减小误差），可以使用对偶变量法（Antithetic Variates）、控制变量法（Control Variates）或重要性采样（Importance Sampling）等技巧。

---

## 例题0.48

### 题目

由均匀随机变量 $( 0 , ~ 1 )$ 产生随机变量 $X \sim F_{X} \left( x \right)$ 的一般方法。

### 解答

**Defining the Solution**

I've moved past problem analysis and am now drafting the solution steps. The Inverse Transform Theorem is the core. I'm carefully deriving the transformation and incorporating the generalized inverse to cover all cases. LaTeX formatting is being double-checked.


### 题目分析

本题考查的是随机变量生成的**逆变换法**（Inverse Transform Method），这是蒙特卡洛模拟和随机过程模拟中最基础且最重要的方法之一。题目要求利用一个服从 $(0, 1)$ 区间均匀分布的随机变量 $U$，构造出具有特定累积分布函数（CDF）$F_X(x)$ 的随机变量 $X$。核心在于利用分布函数的反函数（或广义反函数）进行变换。

### 解题过程

**1. 设定基础条件**

设 $U$ 是一个服从标准均匀分布的随机变量，即 $U \sim U(0, 1)$。其累积分布函数 $F_U(u)$ 为：
$$
F_U(u) = P(U \le u) = \begin{cases} 0, & u < 0 \\ u, & 0 \le u \le 1 \\ 1, & u > 1 \end{cases}
$$
我们需要寻找一个变换 $X = g(U)$，使得 $X$ 的累积分布函数为给定的 $F_X(x)$。

**2. 构造变换函数**

根据逆变换法的原理，我们取 $X$ 为 $F_X$ 的反函数作用于 $U$。
若 $F_X(x)$ 是严格单调递增且连续的函数，则其反函数 $F_X^{-1}(y)$ 存在且唯一。定义随机变量 $X$ 为：
$$
X = F_X^{-1}(U)
$$

**3. 证明变换后的分布**

我们需要验证上述定义的 $X$ 的累积分布函数确实是 $F_X(x)$。
对于任意实数 $x$，计算 $X \le x$ 的概率：
$$
P(X \le x) = P(F_X^{-1}(U) \le x)
$$
由于 $F_X$ 是单调递增函数，我们在不等式两边同时作用 $F_X$，不等号方向不变：
$$
P(F_X^{-1}(U) \le x) = P(U \le F_X(x))
$$
根据 $U \sim U(0, 1)$ 的性质，对于任意 $y \in [0, 1]$，有 $P(U \le y) = y$。
因为 $F_X(x)$ 是分布函数，其值域为 $[0, 1]$，所以令 $y = F_X(x)$，代入上式得：
$$
P(U \le F_X(x)) = F_X(x)
$$
综上所述：
$$
P(X \le x) = F_X(x)
$$
这证明了随机变量 $X = F_X^{-1}(U)$ 服从分布 $F_X(x)$。

**4. 推广情况（广义反函数）**

如果 $F_X(x)$ 不是严格单调递增（即存在平坦区域）或者不连续（即离散或混合分布），我们需要使用**广义反函数**。定义广义反函数 $F_X^{-1}(u)$ 为：
$$
F_X^{-1}(u) = \inf \{ x : F_X(x) \ge u \}, \quad 0 < u < 1
$$
利用性质 $F_X^{-1}(u) \le x \iff u \le F_X(x)$，上述推导过程依然成立：
$$
P(X \le x) = P(F_X^{-1}(U) \le x) = P(U \le F_X(x)) = F_X(x)
$$

### 答案

由均匀随机变量 $U \sim U(0, 1)$ 产生具有累积分布函数 $F_X(x)$ 的随机变量 $X$ 的一般方法是令：
$$
X = F_X^{-1}(U)
$$
其中 $F_X^{-1}$ 是 $F_X$ 的反函数（或广义反函数，定义为 $F_X^{-1}(u) = \inf \{ x : F_X(x) \ge u \}$）。

### 知识延伸

1.  **定理名称**：此方法称为**逆变换采样法**（Inverse Transform Sampling）或**概率积分变换**（Probability Integral Transform）。
2.  **适用范围**：
    *   适用于所有累积分布函数 $F_X(x)$ 已知且其反函数易于计算的情况。
    *   对于离散型随机变量，该方法等价于查找表法（将 $(0,1)$ 区间划分为对应概率大小的子区间）。
3.  **局限性与技巧**：
    *   如果 $F_X(x)$ 的反函数没有解析表达式（例如正态分布 $\Phi(x)$），则直接使用此方法计算成本较高，通常需要数值逼近或改用**Box-Muller变换**、**拒绝采样法**（Rejection Sampling）等其他方法。
4.  **易错点**：
    *   在定义广义反函数时，使用的是下确界（infimum），且不等号方向是 $F_X(x) \ge u$。
    *   对于离散变量，反函数对应的是阶梯函数的跳跃点。

---

## 例题0.49

### 题目

）用均匀硬币序列产生一个二元变量 $\mathit{X}$ ，使得 $P_{X} ( 1 )=\frac{1} {\sqrt{e}} , \, \, \, P_{X} ( 0 )=1-\frac{1} {\sqrt{e}}$ .

### 解答

### 题目分析

本题要求用均匀硬币（即每次抛掷正反面概率均为1/2）的序列来产生一个二元随机变量X，使其满足特定的概率分布：$P_X(1) = \frac{1}{\sqrt{e}}$，$P_X(0) = 1 - \frac{1}{\sqrt{e}}$。这是一个随机模拟问题，考查如何用简单的均匀随机源（硬币）来生成任意给定概率分布的随机变量。核心思想是将目标概率用二进制展开，通过硬币序列的结果来逼近该概率。

### 解题过程

**方法：二进制展开法**

将目标概率 $p = \frac{1}{\sqrt{e}}$ 用二进制小数展开：

$$p = \frac{1}{\sqrt{e}} = \sum_{i=1}^{\infty} b_i \cdot 2^{-i}$$

其中 $b_i \in \{0,1\}$ 是二进制位。

**算法步骤：**

1. 计算 $p = \frac{1}{\sqrt{e}} = e^{-1/2} \approx 0.6065306597$

2. 将 $p$ 转换为二进制：
   - $p \times 2 = 1.213...$，取整数部分 $b_1 = 1$，余 $0.213...$
   - $0.213 \times 2 = 0.426...$，$b_2 = 0$，余 $0.426...$
   - $0.426 \times 2 = 0.852...$，$b_3 = 0$，余 $0.852...$
   - $0.852 \times 2 = 1.704...$，$b_4 = 1$，余 $0.704...$
   - 继续此过程...
   
   得到 $p \approx (0.1001101...)_2$

3. **生成算法：**
   - 依次抛掷硬币，设第 $i$ 次结果为 $C_i$（正面为1，反面为0）
   - 从 $i=1$ 开始比较：
     - 如果 $C_i = 1$ 且 $b_i = 0$，则输出 $X = 0$（停止）
     - 如果 $C_i = 0$ 且 $b_i = 1$，则输出 $X = 1$（停止）
     - 如果 $C_i = b_i$，则继续抛下一次硬币
   
4. **正确性验证：**

   算法停止在第 $k$ 次的概率为：
   $$P(\text{停止在第}k\text{次}) = 2^{-k}$$
   
   输出 $X=1$ 当且仅当在某次 $C_i = 0$ 且 $b_i = 1$：
   $$P(X=1) = \sum_{i=1}^{\infty} P(C_1=b_1, \ldots, C_{i-1}=b_{i-1}, C_i=0, b_i=1)$$
   $$= \sum_{i: b_i=1} 2^{-i} = \sum_{i=1}^{\infty} b_i \cdot 2^{-i} = p = \frac{1}{\sqrt{e}}$$

   类似地，$P(X=0) = 1 - \frac{1}{\sqrt{e}}$

### 答案

**算法描述：**

1. 计算 $p = \frac{1}{\sqrt{e}}$ 的二进制展开 $(0.b_1b_2b_3...)_2$

2. 依次抛掷均匀硬币，得到序列 $C_1, C_2, C_3, ...$

3. 对于第 $i$ 次抛掷：
   - 若 $C_i \neq b_i$：输出 $X = b_i$，停止
   - 若 $C_i = b_i$：继续抛掷

该算法以概率 $\frac{1}{\sqrt{e}}$ 输出1，以概率 $1-\frac{1}{\sqrt{e}}$ 输出0。

### 知识延伸

**涉及的核心定理：**
- **二进制展开定理**：任何 $[0,1]$ 区间的实数都可以表示为二进制小数
- **几何分布性质**：算法的期望停止时间为2次抛掷

**重要技巧：**
- 用均匀分布生成任意离散分布的通用方法
- 对于有理概率，可用更简单的方法（如拒绝采样）
- 对于无理概率，二进制展开法是理论上精确的方法

**易错点：**
- 注意算法的停止条件是 $C_i \neq b_i$，而非其他条件
- 二进制展开可能是无限的，但算法几乎必然在有限步内停止
- 实际实现时需要足够精度计算二进制位

---

## 例题0.50

### 题目

产生 $n$ 维球内均匀分布的点。 0.5.7 概率应用

### 解答

**Formulating a Plan**

The analysis is progressing well, with the framework for the "Analysis" section firmly in place. I'm focusing on concisely presenting the problem's core requirements, key definitions like the n-ball, and the fundamental concepts needed to solve it. I'm aiming for clarity in describing the task.


### 题目分析

本题要求给出一个算法或数学构造，用于生成在 $n$ 维单位球体 $B_n = \{ \mathbf{x} \in \mathbb{R}^n : \|\mathbf{x}\| \le 1 \}$ 内部均匀分布的随机向量。
题目涉及的核心数学概念包括：
1.  **多元连续型随机变量的变换**：如何从简单的分布（如均匀分布、正态分布）构造复杂的联合分布。
2.  **球对称性（Isotropy）**：利用标准正态分布的旋转不变性生成均匀方向。
3.  **径向分布（Radial Distribution）**：推导均匀分布点到原点的距离所服从的概率分布。

本题的目标是找到一个随机向量 $\mathbf{X}$ 的构造表达式，使其概率密度函数在球内为常数，球外为 0。

### 解题过程

要在 $n$ 维单位球内产生均匀分布的点，我们可以将该问题分解为两个独立的步骤：
1.  产生一个在 $n$ 维单位球面 $S^{n-1}$ 上均匀分布的方向向量。
2.  产生一个符合特定分布的径向距离（半径），使得点在体积上的分布是均匀的。

#### 第一步：生成均匀方向

考虑 $n$ 个相互独立的标准正态随机变量 $Z_1, Z_2, \dots, Z_n$，即 $Z_i \sim N(0, 1)$。
构成的随机向量 $\mathbf{Z} = (Z_1, Z_2, \dots, Z_n)^T$ 的联合概率密度函数为：
$$
f_{\mathbf{Z}}(\mathbf{z}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-z_i^2/2} = (2\pi)^{-n/2} e^{-\frac{1}{2} \sum_{i=1}^n z_i^2} = (2\pi)^{-n/2} e^{-\frac{1}{2} \|\mathbf{z}\|^2}
$$
观察可知，该密度函数仅依赖于向量的模长 $\|\mathbf{z}\|$，而与方向无关。这意味着 $\mathbf{Z}$ 具有球对称性（旋转不变性）。

因此，归一化后的向量 $\mathbf{D}$ 在单位球面 $S^{n-1}$ 上是均匀分布的：
$$
\mathbf{D} = \frac{\mathbf{Z}}{\|\mathbf{Z}\|} = \frac{(Z_1, \dots, Z_n)^T}{\sqrt{Z_1^2 + \dots + Z_n^2}}
$$

#### 第二步：生成径向距离

设 $R$ 为球内均匀分布点到原点的距离（随机变量）。我们需要确定 $R$ 的分布。
对于 $n$ 维球体，半径为 $r$ 的球体积 $V_n(r)$ 与 $r^n$ 成正比，即 $V_n(r) = C_n r^n$，其中 $C_n$ 是与维数有关的常数。

若点在单位球内均匀分布，则其落在半径为 $r$ ($0 \le r \le 1$) 的小球内的概率 $P(R \le r)$ 应等于小球体积与单位球体积之比：
$$
F_R(r) = P(R \le r) = \frac{V_n(r)}{V_n(1)} = \frac{C_n r^n}{C_n 1^n} = r^n, \quad 0 \le r \le 1
$$
我们需要产生一个具有累积分布函数 $F_R(r) = r^n$ 的随机变量 $R$。
利用**逆变换采样法（Inverse Transform Sampling）**：
设 $U$ 为 $(0, 1)$ 上的均匀分布随机变量，即 $U \sim U(0, 1)$。令 $F_R(R) = U$，即：
$$
R^n = U \implies R = U^{1/n}
$$
注意：由于 $U$ 和 $1-U$ 同分布，有时也写作 $(1-U)^{1/n}$，但 $U^{1/n}$ 更为常用。

#### 第三步：合成最终向量

将均匀的方向向量 $\mathbf{D}$ 与正确的径向距离 $R$ 相乘，即可得到在 $n$ 维球内均匀分布的随机向量 $\mathbf{X}$：
$$
\mathbf{X} = R \cdot \mathbf{D} = U^{1/n} \cdot \frac{\mathbf{Z}}{\|\mathbf{Z}\|}
$$

**总结步骤如下**：
1.  生成 $n$ 个独立的标准正态分布随机数 $Z_1, \dots, Z_n \sim N(0, 1)$。
2.  生成 1 个均匀分布随机数 $U \sim U(0, 1)$。
3.  计算归一化因子 $\|\mathbf{Z}\| = \sqrt{\sum_{i=1}^n Z_i^2}$。
4.  计算最终坐标向量 $\mathbf{X} = (X_1, \dots, X_n)$，其中：
$$
X_i = \frac{Z_i}{\|\mathbf{Z}\|} \cdot U^{1/n}, \quad i=1, \dots, n
$$

### 答案

在 $n$ 维单位球内均匀分布的随机点 $\mathbf{X}$ 可通过以下公式生成：

$$
\mathbf{X} = U^{1/n} \frac{\mathbf{Z}}{\|\mathbf{Z}\|}
$$

其中：
*   $\mathbf{Z} = (Z_1, \dots, Z_n)^T$ 是由独立同分布的标准正态随机变量构成的向量，$Z_i \sim N(0, 1)$。
*   $U$ 是独立于 $\mathbf{Z}$ 的均匀分布随机变量，$U \sim U(0, 1)$。
*   $\|\mathbf{Z}\| = \sqrt{Z_1^2 + \dots + Z_n^2}$ 是欧几里得范数。

### 知识延伸

1.  **Muller 方法**：上述利用正态分布生成球面上均匀点的方法被称为 Muller 方法（或 Normal-deviate method）。这是生成高维球面均匀分布最通用的方法。
2.  **维度灾难与拒绝采样**：
    *   在低维（如 $n=2, 3$）时，可以使用**拒绝采样法**：在一个包围球体的超立方体内产生均匀点，若点在球外则丢弃。
    *   但在高维情况下，球体体积与外切超立方体体积之比 $\frac{V_{\text{ball}}}{V_{\text{cube}}} \to 0$ 当 $n \to \infty$。例如 $n=10$ 时，该比率约为 $0.0025$。这意味着拒绝采样法的效率会随维度增加呈指数级下降，因此必须使用本题所述的极坐标变换法。
3.  **常见易错点**：
    *   **错误做法**：直接令半径 $R \sim U(0, 1)$。
    *   **后果**：如果 $R$ 是均匀分布的，生成的点会聚集在球心附近。因为在 $n$ 维空间中，大部分体积都集中在表面附近（壳层），半径必须服从 $f_R(r) = n r^{n-1}$ 才能抵消体积效应。
4.  **相关公式**：
    *   $n$ 维球体体积：$V_n = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} R^n$。
    *   $n$ 维球面面积：$S_{n-1} = \frac{2\pi^{n/2}}{\Gamma(\frac{n}{2})} R^{n-1}$。

---

## 例题0.51

### 题目

设两个多项式 $f ( x ) , \, \, g ( x )$ ，编写一段程序计算乘积 $h ( x )=f ( x ) g ( x )$ 。给出一种测试所写程序正确与否的方法。

### 解答

**Considering Problem Details**

\n\n

I've clarified the solution approach. My focus now is on translating the algorithmic steps into LaTeX. I'll define polynomials, outline the convolution formula, and formalize the verification test using random evaluation, ensuring that the final output aligns perfectly with the prescribed format. I'm prioritizing clarity in mathematical notation.


### 题目分析

本题考查多项式的代数运算与随机算法（Randomized Algorithms）中的**多项式恒等测试（Polynomial Identity Testing）**。
题目包含两个部分：
1.  **计算**：实现多项式乘法，即求卷积。对于两个多项式 $f(x)$ 和 $g(x)$，其乘积 $h(x)$ 的系数由 $f$ 和 $g$ 系数的卷积给出。
2.  **验证**：设计一种方法测试结果的正确性。在概率论与随机过程的背景下，这通常指向利用随机化方法（蒙特卡洛方法）进行高效验证，而非重新计算一遍。核心原理基于代数基本定理：非零多项式的根的数量有限。

### 解题过程

#### 1. 多项式乘法程序的算法逻辑

设多项式 $f(x)$ 和 $g(x)$ 分别为：
$$f(x) = \sum_{i=0}^{n} a_i x^i, \quad g(x) = \sum_{j=0}^{m} b_j x^j$$
其中 $n$ 和 $m$ 分别为 $f(x)$ 和 $g(x)$ 的次数。

它们的乘积 $h(x) = f(x)g(x)$ 是一个次数为 $n+m$ 的多项式：
$$h(x) = \sum_{k=0}^{n+m} c_k x^k$$

其中，系数 $c_k$ 由卷积公式给出：
$$c_k = \sum_{i=0}^{k} a_i b_{k-i}$$
注意：在此求和中，若下标超出定义域（即 $i > n$ 或 $k-i > m$），则对应的 $a_i$ 或 $b_{k-i}$ 取为 $0$。

**程序逻辑（伪代码）：**
1.  初始化数组 $C$ 长度为 $n+m+1$，所有元素置为 $0$。
2.  对于 $i$ 从 $0$ 到 $n$：
    对于 $j$ 从 $0$ 到 $m$：
    $$C[i+j] \leftarrow C[i+j] + a_i \cdot b_j$$
3.  返回数组 $C$ 作为 $h(x)$ 的系数。

#### 2. 测试程序正确性的方法（随机化验证）

为了验证计算出的多项式 $h(x)$ 是否确实等于 $f(x)g(x)$，我们不应再次执行复杂的乘法运算（时间复杂度 $O(nm)$ 或 $O((n+m)\log(n+m))$），而应采用**随机化恒等测试**，其时间复杂度仅为 $O(n+m)$。

**理论基础：**
定义差值多项式 $D(x) = h(x) - f(x)g(x)$。
- 如果程序正确，则 $h(x) \equiv f(x)g(x)$，即 $D(x)$ 是零多项式，对任意 $x$ 都有 $D(x) = 0$。
- 如果程序错误，则 $D(x)$ 是一个非零多项式，其次数不超过 $n+m$。根据代数基本定理，一个次数为 $d$ 的非零多项式最多有 $d$ 个根。

**测试步骤：**
1.  从一个足够大的数域或整数集合 $S$ 中，**均匀随机**选取一个数值 $r$。为了保证错误率极低，集合的大小 $|S|$ 应远大于多项式的度数 $n+m$。
2.  分别计算 $v_1 = f(r)$， $v_2 = g(r)$ 和 $v_3 = h(r)$。计算多项式在某一点的值通常使用**秦九韶算法（Horner's Method）**，时间复杂度为 $O(n)$。
3.  检查是否满足 $v_3 = v_1 \cdot v_2$。
    - 若 $v_3 \neq v_1 \cdot v_2$，则程序**一定**有误。
    - 若 $v_3 = v_1 \cdot v_2$，则程序**以极高概率**正确。

**错误概率分析：**
假设程序计算错误（即 $D(x) \not\equiv 0$），测试通过（即 $D(r)=0$）的概率为：
$$P(\text{Error not detected}) = P(D(r) = 0) \le \frac{\deg(D)}{|S|} \le \frac{n+m}{|S|}$$
只要选取足够大的 $|S|$（例如 $|S| \ge 100(n+m)$），错误概率将小于 $1\%$。通过多次独立重复测试，错误概率可呈指数级下降。

### 答案

**1. 乘积计算公式：**
$$h(x) = \sum_{k=0}^{n+m} \left( \sum_{i=0}^{k} a_i b_{k-i} \right) x^k$$

**2. 测试方法：**
采用**随机化恒等测试**。
从大集合 $S$ 中随机选取 $r$，计算并验证：
$$h(r) \stackrel{?}{=} f(r) \cdot g(r)$$
若等式不成立，则程序错误；若等式成立，则程序以概率 $1 - \frac{n+m}{|S|}$ 正确。

### 知识延伸

1.  **Schwartz-Zippel 引理**：本题的测试方法是 Schwartz-Zippel 引理在单变量多项式中的特例。该引理是随机化算法领域的基石，用于判断多元多项式是否恒为零。
2.  **蒙特卡洛算法（Monte Carlo Algorithm）**：这是一种具有**单边错误（One-sided Error）**的随机算法。
    - 如果程序实际上是错的，算法有可能（以小概率）误报为“正确”。
    - 如果程序实际上是对的，算法永远报告“正确”。
3.  **计算复杂度优势**：
    - 朴素多项式乘法复杂度：$O(n^2)$。
    - 快速傅里叶变换（FFT）乘法复杂度：$O(n \log n)$。
    - 随机化验证复杂度：$O(n)$。
    验证比求解快得多，这是随机化验证在工程和理论中的核心价值（例如 Freivalds 算法用于验证矩阵乘法）。
4.  **易错点**：在实际编程实现测试时，若 $f(r)g(r)$ 数值过大导致溢出，通常需要在模大素数域（Finite Field $\mathbb{F}_p$）下进行运算。

---

## 例题0.52

### 题目

设 $m , ~ n$ 是两个大整数。一个函数 $F : \{0 , \: \: 1 , \: \: . . . , \: \: n-1 \} \to\{0 , \: \: 1 , \: \: . . . , \: \: m-1 \}$ 满足 $F ( ( x+$ $y ) \mathrm{m o d} \ n \mathrm{)}=\left( F ( x )+F ( y ) \right)$ mod $m$ 。该函数可以查表计算，即 $F ( x ) , \; \, 0 \leq x \leq n-1$ 是已知的，但被随机修改了，设错的位置占比1/5。设计一种算法，可靠计算 $F ( x ...

### 解答

**Generating the Response**

I'm now implementing the described steps. The "Analysis," "Solution Process," "Answer," and "Knowledge Extension" sections are being constructed using LaTeX and will accurately reflect the established guidelines for a senior-level mathematical response. I'm focusing on clarity and correctness in the exposition of the probabilistic arguments. I am now proceeding to implement the final output in Chinese.


### 题目分析

本题考察的是**随机算法**中的**自校正（Self-Correcting）**技术，具体涉及**群同态（Group Homomorphism）**的性质利用。
题目给出的函数 $F$ 满足 $F((x + y) \pmod n) = (F(x) + F(y)) \pmod m$，说明 $F$ 是从加法群 $\mathbb{Z}_n$ 到 $\mathbb{Z}_m$ 的同态（即线性函数）。
核心问题在于如何利用函数的**随机自归约性（Random Self-Reducibility）**，通过查询带有噪声（错误率为 $1/5$）的函数表，以高概率恢复出任意给定点 $x$ 的正确函数值。这属于概率论在理论计算机科学中的典型应用。

### 解题过程

**1. 利用同态性质构造随机化计算公式**

根据题目条件，函数 $F$ 满足同态性质：
$$F((a + b) \pmod n) = (F(a) + F(b)) \pmod m$$
这意味着对于任意的 $x \in \{0, \dots, n-1\}$ 和任意的 $r \in \{0, \dots, n-1\}$，我们有：
$$F((x + r) \pmod n) = (F(x) + F(r)) \pmod m$$
移项可得计算 $F(x)$ 的公式：
$$F(x) = (F((x + r) \pmod n) - F(r)) \pmod m$$

**2. 设计随机化算法**

设 $\tilde{F}$ 为已被修改的查表函数（即包含错误的函数表）。已知对于任意 $z$，$\tilde{F}(z) = F(z)$ 的概率为 $1 - 1/5 = 4/5$。
为了计算特定输入 $x$ 的值 $F(x)$，我们设计如下算法：

1.  在 $\{0, 1, \dots, n-1\}$ 中均匀随机地选取一个整数 $r$。
2.  计算 $y = (x + r) \pmod n$。
3.  查询函数表，得到 $\tilde{F}(y)$ 和 $\tilde{F}(r)$。
4.  计算估计值：
    $$V = (\tilde{F}(y) - \tilde{F}(r)) \pmod m$$
5.  输出 $V$ 作为 $F(x)$ 的结果。

**3. 算法可靠性分析**

我们需要计算上述算法输出正确结果的概率，即 $P(V = F(x))$。
算法输出正确的前提是查询的两个点 $y$ 和 $r$ 的函数表值都是正确的。

定义事件 $E_1$ 为 $\tilde{F}(r) = F(r)$，定义事件 $E_2$ 为 $\tilde{F}(y) = F(y)$。
根据题目：
$$P(E_1) = \frac{4}{5}, \quad P(E_1^c) = \frac{1}{5}$$
由于 $r$ 是均匀随机选取的，且 $x$ 是固定的，那么 $y = (x + r) \pmod n$ 在 $\{0, \dots, n-1\}$ 上也是均匀分布的。因此：
$$P(E_2) = \frac{4}{5}, \quad P(E_2^c) = \frac{1}{5}$$

我们需要求 $P(E_1 \cap E_2)$。利用布尔不等式（Union Bound）的补集形式：
$$P(E_1 \cap E_2) = 1 - P(E_1^c \cup E_2^c)$$
$$P(E_1 \cap E_2) \ge 1 - (P(E_1^c) + P(E_2^c))$$
代入数值：
$$P(E_1 \cap E_2) \ge 1 - \left(\frac{1}{5} + \frac{1}{5}\right) = 1 - \frac{2}{5} = \frac{3}{5}$$

**4. 提升可靠性（多数投票法）**

单次采样的正确率至少为 $3/5 = 0.6$，这已经大于 $1/2$。为了获得极高的可靠性，可以独立重复上述过程 $k$ 次，得到 $k$ 个估计值，然后取出现次数最多的值（众数）作为最终结果。根据切诺夫界（Chernoff Bound），随着 $k$ 的增加，错误概率将呈指数级下降。

### 答案

对于任意 $x$，算法如下：均匀随机选取 $r \in \{0, \dots, n-1\}$，计算
$$ \text{Result} = (\tilde{F}((x+r) \pmod n) - \tilde{F}(r)) \pmod m $$
该算法单次计算得到正确 $F(x)$ 的概率 $P \ge \frac{3}{5}$。若需更高可靠性，可重复多次并取众数。

### 知识延伸

1.  **随机自归约性 (Random Self-Reducibility)**：本题利用了线性函数可以将对特定点 $x$ 的求值转化为对两个均匀随机分布点 $x+r$ 和 $r$ 的求值。这种性质使得最坏情况下的查询（Worst-case）可以转化为平均情况下的查询（Average-case）。
2.  **自校正程序 (Self-Correcting Programs)**：这是 Blum, Luby, Rubinfeld 等人提出的程序检查理论的基础。只要错误率低于某个阈值（对于线性函数通常是 $1/4$ 或 $1/3$，取决于具体分析），就可以通过随机化方法以任意高概率修正错误。
3.  **布尔不等式 (Boole's Inequality / Union Bound)**：在概率推导中，用于估计多个事件并集发生的概率上界，即 $P(\cup A_i) \le \sum P(A_i)$。本题中用于估算“至少有一个查询出错”的概率上界。
4.  **易错点**：直接查询 $\tilde{F}(x)$ 的错误率是固定的 $1/5$，无法通过重复查询来降低（因为表是固定的，对同一个 $x$ 查询多次结果一样）。必须引入随机变量 $r$ 来“平滑”查询位置，使得查询点均匀分布在整个定义域上，从而利用统计规律。

---

## 习题0.1

### 题目

从 [1，1 000 000] 范围中随机抽取一个数。请运用容斥定理计算这个数能被 4，6 和 9 中一个或多个整除 的概率。

### 解答

**Finalizing the Calculation**

I'm now carefully integrating the LaTeX code into the Chinese solution and polishing the "Knowledge Extension" section. Also, I’ve verified all calculations, paying close attention to the floor function's behavior. I will stick to the standard range of one million as the prompt contained what seems like a typo, and this is a standard probability problem.


### 题目分析

本题考查古典概型中的**容斥定理（Principle of Inclusion-Exclusion）**以及初等数论中的**最小公倍数（LCM）**概念。
==容斥定理求并集大小==

题目要求从集合 $\{1, 2, \dots, N\}$ 中随机抽取一个数，其中 $N=1,000,000$（根据题目语境“1 1100000”推断为1到1,000,000的范围），计算该数能被4、6或9整除的概率。

核心步骤包括：
1.  定义事件：设 $A, B, C$ 分别为被4，6，9整除的事件。
2.  计算各事件及其交集的基数：利用取整函数 $\lfloor \frac{N}{k} \rfloor$ 计算 $1$ 到 $N$ 中能被 $k$ 整除的整数个数。
3.  应用容斥定理求并集基数 $|A \cup B \cup C|$。
4.  计算概率：目标基数除以总样本空间基数。

### 解题过程

设样本空间为 $\Omega = \{1, 2, \dots, 1000000\}$，样本总数 $|\Omega| = 1,000,000$。

定义以下事件：
- $A$：选出的数能被 $4$ 整除。
- $B$：选出的数能被 $6$ 整除。
- $C$：选出的数能被 $9$ 整除。

我们需要求解的是事件 $A \cup B \cup C$ 的概率，即 $P(A \cup B \cup C)$。

根据**容斥定理**，事件 $A \cup B \cup C$ 中元素的个数为：
$$|A \cup B \cup C| = |A| + |B| + |C| - (|A \cap B| + |A \cap C| + |B \cap C|) + |A \cap B \cap C|$$

**第一步：计算单个集合的基数**

在 $1$ 到 $N$ 的范围内，能被 $k$ 整除的数的个数为 $\lfloor \frac{N}{k} \rfloor$。

1.  能被 $4$ 整除的数：
    $$|A| = \left\lfloor \frac{1000000}{4} \right\rfloor = 250,000$$

2.  能被 $6$ 整除的数：
    $$|B| = \left\lfloor \frac{1000000}{6} \right\rfloor = 166,666$$

3.  能被 $9$ 整除的数：
    $$|C| = \left\lfloor \frac{1000000}{9} \right\rfloor = 111,111$$

**第二步：计算两两交集的基数**

两个集合的交集意味着该数能同时被两个数整除，即被它们的**最小公倍数（LCM）**整除。

1.  $A \cap B$：能被 $\text{lcm}(4, 6) = 12$ 整除：
    $$|A \cap B| = \left\lfloor \frac{1000000}{12} \right\rfloor = 83,333$$

2.  $A \cap C$：能被 $\text{lcm}(4, 9) = 36$ 整除：
    $$|A \cap C| = \left\lfloor \frac{1000000}{36} \right\rfloor = 27,777$$

3.  $B \cap C$：能被 $\text{lcm}(6, 9) = 18$ 整除：
    $$|B \cap C| = \left\lfloor \frac{1000000}{18} \right\rfloor = 55,555$$

**第三步：计算三个集合交集的基数**

$A \cap B \cap C$：能被 $\text{lcm}(4, 6, 9)$ 整除。
由于 $4=2^2, 6=2\times3, 9=3^2$，则 $\text{lcm}(4, 6, 9) = 2^2 \times 3^2 = 36$。

$$|A \cap B \cap C| = \left\lfloor \frac{1000000}{36} \right\rfloor = 27,777$$

**第四步：代入容斥定理公式**

$$
\begin{aligned}
|A \cup B \cup C| &= (250,000 + 166,666 + 111,111) \\
&\quad - (83,333 + 27,777 + 55,555) \\
&\quad + 27,777
\end{aligned}
$$

计算各项和：
- 单个集合之和：$250,000 + 166,666 + 111,111 = 527,777$
- 两两交集之和：$83,333 + 27,777 + 55,555 = 166,665$
- 三个交集：$27,777$

代入计算：
$$|A \cup B \cup C| = 527,777 - 166,665 + 27,777 = 388,889$$

**第五步：计算概率**

$$P(A \cup B \cup C) = \frac{|A \cup B \cup C|}{|\Omega|} = \frac{388,889}{1,000,000}$$

### 答案

该数能被4，6和9中一个或多个整除的概率为：
$$0.388889$$

### 知识延伸

1.  **容斥定理（Inclusion-Exclusion Principle）**：
    用于计算有限个集合之并集的元素个数。对于三个集合，公式为：
    $$P(A \cup B \cup C) = \sum P(A_i) - \sum P(A_i \cap A_j) + P(A_i \cap A_j \cap A_k)$$
    该定理是概率论和组合数学中处理“至少发生一个”类问题的基础工具。

2.  **整除性与最小公倍数**：
    在处理整除问题时，事件“同时被 $a$ 和 $b$ 整除”等价于“被 $\text{lcm}(a, b)$ 整除”。
    **易错点**：直接使用乘积 $a \times b$ 而非 $\text{lcm}(a, b)$。例如本题中 $A \cap B$ 是被12整除，而非24。

3.  **取整函数（Floor Function）**：
    计算 $1$ 到 $N$ 中 $k$ 的倍数个数时，严格公式为 $\lfloor N/k \rfloor$。在 $N$ 很大时，可以直接用 $N/k$ 近似，但在精确计数问题中必须保留整数部分。

---

## 习题0.2

### 题目

有一枚均匀的硬币和一枚两面都是头像（正面）的硬币，以相同概率从这两枚硬币中随机选择一枚并投 掷。已知投掷结果是出现正面，那么投掷的是两面均是头像的硬币的概率是多少。

### 解答

**Thinking Through the Solution**

\n\n

I've just finished outlining the "解题过程" section, meticulously breaking down each step. I've now meticulously identified the probabilities and built up towards the application of Bayes' Theorem. Next up, I will construct the "答案" section and "知识延伸". I'm making sure my LaTeX formatting is correct and the logic is easy to follow.


### 题目分析

本题描述了一个典型的条件概率问题。题目中涉及两个互斥的随机事件（选择哪一枚硬币）以及一个观测事件（投掷结果为正面）。本题考查的核心概念是**贝叶斯公式（Bayes' Theorem）**和**全概率公式（Law of Total Probability）**。我们需要求解的目标是在已知观测结果为正面的条件下，反推所选硬币为两面均是头像的硬币的后验概率。

### 解题过程

首先，我们需要定义相关的随机事件。

设 $C_1$ 表示“选到的是均匀硬币”这一事件。
设 $C_2$ 表示“选到的是两面均是头像的硬币”这一事件。
设 $H$ 表示“投掷结果出现正面（Head）”这一事件。

根据题目描述，我们已知以下先验概率：
由于是以相同概率随机选择一枚硬币，故：
$$P(C_1) = 0.5, \quad P(C_2) = 0.5$$

接下来，确定在给定硬币类型下的条件概率（似然概率）：
对于均匀硬币，出现正面的概率为 $0.5$：
$$P(H | C_1) = 0.5$$
对于两面均是头像的硬币，出现正面的概率为 $1$：
$$P(H | C_2) = 1$$

我们需要求解的是在已知出现正面的条件下，该硬币是两面均是头像硬币的概率，即 $P(C_2 | H)$。

根据**贝叶斯公式**：
$$P(C_2 | H) = \frac{P(H | C_2)P(C_2)}{P(H)}$$

其中分母 $P(H)$ 需要使用**全概率公式**进行计算。$C_1$ 和 $C_2$ 构成了样本空间的一个划分，因此：
$$P(H) = P(H | C_1)P(C_1) + P(H | C_2)P(C_2)$$

将已知数值代入全概率公式计算 $P(H)$：
$$
\begin{aligned}
P(H) &= 0.5 \times 0.5 + 1 \times 0.5 \\
&= 0.25 + 0.5 \\
&= 0.75
\end{aligned}
$$
即出现正面的总概率为 $0.75$（或 $\frac{3}{4}$）。

最后，计算后验概率 $P(C_2 | H)$：
$$
\begin{aligned}
P(C_2 | H) &= \frac{1 \times 0.5}{0.75} \\
&= \frac{0.5}{0.75} \\
&= \frac{2}{3}
\end{aligned}
$$

### 答案

$$ \frac{2}{3} $$

### 知识延伸

本题是贝叶斯推断的入门级经典案例。

1.  **核心定理**：
    *   **全概率公式**：用于计算复杂事件的总概率，公式为 $P(B) = \sum_{i} P(B|A_i)P(A_i)$，其中 $\{A_i\}$ 为样本空间的一个划分。
    *   **贝叶斯公式**：用于根据新的观测证据更新对假设的信念（概率），公式为 $P(A_k|B) = \frac{P(B|A_k)P(A_k)}{\sum_{i} P(B|A_i)P(A_i)}$。

2.  **解题技巧**：
    *   清晰地定义事件是解题的第一步。
    *   区分**先验概率**（观测前，如选硬币的概率 $0.5$）和**后验概率**（观测后，如本题结果 $\frac{2}{3}$）。
    *   识别“完备事件组”（Partition），即所有可能的假设必须互斥且概率之和为1。

3.  **易错点**：
    *   直觉误判：初学者常直觉认为既然选硬币概率均等，结果也是正面，那么是哪枚硬币的概率应该还是 $0.5$。这是忽略了“两面均是头像的硬币更容易产生正面”这一似然信息，从而导致后验概率向该硬币倾斜。
    *   混淆 $P(A|B)$ 与 $P(B|A)$。

---

## 习题0.3

### 题目

连续地抛掷一枚均匀的硬币。 （1）求抛掷的前四次是下列情况的概率： H, H, H, H. T，H, H, H. (2）求模式T，H，H，H出现在模式H，H，H，H之前的概率。

### 解答

**Refining the Solution**

The core logic is finalized; the probability calculations for both parts are clear, and the elegant structure-based reasoning for Part 2 is solidified. Now, I'm verifying all assumptions to prevent errors in the final response, with a focus on formatting and ensuring it meets all the specified constraints. I'm double-checking the LaTeX syntax for correctness.


### 题目分析

本题主要考查概率论中的**独立重复试验（伯努利试验）**以及**随机过程中的模式匹配（Pattern Matching）**问题。

1.  **第一问**：考查基础的独立事件概率计算。由于硬币是均匀的，每次抛掷结果相互独立且概率相等。
2.  **第二问**：考查两个特定模式（Pattern）出现的先后顺序概率。这是一个经典的“赌徒破产问题”变体或“模式竞争”问题。虽然可以使用马尔可夫链（Markov Chain）或鞅（Martingale）方法求解，但对于本题特定的两个模式，通过分析模式结构之间的包含关系，存在更简便的逻辑推导方法。

需要求解的目标是：
1.  前四次抛掷恰好是特定序列的概率。
2.  模式 $T, H, H, H$ 先于模式 $H, H, H, H$ 出现的概率。

---

## 习题0.4

### 题目

甲乙两人比赛，规定只要中间有一人赢得了 $n$ 局，比赛立即结束。假定比赛在两人间公平进行，即每人赢得一局比赛的概率都是1/2,与其他不同局的结果无关.那么比赛结束时，失败一方已经赢得 $k$ 局的概率是多少？ 5.投掷10枚标准的六面体骰子，假定投掷每枚骰子是独立的。它们的点数之和能被6整除的概率是多少？ 6.对 $n$ 个人进行核酸检测。每个人可以单独检测，但费用过高。合并检测可以减少费用。把 $k$ 个人的样本合起来同时分析，如果检测结果呈阴性，对这 $k$ 个人的组，这一次检测就好了。如果检测结果呈阳性，则这 $k$ 个人需要再进行单独检测，因此这 $k$ 个人需要进行 $k+1$ 次...

### 解答

**Reviewing and Refining Formulas**

I'm now meticulously reviewing the formulas and notation, especially for Problem 0.4 and Problem 6. I'm focusing on ensuring consistent notation and clarity within the LaTeX environment, double-checking the indexing, and simplifying expressions where possible for a cleaner presentation. I also am making minor adjustments in language for added precision.


### 题目分析

本组习题包含三个独立的概率问题，分别涉及以下核心概念：

1.  **习题 0.4（比赛问题）**：考查**负二项分布（Negative Binomial Distribution）**或**巴斯卡分布**的变体。核心在于理解在独立重复试验（伯努利试验）中，达到特定成功次数所需的试验次数分布，并结合对称性进行求解。
2.  **习题 5（骰子整除问题）**：考查随机变量和的模运算性质。核心在于利用**独立性**和**条件概率**（或生成函数/单位根性质）来简化计算。
3.  **习题 6（分组检测问题）**：考查**期望值的线性性质**以及基于概率模型的**最优化问题**。这是经典的“多尔夫曼分组检测（Dorfman Group Testing）”模型。

需要求解的目标分别为：特定比赛结果的概率、骰子点数和被6整除的概率、分组检测的阳性概率、期望次数、最优分组策略及分组检测优于单独检测的条件。

---

## 习题0.5

### 题目

令 $N=( N ( t ) , ~ t \geq0 )$ 是参数为2 的泊松过程，求： (1) $P ( N ( 1 )=2 )$ (2) $P ( N ( 1 )=2 , \, \, \, N ( 3 )=6 )$ (3) $P ( N ( 1 )=2 | N ( 3 )=6 )$ 1

### 解答

**Reviewing the Calculations**

I've meticulously reviewed the calculations, specifically the probabilities involved in question 2 and 3. Both the direct calculation using independent increments and the conditional probability using the definition and binomial distribution methods have been cross-checked, and the results are consistent. This confirms the accuracy of the final answers. Now, all that remains is to format them properly.


### 题目分析

本题给定了一个参数为 $\lambda=2$ 的泊松过程 $N=(N(t), t \geq 0)$。题目要求计算三个概率值：(1) 特定时刻状态的边际概率；(2) 两个不同时刻状态的联合概率；(3) 给定将来时刻状态下，过去时刻状态的条件概率。

本题考查的核心概念包括：
1.  **泊松过程的定义与一维概率分布**：$N(t)$ 服从参数为 $\lambda t$ 的泊松分布。
2.  **独立增量性**：不相交时间区间内的增量是相互独立的。
3.  **平稳增量性**：增量的分布仅依赖于时间差。
4.  **条件概率**：利用联合概率与边缘概率计算，或利用泊松过程的条件分布性质（二项分布）。

### 解题过程

已知泊松过程的参数 $\lambda = 2$。根据定义，对于任意 $t > 0$，随机变量 $N(t)$ 服从参数为 $\lambda t = 2t$ 的泊松分布，即：
$$P(N(t)=k) = \frac{e^{-2t}(2t)^k}{k!}, \quad k=0,1,2,\dots$$

#### (1) 求解 $P(N(1)=2)$

这里 $t=1$，参数为 $\lambda \times 1 = 2$。代入泊松分布公式：
$$
\begin{aligned}
P(N(1)=2) &= \frac{e^{-2(1)} (2 \times 1)^2}{2!} \\
&= \frac{e^{-2} \cdot 4}{2} \\
&= 2e^{-2}
\end{aligned}
$$

#### (2) 求解 $P(N(1)=2, N(3)=6)$

利用泊松过程的**独立增量性**。事件 $\{N(1)=2, N(3)=6\}$ 等价于 $\{N(1)=2\}$ 且 $\{N(3)-N(1) = 6-2 = 4\}$。
由于 $0 < 1 < 3$，增量 $N(1)-N(0)$ 与 $N(3)-N(1)$ 相互独立。

1.  $N(1) \sim \text{Poisson}(2 \times 1) = \text{Poisson}(2)$
2.  $N(3)-N(1) \sim \text{Poisson}(2 \times (3-1)) = \text{Poisson}(4)$

计算如下：
$$
\begin{aligned}
P(N(1)=2, N(3)=6) &= P(N(1)=2, N(3)-N(1)=4) \\
&= P(N(1)=2) \cdot P(N(3)-N(1)=4) \\
&= \left( \frac{e^{-2} 2^2}{2!} \right) \cdot \left( \frac{e^{-4} 4^4}{4!} \right) \\
&= (2e^{-2}) \cdot \left( \frac{e^{-4} \cdot 256}{24} \right) \\
&= 2e^{-2} \cdot \frac{32e^{-4}}{3} \\
&= \frac{64}{3}e^{-6}
\end{aligned}
$$

#### (3) 求解 $P(N(1)=2 | N(3)=6)$

根据条件概率的定义：
$$P(A|B) = \frac{P(AB)}{P(B)}$$
其中 $A = \{N(1)=2\}$，$B = \{N(3)=6\}$。

**方法一：直接计算法**
分子已在第(2)问中求得：$P(N(1)=2, N(3)=6) = \frac{64}{3}e^{-6}$。
分母为 $N(3)$ 的边缘概率，此时 $t=3$，参数为 $\lambda \times 3 = 6$：
$$
P(N(3)=6) = \frac{e^{-6} 6^6}{6!} = \frac{e^{-6} \cdot 46656}{720} = \frac{324}{5}e^{-6}
$$
代入公式：
$$
\begin{aligned}
P(N(1)=2 | N(3)=6) &= \frac{\frac{64}{3}e^{-6}}{\frac{324}{5}e^{-6}} \\
&= \frac{64}{3} \cdot \frac{5}{324} \\
&= \frac{16 \cdot 5}{3 \cdot 81} \\
&= \frac{80}{243}
\end{aligned}
$$

**方法二：利用二项分布性质（推荐验证用）**
对于泊松过程，给定 $N(t)=n$，则 $s < t$ 时刻的 $N(s)$ 服从二项分布 $B(n, \frac{s}{t})$。
本题中 $s=1, t=3, n=6$，成功概率 $p = \frac{1}{3}$。
$$
\begin{aligned}
P(N(1)=2 | N(3)=6) &= \binom{6}{2} \left(\frac{1}{3}\right)^2 \left(1-\frac{1}{3}\right)^{6-2} \\
&= 15 \cdot \frac{1}{9} \cdot \left(\frac{2}{3}\right)^4 \\
&= \frac{5}{3} \cdot \frac{16}{81} \\
&= \frac{80}{243}
\end{aligned}
$$

### 答案

(1) $P(N(1)=2) = 2e^{-2}$

(2) $P(N(1)=2, N(3)=6) = \frac{64}{3}e^{-6}$

(3) $P(N(1)=2 | N(3)=6) = \frac{80}{243}$

### 知识延伸

1.  **泊松过程的核心性质**：
    *   **独立增量**：$N(t_2)-N(t_1) \perp N(t_1)-N(t_0)$。
    *   **平稳增量**：$N(t+s)-N(s) \sim \text{Poisson}(\lambda t)$。

2.  **重要公式**：
    *   泊松分布概率质量函数：$P(X=k) = \frac{e^{-\mu}\mu^k}{k!}$。
    *   条件分布性质：若 $s < t$，则 $(N(s) | N(t)=n) \sim B(n, \frac{s}{t})$。这是一个非常高效的解题技巧，能避免繁琐的指数运算。

3.  **易错点**：
    *   在计算联合概率时，直接将 $N(t_1)$ 和 $N(t_2)$ 视为独立变量是错误的，必须转化为增量形式 $N(t_2)-N(t_1)$ 才能利用独立性。
    *   计算阶乘和指数幂时容易出现算术错误，建议化简分数后再计算。

---

## 习题0.6

### 题目

对 n 个人进行核酸检测。每个人可以单独检测，但费用过高。合并检测可以减少费用。把 k 个人的样本 合起来同时分析，如果检测结果呈阴性，对这k 个人的组，这一次检测就好了。如果检测结果呈阳性，则这k 个 人需要再进行单独检测，因此这 k 个人需要进行 k + 1 次检测。假定我们产生了 n/k 个不同的组，每组 k 个人 （k 能整除n） ，并用合并法进行检测。假设对于独立检测，每个人呈阳性的概率为p。 （1）对 k 个人的合并样本，检测呈阳性的概率是多少？ （2）需要检测的期望次数是多少？ （3）描述如何求最优的k 值。 （4）给出一个不等式，说明对什么样的 p 值，合并检测比每个人单独检测更好。

### 解答

**Refining Calculations and Format**

I've checked the calculations and confirmed the logic for each part of the problem. LaTeX syntax has been reviewed, and I'm satisfied with the formatting, ensuring it meets all the requirements. Preparing to finalize the "Answer" section. I will be sure to review it again.


### 题目分析

本题考查 **泊松过程（Poisson Process）** 的基本定义及其核心性质。
题目给定了一个参数 $\lambda = 2$ 的泊松过程 $N = (N(t), t \geq 0)$。
主要涉及的知识点包括：
1.  **泊松分布公式**：任意时刻 $t$，随机变量 $N(t)$ 服从参数为 $\lambda t$ 的泊松分布。
2.  **独立增量性**：不重叠的时间区间内的增量是相互独立的。
3.  **平稳增量性**：时间区间 $(s, s+t]$ 内的增量 $N(s+t) - N(s)$ 的分布仅依赖于时间长度 $t$，且与 $N(t)$ 同分布。
4.  **条件概率**：利用集合关系求解条件概率。

---

## 习题0.7

### 题目

设 $A$ 、 $B$ 是两个事件。证明如下示性变量之间的关系并回答有关问题。 (1) $\mathbb{I}_{\Omega}=1 , \, \, \, \mathbb{I}_{\phi}=0$ ; (2) $\mathbb{I}_{\bar{A}}=1-\mathbb{I}_{A}$ ; (3) $\mathbb{I}_{A \cup B}=\operatorname* {m a x} ( \mathbb{I}_{A} , \mathbb{I}_{B} ) , \, \, \, \mathbb{I}_{A B}=\operatorname* {m i n} ( \mathbb{I}_{A}...

### 解答

**Reviewing Proof Construction**

I'm now revisiting Part 3 of the "解题过程" section, specifically the proofs regarding the union and intersection of events. I am carefully constructing the proofs using all relevant cases to solidify their validity. This methodical approach will ensure the solution's mathematical rigor.


### 题目分析

本题主要考查**示性变量（Indicator Variable）**（或称示性函数）的基本定义及其代数性质。示性变量 $\mathbb{I}_A$ 是概率论中将集合运算转化为代数运算的重要工具。
题目要求证明示性变量与基本事件（全集、空集）、补集、并集、交集的关系，并解释模2加法和乘法对应的集合意义。
核心概念包括：
1.  示性函数的定义：$\mathbb{I}_A(\omega) = 1$ 当 $\omega \in A$，否则为 $0$。
2.  集合运算与代数运算的对应关系。
3.  对称差（Symmetric Difference）的概念。

### 解题过程

设样本空间为 $\Omega$，对于任意事件 $E$，其示性变量定义为：
$$
\mathbb{I}_{E}(\omega) = \begin{cases} 1, & \omega \in E \\ 0, & \omega \notin E \end{cases}
$$

**(1) 证明 $\mathbb{I}_{\Omega}=1 , \, \mathbb{I}_{\phi}=0$**

*   对于 $\mathbb{I}_{\Omega}$：
    由于 $\Omega$ 是样本空间，对于任意样本点 $\omega$，都有 $\omega \in \Omega$。
    根据定义，$\mathbb{I}_{\Omega}(\omega) = 1$ 对一切 $\omega$ 成立。
    故 $\mathbb{I}_{\Omega} \equiv 1$（常数函数 1）。

*   对于 $\mathbb{I}_{\phi}$：
    由于 $\phi$ 是空集，不包含任何样本点，即对于任意 $\omega$，都有 $\omega \notin \phi$。
    根据定义，$\mathbb{I}_{\phi}(\omega) = 0$ 对一切 $\omega$ 成立。
    故 $\mathbb{I}_{\phi} \equiv 0$（常数函数 0）。

**(2) 证明 $\mathbb{I}_{\bar{A}}=1-\mathbb{I}_{A}$**

对于任意 $\omega \in \Omega$，分两种情况讨论：
1.  若 $\omega \in A$，则 $\mathbb{I}_{A}(\omega) = 1$。此时 $\omega \notin \bar{A}$，故 $\mathbb{I}_{\bar{A}}(\omega) = 0$。
    等式右边：$1 - \mathbb{I}_{A}(\omega) = 1 - 1 = 0$。左边等于右边。
2.  若 $\omega \notin A$，则 $\mathbb{I}_{A}(\omega) = 0$。此时 $\omega \in \bar{A}$，故 $\mathbb{I}_{\bar{A}}(\omega) = 1$。
    等式右边：$1 - \mathbb{I}_{A}(\omega) = 1 - 0 = 1$。左边等于右边。

综上，$\mathbb{I}_{\bar{A}}=1-\mathbb{I}_{A}$ 成立。

**(3) 证明 $\mathbb{I}_{A \cup B}=\operatorname{max} ( \mathbb{I}_{A} , \mathbb{I}_{B} )$ 与 $\mathbb{I}_{A B}=\operatorname{min} ( \mathbb{I}_{A} , \mathbb{I}_{B} )$**

*   **证明并集关系**：
    $\omega \in A \cup B$ 当且仅当 $\omega \in A$ 或 $\omega \in B$。
    这意味着 $\mathbb{I}_A(\omega)$ 和 $\mathbb{I}_B(\omega)$ 中至少有一个为 1。
    *   若 $\omega \in A$ 且 $\omega \in B$，则 $\mathbb{I}_A=1, \mathbb{I}_B=1$，$\max(1,1)=1$，且 $\omega \in A \cup B \implies \mathbb{I}_{A \cup B}=1$。
    *   若 $\omega \in A$ 且 $\omega \notin B$，则 $\mathbb{I}_A=1, \mathbb{I}_B=0$，$\max(1,0)=1$，且 $\omega \in A \cup B \implies \mathbb{I}_{A \cup B}=1$。
    *   若 $\omega \notin A$ 且 $\omega \in B$，则 $\mathbb{I}_A=0, \mathbb{I}_B=1$，$\max(0,1)=1$，且 $\omega \in A \cup B \implies \mathbb{I}_{A \cup B}=1$。
    *   若 $\omega \notin A$ 且 $\omega \notin B$，则 $\mathbb{I}_A=0, \mathbb{I}_B=0$，$\max(0,0)=0$，且 $\omega \notin A \cup B \implies \mathbb{I}_{A \cup B}=0$。
    由此可见，$\mathbb{I}_{A \cup B}=\operatorname{max} ( \mathbb{I}_{A} , \mathbb{I}_{B} )$。

*   **证明交集关系**：
    $\omega \in AB$（即 $A \cap B$）当且仅当 $\omega \in A$ 且 $\omega \in B$。
    这意味着 $\mathbb{I}_A(\omega)$ 和 $\mathbb{I}_B(\omega)$ 必须同时为 1。
    *   只有当 $\mathbb{I}_A=1$ 且 $\mathbb{I}_B=1$ 时，$\min(\mathbb{I}_A, \mathbb{I}_B)=1$，此时 $\omega \in AB$，即 $\mathbb{I}_{AB}=1$。
    *   其他情况（至少有一个为0），$\min(\mathbb{I}_A, \mathbb{I}_B)=0$，此时 $\omega \notin AB$，即 $\mathbb{I}_{AB}=0$。
    由此可见，$\mathbb{I}_{A B}=\operatorname{min} ( \mathbb{I}_{A} , \mathbb{I}_{B} )$。
    *(注：由于取值只能是0或1，$\min(\mathbb{I}_A, \mathbb{I}_B)$ 等价于乘积 $\mathbb{I}_A \mathbb{I}_B$)*。

**(4) 回答 $\mathbb{I}_{A}+\mathbb{I}_{B} \pmod 2$ 和 $\mathbb{I}_{A} \mathbb{I}_{B}$ 对应的事件**

*   **关于 $\mathbb{I}_{A}+\mathbb{I}_{B} \pmod 2$**：
    考察该表达式的值：
    *   若 $\omega \in A$ 且 $\omega \notin B$：$1 + 0 = 1 \equiv 1 \pmod 2$。
    *   若 $\omega \notin A$ 且 $\omega \in B$：$0 + 1 = 1 \equiv 1 \pmod 2$。
    *   若 $\omega \in A$ 且 $\omega \in B$：$1 + 1 = 2 \equiv 0 \pmod 2$。
    *   若 $\omega \notin A$ 且 $\omega \notin B$：$0 + 0 = 0 \equiv 0 \pmod 2$。
    
    该变量为 1 当且仅当 $\omega$ 属于 $A$ 或属于 $B$，但不同时属于两者。这对应的事件是 $A$ 与 $B$ 的**对称差**（Symmetric Difference），记为 $A \Delta B$ 或 $(A \setminus B) \cup (B \setminus A)$。

*   **关于 $\mathbb{I}_{A} \mathbb{I}_{B}$**：
    考察乘积的值：
    *   $\mathbb{I}_{A}(\omega) \cdot \mathbb{I}_{B}(\omega) = 1$ 当且仅当 $\mathbb{I}_{A}(\omega)=1$ 且 $\mathbb{I}_{B}(\omega)=1$。
    *   这等价于 $\omega \in A$ 且 $\omega \in B$。
    
    故该变量对应的事件是 $A$ 与 $B$ 的**交集**，即 $A \cap B$（或简写为 $AB$）。

### 答案

1.  $\mathbb{I}_{A}+\mathbb{I}_{B} \pmod 2$ 对应的事件是 **$A$ 与 $B$ 的对称差**，记作 $A \Delta B$。
2.  $\mathbb{I}_{A} \mathbb{I}_{B}$ 对应的事件是 **$A$ 与 $B$ 的交集**，记作 $A \cap B$。

### 知识延伸

1.  **示性变量的期望**：示性变量最重要的性质是其数学期望等于该事件发生的概率，即 $E[\mathbb{I}_A] = P(A)$。这是概率方法（Probabilistic Method）中的基础。
2.  **包含排斥原理（Inclusion-Exclusion Principle）**：利用示性变量的代数性质可以简洁地证明包含排斥原理。例如，利用恒等式 $1 - \mathbb{I}_{\cup A_i} = \prod (1 - \mathbb{I}_{A_i})$ 展开即可。
3.  **常用公式**：
    *   $\mathbb{I}_{A \cap B} = \mathbb{I}_A \cdot \mathbb{I}_B$
    *   $\mathbb{I}_{A \cup B} = \mathbb{I}_A + \mathbb{I}_B - \mathbb{I}_A \mathbb{I}_B$ （对应 $P(A \cup B) = P(A) + P(B) - P(AB)$）
    *   $\mathbb{I}_{A \Delta B} = (\mathbb{I}_A - \mathbb{I}_B)^2 = |\mathbb{I}_A - \mathbb{I}_B|$
4.  **易错点**：在处理并集时，不能简单地将示性变量相加（$\mathbb{I}_{A \cup B} \neq \mathbb{I}_A + \mathbb{I}_B$），除非 $A$ 和 $B$ 互斥。模2加法实际上对应的是逻辑异或（XOR）操作。

---

## 习题0.8

### 题目

在《万里归途》电影里有一个情景，穆夫塔刁难宗大伟，发起“轮盘赌”。现设枪中子弹数服从概率为 $\frac{1} {2}$ 的0-1 两点分布。请细化概率模型，分析随着“轮盘赌”进行，枪中无子弹的概率是如何变化的？具体地，无子弹的初始概率为 $\frac1 2$ 。选择至少两个概率模型（必要时可以改变两轮之间的规则)，分析第 $i \geq1$ 枪后无子弹的概率。

### 解答

**Refining the Models**

I'm now putting the finishing touches on the probability models. I've successfully drafted both the "no respin" and "respin" scenarios, including the necessary calculations. I'm focusing on clarifying the language and ensuring the mathematical rigor meets the standard expected by a probability professor. The knowledge extension section is also taking shape.


### 题目分析

本题考查的核心概念是**贝叶斯公式（Bayes' Theorem）**与**条件概率**。题目描述了一个参数推断问题：基于观测到的数据（连续 $i$ 次扣动扳机未击发），更新对系统内部状态（枪中是否有子弹）的概率估计。

我们需要建立数学模型来描述“轮盘赌”的机制，设定假设空间（枪空 $H_0$ vs 枪有一弹 $H_1$），并计算在事件 $A_i$（前 $i$ 枪均为哑火）发生的条件下，事件 $H_0$ 的后验概率。

需要求解的目标是：$P(\text{无子弹} \mid \text{前 } i \text{ 枪未响})$。

### 解题过程

设随机变量 $K$ 表示枪中的子弹数量。根据题意，初始分布（先验概率）为：
$$P(K=0) = \frac{1}{2}, \quad P(K=1) = \frac{1}{2}$$
设左轮手枪的弹巢容量为 $N$（通常 $N=6$）。
记事件 $A_i$ 为“前 $i$ 次扣动扳机均未击发子弹”（即幸存了 $i$ 轮）。

我们需要计算的是第 $i$ 枪之后，枪中无子弹的后验概率，即 $P(K=0 \mid A_i)$。
根据贝叶斯公式：
$$P(K=0 \mid A_i) = \frac{P(A_i \mid K=0)P(K=0)}{P(A_i \mid K=0)P(K=0) + P(A_i \mid K=1)P(K=1)}$$
由于 $P(K=0) = P(K=1) = \frac{1}{2}$，公式可简化为：
$$P(K=0 \mid A_i) = \frac{P(A_i \mid K=0)}{P(A_i \mid K=0) + P(A_i \mid K=1)}$$
显然，如果枪中无子弹，无论扣动多少次都不会击发，故对于任意模型：
$$P(A_i \mid K=0) = 1$$
因此，核心在于计算似然概率 $P(A_i \mid K=1)$，这取决于具体的游戏规则。

#### 模型一：标准轮盘赌（不重转模型）

**规则设定**：
在装入子弹（如果有）后，转动一次弹巢，随后连续扣动扳机，中间不再转动弹巢。这对应于**不放回抽样**。

**推导**：
假设枪中有1颗子弹，分布在 $N$ 个弹巢中的任意一个位置是等可能的。
在前 $i$ 次射击中，只要子弹不在前 $i$ 个被撞击的弹巢位置，就不会击发。
共有 $N$ 种可能的子弹位置，其中有 $N-i$ 种位置会导致前 $i$ 枪不响（即子弹位于剩下的 $N-i$ 个位置中）。
因此，在 $K=1$ 的条件下，前 $i$ 枪不响的概率为：
$$P(A_i \mid K=1) = \frac{N-i}{N}, \quad \text{其中 } 1 \leq i < N$$
*注：若 $i=N$ 且未响，则必然无子弹（假设枪支功能正常），此时概率跃升为1。*

代入贝叶斯公式：
$$
\begin{aligned}
P(K=0 \mid A_i) &= \frac{1}{1 + \frac{N-i}{N}} \\
&= \frac{N}{N + N - i} \\
&= \frac{N}{2N - i}
\end{aligned}
$$

**分析**：
随着 $i$ 的增加，分母 $2N-i$ 减小，概率 $P(K=0 \mid A_i)$ 单调递增。这意味着每幸存一枪，枪里没子弹的可能性就越大。

#### 模型二：独立轮盘赌（重转模型）

**规则设定**：
每次扣动扳机前，都重新随机转动弹巢。这对应于**有放回抽样**（独立伯努利试验）。

**推导**：
假设枪中有1颗子弹。每次射击时，击发的概率为 $\frac{1}{N}$，未击发的概率为 $1 - \frac{1}{N}$。
由于每次射击前都重转，各次射击结果相互独立。
因此，在 $K=1$ 的条件下，连续 $i$ 次不击发的概率为：
$$P(A_i \mid K=1) = \left(1 - \frac{1}{N}\right)^i$$

代入贝叶斯公式：
$$
\begin{aligned}
P(K=0 \mid A_i) &= \frac{1}{1 + \left(1 - \frac{1}{N}\right)^i} \\
&= \frac{1}{1 + \left(\frac{N-1}{N}\right)^i}
\end{aligned}
$$

**分析**：
由于 $0 < \frac{N-1}{N} < 1$，当 $i$ 增加时，$\left(\frac{N-1}{N}\right)^i$ 单调递减趋向于0。因此分母减小，整体概率 $P(K=0 \mid A_i)$ 单调递增并趋向于1。

---

