"""
éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿä¸»ç¨‹åº
åŠŸèƒ½ï¼šåŸºäºå‘é‡æ•°æ®åº“è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®ç­”
"""

import os
from datetime import datetime
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from config_manager import (
    get_api_config, get_model_config, get_database_config, get_retrieval_config
)


def get_embeddings(offline=None):
    """è·å– Embedding æ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ° HuggingFace æˆ– APIï¼‰"""
    model_config = get_model_config()
    embedding_model = model_config.get("embedding_model", "local")
    
    # æ£€æŸ¥ç¦»çº¿æ¨¡å¼
    if offline is None:
        offline = model_config.get("offline_mode", False)
    
    if embedding_model == "local":
        import os as _os
        
        # ç¦»çº¿æ¨¡å¼ï¼šå®Œå…¨ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œä¸è”ç½‘
        if offline:
            _os.environ['HF_HUB_OFFLINE'] = '1'
            _os.environ['TRANSFORMERS_OFFLINE'] = '1'
            print("  ğŸ  ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ Embedding æ¨¡å‹")
        else:
            # è®¾ç½® HuggingFace é•œåƒï¼ˆåœ¨çº¿æ¨¡å¼ï¼‰
            _os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        from langchain_huggingface import HuggingFaceEmbeddings
        
        # è‡ªåŠ¨æ£€æµ‹ GPU
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"  ğŸ“¦ ä½¿ç”¨æœ¬åœ° HuggingFace Embedding æ¨¡å‹ (è®¾å¤‡: {device})...")
        
        return HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
    else:
        # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        api_config = get_api_config()
        return OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=api_config["api_key"],
            openai_api_base=api_config["base_url"],
        )


def get_llm(model_name=None, temperature=0.3, offline=None):
    """è·å– LLM å®ä¾‹ï¼Œæ”¯æŒåœ¨çº¿ API å’Œç¦»çº¿æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰"""
    model_config = get_model_config()
    
    # å¦‚æœæœªæŒ‡å®šï¼Œä»é…ç½®è¯»å–ç¦»çº¿æ¨¡å¼
    if offline is None:
        offline = model_config.get("offline_mode", False)
    
    if offline:
        # ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹
        local_url = model_config.get("local_llm_url", "http://localhost:11434")
        local_model = model_config.get("local_llm_model", "qwen2.5:7b")
        
        print(f"  ğŸ  ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹: {local_model}")
        
        return ChatOpenAI(
            model=local_model,
            temperature=temperature,
            openai_api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå® API key
            openai_api_base=f"{local_url}/v1",
        )
    else:
        # åœ¨çº¿æ¨¡å¼ï¼šä½¿ç”¨ API
        api_config = get_api_config()
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°å°±ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
        if model_name is None:
            chat_models = model_config.get("chat_models", ["gemini-3-pro-preview"])
            model_name = chat_models[0] if isinstance(chat_models, list) else chat_models
        
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            openai_api_key=api_config["api_key"],
            openai_api_base=api_config["base_url"],
        )


def load_vectorstore(offline=None):
    """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“"""
    db_config = get_database_config()
    model_config = get_model_config()
    persist_directory = db_config.get("chroma_dir", "./chroma_db")
    
    # æ£€æŸ¥ç¦»çº¿æ¨¡å¼
    if offline is None:
        offline = model_config.get("offline_mode", False)
    
    if not os.path.exists(persist_directory):
        raise FileNotFoundError(
            f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {persist_directory}\n"
            f"è¯·å…ˆè¿è¡Œ 'python ingest.py' æ„å»ºæ•°æ®åº“"
        )
    
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {persist_directory}")
    
    embeddings = get_embeddings(offline=offline)
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
    return vectorstore


def create_qa_chain(vectorstore, llm):
    """åˆ›å»ºç®€å•çš„ QA æ£€ç´¢å‡½æ•°"""
    retrieval_config = get_retrieval_config()
    top_k = retrieval_config.get("top_k", 5)
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": top_k}
    )
    
    class SimpleQAChain:
        def __init__(self, retriever, llm):
            self.retriever = retriever
            self.llm = llm
        
        def invoke(self, inputs):
            question = inputs.get("input", "")
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            docs = self.retriever.invoke(question)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n---\n\n".join([doc.page_content for doc in docs])
            
            # æ„å»ºæç¤º
            prompt = f"""ä½ æ˜¯ä¸€ä½æ¦‚ç‡è®ºä¸éšæœºè¿‡ç¨‹é¢†åŸŸçš„ä¸“å®¶æ•™æˆã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

**èƒŒæ™¯çŸ¥è¯†**:
{context}

---

**å›ç­”è¦æ±‚**ï¼š
1. ä¸¥æ ¼åŸºäºèƒŒæ™¯çŸ¥è¯†ä¸­çš„å®šä¹‰å’Œå®šç†è¿›è¡Œå›ç­”
2. **æ•°å­¦å…¬å¼æ ¼å¼è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰**ï¼š
   - è¡Œå†…å…¬å¼**å¿…é¡»**ä½¿ç”¨ç¾å…ƒç¬¦å·æ ¼å¼ï¼š`$å…¬å¼$`ï¼Œä¾‹å¦‚ $P(X=k)$
   - è¡Œé—´å…¬å¼**å¿…é¡»**ä½¿ç”¨åŒç¾å…ƒç¬¦å·æ ¼å¼ï¼š`$$å…¬å¼$$`
   - **ç¦æ­¢**ä½¿ç”¨ \\( \\) æˆ– \\[ \\] æ ¼å¼ï¼
3. ä½¿ç”¨æ ‡å‡†æ¦‚ç‡è®ºè®°å·ï¼ˆ$P$, $E$, $\\operatorname{{Var}}$, $\\sigma$ ç­‰ï¼‰
4. å¦‚æœæ˜¯ä¾‹é¢˜æˆ–ä¹ é¢˜ï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„è§£é¢˜æ­¥éª¤
5. å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ çš„ä¸“ä¸šè§è§£
6. å›ç­”æ—¶ä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œæ¡ç†

**å­¦ç”Ÿé—®é¢˜**: {question}

**æ•™æˆå›ç­”**:"""
            
            # è°ƒç”¨ LLM
            response = self.llm.invoke(prompt)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”å¯¹è±¡
            if isinstance(response, str):
                answer = response
            else:
                answer = response.content
            
            # åå¤„ç†ï¼šè½¬æ¢ LaTeX å…¬å¼æ ¼å¼ï¼Œç¡®ä¿ Markdown å…¼å®¹
            answer = convert_latex_format(answer)
            
            return {
                "input": question,
                "context": docs,
                "answer": answer
            }
    
    return SimpleQAChain(retriever, llm)


def convert_latex_format(text):
    """
    å°† LaTeX å…¬å¼æ ¼å¼ä» \\(...\\) å’Œ \\[...\\] è½¬æ¢ä¸º $...$ å’Œ $$...$$ æ ¼å¼
    è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨æ ‡å‡† Markdown æ¸²æŸ“å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºæ•°å­¦å…¬å¼
    """
    import re
    
    # è½¬æ¢è¡Œé—´å…¬å¼ï¼š\[...\] -> $$...$$
    text = re.sub(r'\\\[(.+?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    
    # è½¬æ¢è¡Œå†…å…¬å¼ï¼š\(...\) -> $...$
    text = re.sub(r'\\\((.+?)\\\)', r'$\1$', text, flags=re.DOTALL)
    
    return text


def save_answer_as_markdown(question, answer, context_docs, output_dir="./answers", mode="rag"):
    """å°†å›ç­”ä¿å­˜ä¸ºæ ¼å¼åŒ–çš„Markdownæ–‡æ¡£
    
    Args:
        mode: "rag" - AIç”Ÿæˆå›ç­”, "search" - çº¯æ£€ç´¢ç»“æœ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³é¿å…é‡å¤ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_question = "".join(c for c in question[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_question = safe_question.replace(' ', '_')
    prefix = "search_" if mode == "search" else ""
    filename = f"{prefix}{timestamp}_{safe_question}.md"
    filepath = os.path.join(output_dir, filename)
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©æ ‡é¢˜
    if mode == "search":
        title = "# éšæœºè¿‡ç¨‹ - æ£€ç´¢ç»“æœ\n"
        section_title = "## ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹"
    else:
        title = "# éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿ - å›ç­”è®°å½•\n"
        section_title = "## ğŸ¤– AI å›ç­”"
    
    # æ„å»ºMarkdownå†…å®¹
    markdown_content = f"""{title}
**æŸ¥è¯¢æ—¶é—´**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}
**é—®é¢˜**: {question}

{section_title}

{answer}

---

## ğŸ“š å‚è€ƒæ¥æº ({len(context_docs)} ä¸ª)

"""
    
    # æ·»åŠ å‚è€ƒæ¥æº
    for i, doc in enumerate(context_docs[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ¥æº
        doc_type = doc.metadata.get('type', 'original')
        type_label = {
            'original': 'ğŸ“„ åŸæ–‡',
            'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜',
            'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
        }.get(doc_type, 'ğŸ“„')
        
        extra_info = ""
        if 'problem_id' in doc.metadata:
            extra_info = f" - ä¾‹é¢˜ {doc.metadata['problem_id']}"
        elif 'topic' in doc.metadata:
            extra_info = f" - {doc.metadata['topic']}"
        
        markdown_content += f"""
### [{i}] {type_label}{extra_info}

```
{doc.page_content[:500]}{"..." if len(doc.page_content) > 500 else ""}
```

---
"""
    
    markdown_content += f"""

## ğŸ“„ ç³»ç»Ÿä¿¡æ¯

- **å‘é‡æ•°æ®åº“**: Chroma
- **åµŒå…¥æ¨¡å‹**: æœ¬åœ° HuggingFace
- **LLM**: {get_model_config().get('chat_models', ['é»˜è®¤'])[0]}
- **æ£€ç´¢æ–‡æ¡£æ•°**: {len(context_docs)}

---
*ç”±éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿç”Ÿæˆ*
"""
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    return filepath


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼é—®ç­”"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    OFFLINE_MODE = '--offline' in sys.argv
    SEARCH_ONLY = '--search' in sys.argv  # çº¯æ£€ç´¢æ¨¡å¼ï¼šåªè¿”å›åŸæ–‡ï¼Œä¸ä½¿ç”¨ LLM
    
    print("="*60)
    print(" ğŸ“ éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿ")
    if OFFLINE_MODE:
        print(" ğŸ  ç¦»çº¿æ¨¡å¼ï¼ˆä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ï¼‰")
    if SEARCH_ONLY:
        print(" ğŸ” çº¯æ£€ç´¢æ¨¡å¼ï¼ˆåªè¿”å›æ•™æåŸæ–‡ï¼Œä¸ä½¿ç”¨ LLMï¼‰")
    print("="*60)
    
    try:
        # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
        model_config = get_model_config()
        offline = OFFLINE_MODE or model_config.get("offline_mode", False)
        
        vectorstore = load_vectorstore(offline=offline)
        retrieval_config = get_retrieval_config()
        top_k = retrieval_config.get("top_k", 5)
        
        # çº¯æ£€ç´¢æ¨¡å¼ï¼šä¸éœ€è¦ LLM
        if not SEARCH_ONLY:
            print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹...")
            llm = get_llm(temperature=model_config.get("temperature", 0.3), offline=offline)
            print("âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
            
            print("ğŸ”— æ­£åœ¨åˆ›å»º QA æ£€ç´¢é“¾...")
            qa_chain = create_qa_chain(vectorstore, llm)
            print("âœ… QA é“¾åˆ›å»ºæˆåŠŸ\n")
        else:
            qa_chain = None
            print("âœ… çº¯æ£€ç´¢æ¨¡å¼å°±ç»ª\n")
        
        print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰")
        if SEARCH_ONLY:
            print("ğŸ“– çº¯æ£€ç´¢æ¨¡å¼ï¼šç›´æ¥è¿”å›æ•™æåŸæ–‡å’Œå·²æœ‰è§£ç­”")
        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼š")
        print("   - ä»€ä¹ˆæ˜¯é©¬å°”å¯å¤«é“¾ï¼Ÿ")
        print("   - æ³Šæ¾è¿‡ç¨‹æœ‰ä»€ä¹ˆæ€§è´¨ï¼Ÿ")
        print("   - è§£é‡Šå¸¸è¿”æ€å’Œç¬æ—¶æ€çš„åŒºåˆ«")
        print()
        
        while True:
            question = input("ğŸ™‹ æ‚¨çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            try:
                print("\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†...")
                
                # çº¯æ£€ç´¢æ¨¡å¼ï¼šåªè¿”å›æ£€ç´¢ç»“æœï¼Œä¸ä½¿ç”¨ LLM
                if SEARCH_ONLY:
                    docs = vectorstore.similarity_search(question, k=top_k)
                    
                    print("\n" + "="*60)
                    print(f"ğŸ“š æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
                    print("="*60)
                    
                    # æ„å»ºç»“æœæ–‡æœ¬
                    result_text = ""
                    for i, doc in enumerate(docs, 1):
                        doc_type = doc.metadata.get('type', 'original')
                        type_label = {
                            'original': 'ğŸ“„ æ•™æåŸæ–‡',
                            'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜/ä¹ é¢˜',
                            'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
                        }.get(doc_type, 'ğŸ“„')
                        
                        # è·å–æ¥æºä¿¡æ¯
                        extra_info = ""
                        source_info = ""
                        
                        # é¡µç ä¿¡æ¯
                        if 'page' in doc.metadata:
                            source_info += f"ç¬¬ {doc.metadata['page'] + 1} é¡µ"
                        
                        # æ¥æºæ–‡ä»¶
                        if 'source_file' in doc.metadata:
                            if source_info:
                                source_info += f" | {doc.metadata['source_file']}"
                            else:
                                source_info = doc.metadata['source_file']
                        elif 'source' in doc.metadata:
                            # PyPDFLoader é»˜è®¤çš„ source å­—æ®µ
                            import os
                            source_name = os.path.basename(doc.metadata['source'])
                            if source_info:
                                source_info += f" | {source_name}"
                            else:
                                source_info = source_name
                        
                        # é¢˜ç›®ID
                        if 'problem_id' in doc.metadata:
                            extra_info = f" - {doc.metadata['problem_id']}"
                        elif 'topic' in doc.metadata:
                            extra_info = f" - {doc.metadata['topic']}"
                        
                        # æ„å»ºæ ‡é¢˜
                        header = f"[{i}] {type_label}{extra_info}"
                        if source_info:
                            header += f"\n    ğŸ“ æ¥æº: {source_info}"
                        
                        print(f"\n{'='*60}")
                        print(header)
                        print("-"*60)
                        # æ˜¾ç¤ºå®Œæ•´å†…å®¹
                        print(doc.page_content)
                        
                        # ç´¯ç§¯åˆ°ç»“æœæ–‡æœ¬ï¼ˆMarkdown æ ¼å¼ï¼‰
                        result_text += f"\n## [{i}] {type_label}{extra_info}\n\n"
                        if source_info:
                            result_text += f"**ğŸ“ æ¥æº**: {source_info}\n\n"
                        result_text += doc.page_content + "\n\n---\n"
                    
                    print("\n" + "="*60)
                    
                    # ä¿å­˜åˆ° Markdown æ–‡ä»¶
                    try:
                        filepath = save_answer_as_markdown(
                            question=question,
                            answer=result_text,
                            context_docs=docs,
                            mode="search"  # æ ‡è®°ä¸ºæ£€ç´¢æ¨¡å¼
                        )
                        print(f"ğŸ’¾ æ£€ç´¢ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
                    except Exception as e:
                        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                else:
                    # æ­£å¸¸ RAG æ¨¡å¼
                    result = qa_chain.invoke({"input": question})
                    
                    print("\n" + "="*60)
                    print("ğŸ¤– å›ç­”:")
                    print("-"*60)
                    print(result['answer'])
                    print("="*60)
                    
                    # è‡ªåŠ¨ä¿å­˜å›ç­”ä¸ºMarkdownæ–‡ä»¶
                    try:
                        filepath = save_answer_as_markdown(
                            question=result['input'],
                            answer=result['answer'],
                            context_docs=result['context']
                        )
                        print(f"ğŸ’¾ å›ç­”å·²è‡ªåŠ¨ä¿å­˜åˆ°: {filepath}")
                        print("ğŸ“„ å¯ä»¥ä½¿ç”¨ Markdown æŸ¥çœ‹å™¨æ‰“å¼€æ–‡ä»¶ï¼Œæ•°å­¦å…¬å¼å°†æ­£ç¡®æ˜¾ç¤º")
                    except Exception as e:
                        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                    
                    if result.get('context'):
                        print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['context'])} ä¸ª):")
                        for i, doc in enumerate(result['context'][:3], 1):
                            doc_type = doc.metadata.get('type', 'original')
                            type_label = {
                                'original': 'ğŸ“„ åŸæ–‡',
                                'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜',
                                'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
                            }.get(doc_type, 'ğŸ“„')
                            
                            extra_info = ""
                            if 'problem_id' in doc.metadata:
                                extra_info = f" - ä¾‹é¢˜ {doc.metadata['problem_id']}"
                            elif 'topic' in doc.metadata:
                                extra_info = f" - {doc.metadata['topic']}"
                            
                            print(f"\n[{i}] {type_label}{extra_info}")
                            print("-"*40)
                            content = doc.page_content[:150]
                            print(content + "..." if len(doc.page_content) > 150 else content)
                
                print("\n")
                
            except Exception as e:
                print(f"\nâŒ é—®ç­”å‡ºé”™: {str(e)}\n")
    
    except FileNotFoundError as e:
        print(f"\nâŒ {str(e)}\n")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}\n")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
