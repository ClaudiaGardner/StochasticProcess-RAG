"""
æ–‡æ¡£æ‘„å…¥ä¸å‘é‡åŒ–æ¨¡å—
åŠŸèƒ½ï¼šè§£æ PDF æ–‡æ¡£ã€æå–ä¾‹é¢˜ä¹ é¢˜ã€è°ƒç”¨ API è§£ç­”ã€å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Chroma æ•°æ®åº“
"""

import os
import re
import json
from pathlib import Path

# PDF è§£æåº“
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

# OCR åº“ï¼ˆæ”¯æŒæ•°å­¦å…¬å¼ï¼‰
try:
    from pix2text import Pix2Text
    HAS_PIX2TEXT = True
except ImportError:
    HAS_PIX2TEXT = False
    print("âš ï¸ Pix2Text æœªå®‰è£…ï¼ŒOCR åŠŸèƒ½ä¸å¯ç”¨")

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from config_manager import (
    get_api_config, get_model_config, get_database_config,
    get_ingestion_config, get_topics
)


def load_pdf_with_ocr(pdf_path, use_ocr=True):
    """ä½¿ç”¨ OCR åŠ è½½ PDFï¼ˆæ”¯æŒæ•°å­¦å…¬å¼è¯†åˆ«ï¼‰ï¼Œå¸¦ç¼“å­˜"""
    import pickle
    import hashlib
    
    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
    pdf_hash = hashlib.md5(open(pdf_path, 'rb').read()).hexdigest()[:8]
    cache_file = f"./ocr_cache_{pdf_hash}.pkl"
    
    # æ£€æŸ¥ç¼“å­˜
    if os.path.exists(cache_file):
        print(f"ğŸ“¦ å‘ç° OCR ç¼“å­˜æ–‡ä»¶: {cache_file}")
        try:
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
            documents = [Document(page_content=d['content'], metadata=d['metadata']) 
                        for d in cached_data]
            print(f"âœ… ä»ç¼“å­˜åŠ è½½ {len(documents)} é¡µ")
            return documents
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥: {e}ï¼Œé‡æ–° OCR...")
    
    if not HAS_PYMUPDF:
        print("âŒ PyMuPDF æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œ OCR")
        return None
    
    print(f"ğŸ“– ä½¿ç”¨ {'Pix2Text OCR' if use_ocr and HAS_PIX2TEXT else 'PyMuPDF'} åŠ è½½ PDF: {pdf_path}")
    doc = fitz.open(pdf_path)
    documents = []
    
    # åˆå§‹åŒ– OCR
    p2t = None
    if use_ocr and HAS_PIX2TEXT:
        print("  ğŸ”„ åˆå§‹åŒ– Pix2Text OCRï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰...")
        try:
            p2t = Pix2Text.from_config()
            print("  âœ… Pix2Text OCR åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ Pix2Text åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ° PyMuPDF")
            p2t = None
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        if p2t:
            # ä½¿ç”¨ OCR æå–ï¼ˆæ”¯æŒæ•°å­¦å…¬å¼ï¼‰
            try:
                # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
                pix = page.get_pixmap(dpi=200)
                img_path = f"temp_page_{page_num}.png"
                pix.save(img_path)
                
                # OCR è¯†åˆ«
                result = p2t.recognize(img_path, resized_shape=1200)
                text = result.to_markdown() if hasattr(result, 'to_markdown') else str(result)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(img_path)
                
                if text.strip():
                    documents.append(Document(
                        page_content=text,
                        metadata={"source": pdf_path, "page": page_num, "method": "ocr"}
                    ))
                    print(f"  ğŸ“„ OCR å®Œæˆç¬¬ {page_num + 1} é¡µ")
            except Exception as e:
                print(f"  âš ï¸ ç¬¬ {page_num + 1} é¡µ OCR å¤±è´¥: {e}")
                # å›é€€åˆ° PyMuPDF
                text = page.get_text("text")
                if text.strip():
                    documents.append(Document(
                        page_content=text,
                        metadata={"source": pdf_path, "page": page_num, "method": "pymupdf"}
                    ))
        else:
            # ä½¿ç”¨ PyMuPDF æå–
            text = page.get_text("text")
            if text.strip():
                documents.append(Document(
                    page_content=text,
                    metadata={"source": pdf_path, "page": page_num, "method": "pymupdf"}
                ))
    
    doc.close()
    
    # ä¿å­˜ç¼“å­˜
    try:
        cache_data = [{'content': d.page_content, 'metadata': d.metadata} for d in documents]
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        print(f"ğŸ’¾ OCR ç»“æœå·²ç¼“å­˜åˆ°: {cache_file}")
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} é¡µ")
    return documents


def load_pdf_with_pymupdf(pdf_path):
    """ä½¿ç”¨ PyMuPDF åŠ è½½ PDFï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰"""
    if not HAS_PYMUPDF:
        return None
    
    print(f"ğŸ“– ä½¿ç”¨ PyMuPDF åŠ è½½ PDF: {pdf_path}")
    doc = fitz.open(pdf_path)
    documents = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text("text")  # æå–çº¯æ–‡æœ¬
        if text.strip():
            documents.append(Document(
                page_content=text,
                metadata={"source": pdf_path, "page": page_num}
            ))
    
    doc.close()
    print(f"âœ… PyMuPDF æˆåŠŸåŠ è½½ {len(documents)} é¡µ")
    return documents


def get_embeddings():
    """è·å– Embedding æ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ° HuggingFace æˆ– APIï¼‰"""
    model_config = get_model_config()
    embedding_model = model_config.get("embedding_model", "local")
    
    if embedding_model == "local":
        # è®¾ç½® HuggingFace é•œåƒï¼ˆè§£å†³å›½å†…ç½‘ç»œé—®é¢˜ï¼‰
        import os
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        from langchain_huggingface import HuggingFaceEmbeddings
        print("  ğŸ“¦ ä½¿ç”¨æœ¬åœ° HuggingFace Embedding æ¨¡å‹...")
        
        # å°è¯•å¤šä¸ªæ¨¡å‹ï¼ŒæŒ‰é¡ºåºå›é€€
        models_to_try = [
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "sentence-transformers/all-MiniLM-L6-v2",
            "BAAI/bge-small-zh-v1.5",
        ]
        
        for model_name in models_to_try:
            try:
                print(f"  ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {model_name}")
                return HuggingFaceEmbeddings(
                    model_name=model_name,
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
            except Exception as e:
                print(f"  âš ï¸ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {str(e)[:80]}")
                continue
        
        raise RuntimeError("æ‰€æœ‰ Embedding æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    else:
        # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        api_config = get_api_config()
        return OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=api_config["api_key"],
            openai_api_base=api_config["base_url"],
        )


def get_llm(model_name=None):
    """è·å– LLM å®ä¾‹ï¼Œæ”¯æŒæŒ‡å®šæ¨¡å‹åç§°"""
    api_config = get_api_config()
    model_config = get_model_config()
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°å°±ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
    if model_name is None:
        chat_models = model_config.get("chat_models", ["gemini-3-pro-preview"])
        model_name = chat_models[0] if isinstance(chat_models, list) else chat_models
    
    return ChatOpenAI(
        model=model_name,
        temperature=model_config.get("temperature", 0.3),
        openai_api_key=api_config["api_key"],
        openai_api_base=api_config["base_url"],
    )


def load_and_split_pdf(pdf_path, chunk_size=800, chunk_overlap=150, use_ocr=False):
    """åŠ è½½ PDF å¹¶åˆ‡åˆ†æ–‡æ¡£ï¼ˆæ”¯æŒ OCR æ¨¡å¼ï¼‰"""
    
    # æ ¹æ®å‚æ•°é€‰æ‹©åŠ è½½æ–¹å¼
    if use_ocr:
        documents = load_pdf_with_ocr(pdf_path, use_ocr=True)
    else:
        documents = load_pdf_with_pymupdf(pdf_path)
    
    if documents is None:
        # å›é€€åˆ° PyPDFLoader
        print(f"ğŸ“– ä½¿ç”¨ PyPDFLoader åŠ è½½ PDF: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} é¡µ")
    
    separators = [
        "\nã€ä¾‹é¢˜",
        "\nã€ä¾‹",
        "\nä¾‹é¢˜",
        "\nä¹ é¢˜",
        "\nÂ§",
        "\nå®šä¹‰",
        "\nå®šç†",
        "\nè¯æ˜",
        "\n\n",
        "\n",
        "ã€‚",
        " ",
        ""
    ]
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=len,
    )
    
    print(f"ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
    splits = text_splitter.split_documents(documents)
    print(f"âœ… æˆåŠŸåˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")
    
    return splits, documents


def extract_problems(documents):
    """ä»æ–‡æ¡£ä¸­æå–ä¾‹é¢˜å’Œä¹ é¢˜ï¼ˆè¯¾åä½œä¸šï¼‰- æ”¹è¿›ç‰ˆ"""
    problems = []
    
    # åˆå¹¶æ‰€æœ‰é¡µé¢å†…å®¹
    full_text = "\n".join([doc.page_content for doc in documents])
    
    # ä¾‹é¢˜åŒ¹é…æ¨¡å¼ - ä½¿ç”¨æ›´çµæ´»çš„æ¨¡å¼
    example_ids = set()
    # åŒ¹é… ã€ä¾‹é¢˜X.Xã€‘ æ ¼å¼
    for m in re.finditer(r'ã€ä¾‹é¢˜[\s]*([0-9]+\.[0-9]+)ã€‘', full_text):
        example_ids.add(m.group(1))
    # ä¹Ÿå°è¯•åŒ¹é… "ä¾‹é¢˜ X.X" æ ¼å¼ï¼ˆæ— æ–¹æ‹¬å·ï¼‰
    for m in re.finditer(r'ä¾‹é¢˜\s*([0-9]+\.[0-9]+)', full_text):
        example_ids.add(m.group(1))
    
    print(f"  ğŸ“Š PDFä¸­æ£€æµ‹åˆ° {len(example_ids)} ä¸ªä¾‹é¢˜ç¼–å·")
    
    # å¯¹æ¯ä¸ªä¾‹é¢˜ç¼–å·æå–å†…å®¹
    for eid in sorted(example_ids, key=lambda x: [float(n) for n in x.split('.')]):
        # æŸ¥æ‰¾è¯¥ä¾‹é¢˜çš„å†…å®¹ï¼ˆåˆ°ä¸‹ä¸€ä¸ªä¾‹é¢˜æˆ–ç« èŠ‚ç»“æŸï¼‰
        pattern = rf'ã€?ä¾‹é¢˜[\s]*{re.escape(eid)}ã€‘?\s*(.+?)(?=ã€?ä¾‹é¢˜|Â§\s*[0-9]+\.[0-9]+|$)'
        match = re.search(pattern, full_text, re.DOTALL)
        
        if match:
            content = match.group(1).strip()
            content = re.sub(r'\s+', ' ', content)[:2000]
            
            if len(content) > 10:
                problems.append({
                    'id': f"ä¾‹é¢˜{eid}",
                    'content': content,
                    'type': 'example'
                })
            else:
                problems.append({
                    'id': f"ä¾‹é¢˜{eid}",
                    'content': f"ä¾‹é¢˜{eid}ï¼ˆPDFè§£æä¸å®Œæ•´ï¼Œè¯·å‚è€ƒåŸæ–‡ï¼‰",
                    'type': 'example'
                })
        else:
            problems.append({
                'id': f"ä¾‹é¢˜{eid}",
                'content': f"ä¾‹é¢˜{eid}ï¼ˆPDFè§£æä¸å®Œæ•´ï¼Œè¯·å‚è€ƒåŸæ–‡ï¼‰",
                'type': 'example'
            })
    
    seen_ids = set()
    
    # æå–è¯¾åä½œä¸š/ä¹ é¢˜ - ä½¿ç”¨æ”¹è¿›çš„å¤šæ¨¡å¼åŒ¹é…
    homework_structure = [
        ('0', 6, ['è¯¾åä½œä¸š', 'ä½œä¸š'], 8),
        ('1', 4, ['è¯¾åä½œä¸š', 'ä½œä¸š'], 3),
        ('2', 4, ['è¯¾åä¹ é¢˜', 'ä¹ é¢˜'], 7),
        ('3', 6, ['è¯¾åä¹ é¢˜', 'ä¹ é¢˜'], 14),
        ('4', 5, ['è¯¾åä½œä¸š', 'ä½œä¸š'], 10),
    ]
    
    for chapter, section, hw_names, num_problems in homework_structure:
        chapter_exercises_found = []
        
        # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…è¯¾åä½œä¸šéƒ¨åˆ†
        section_content = None
        for hw_name in hw_names:
            # æ¨¡å¼1: "X.X è¯¾åä½œä¸š" æˆ– "X.X. è¯¾åä½œä¸š"
            patterns = [
                rf'{chapter}\.{section}\.?\s*{hw_name}\s*\n(.+?)(?=\n[0-9]+\.[0-9]+\s|\nChapter|\Z)',
                rf'{chapter}\.{section}\.?\s*{hw_name}(.+?)(?=\n[0-9]+\.[0-9]+\s|\Z)',
                rf'{hw_name}\s*\n(.+?)(?=\n[0-9]+\.[0-9]+\s|\Z)',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, full_text, re.DOTALL | re.IGNORECASE)
                if match and len(match.group(1)) > 50:
                    section_content = match.group(1)
                    break
            if section_content:
                break
        
        if section_content:
            # å°è¯•å¤šç§ä¹ é¢˜ç¼–å·æ ¼å¼
            # æ ¼å¼1: "1. é¢˜ç›®å†…å®¹" æˆ– "1ã€é¢˜ç›®å†…å®¹"
            hw_problems = re.findall(r'(\d+)[\.ã€\s]\s*(.+?)(?=\n\s*\d+[\.ã€\s]|\Z)', section_content, re.DOTALL)
            
            for hw_num, hw_content in hw_problems:
                hw_num = int(hw_num)
                if hw_num > num_problems:  # è·³è¿‡è¶…å‡ºèŒƒå›´çš„é¢˜å·
                    continue
                    
                prob_id = f"{chapter}.{hw_num}"
                unique_key = f"hw_{prob_id}"
                
                if unique_key in seen_ids:
                    continue
                seen_ids.add(unique_key)
                
                hw_content = hw_content.strip()
                if len(hw_content) > 15:
                    hw_content = re.sub(r'\s+', ' ', hw_content)[:2000]
                    problems.append({
                        'id': f"ä¹ é¢˜{prob_id}",
                        'content': hw_content,
                        'type': 'exercise'
                    })
                    chapter_exercises_found.append(hw_num)
        
        # è¡¥å……ç¼ºå¤±çš„é¢˜å·
        found_count = len([p for p in problems if p['id'].startswith(f"ä¹ é¢˜{chapter}.")])
        if found_count < num_problems:
            print(f"  âš ï¸ ç¬¬{chapter}ç« åªæ‰¾åˆ° {found_count}/{num_problems} é“ä¹ é¢˜")
            for i in range(1, num_problems + 1):
                prob_id = f"{chapter}.{i}"
                unique_key = f"hw_{prob_id}"
                if unique_key not in seen_ids:
                    seen_ids.add(unique_key)
                    problems.append({
                        'id': f"ä¹ é¢˜{prob_id}",
                        'content': f"ç¬¬{chapter}ç« ç¬¬{i}é¢˜ï¼ˆPDFè§£æä¸å®Œæ•´ï¼Œè¯·å‚è€ƒåŸæ–‡ï¼‰",
                        'type': 'exercise'
                    })
    
    # æŒ‰ç±»å‹å’Œ ID æ’åº
    def sort_key(p):
        nums = re.findall(r'[\d\.]+', p['id'])
        if nums:
            parts = nums[0].split('.')
            return (0 if p['type'] == 'example' else 1, 
                    [float(x) if x else 0 for x in parts])
        return (2, [999])
    
    problems.sort(key=sort_key)
    
    return problems



def solve_problem_with_api(llm, problem_id, problem_content, problem_type):
    """ä½¿ç”¨ API è§£ç­”ä¾‹é¢˜æˆ–ä¹ é¢˜ï¼Œæ­£ç¡®å¤„ç†æ•°å­¦å…¬å¼"""
    type_name = 'ä¾‹é¢˜' if problem_type == 'example' else 'ä¹ é¢˜'
    
    prompt = f"""ä½ æ˜¯ä¸€ä½æ¦‚ç‡è®ºä¸éšæœºè¿‡ç¨‹é¢†åŸŸçš„èµ„æ·±æ•°å­¦æ•™æˆã€‚è¯·è¯¦ç»†è§£ç­”ä»¥ä¸‹{type_name}ï¼Œå¹¶ä¸”ä¸è¦æœ‰å®¢å¥—è¯ï¼Œç›´æ¥å®Œæˆè¦æ±‚ã€‚

**é‡è¦è¯´æ˜**ï¼šé¢˜ç›®ä¸­å¯èƒ½åŒ…å«æ•°å­¦å…¬å¼ï¼Œè¯·ä»”ç»†è¯†åˆ«å¹¶æ­£ç¡®ç†è§£ã€‚

---
## {type_name} {problem_id}

{problem_content}

---

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼ä¸¥æ ¼å›ç­”ï¼š

### é¢˜ç›®åˆ†æ
- ç®€æ´æ˜äº†ï¼Œä¸ç”¨åˆ†ç‚¹
- è¯†åˆ«é¢˜ç›®ä¸­çš„æ•°å­¦ç¬¦å·å’Œå…¬å¼
- åˆ†ææœ¬é¢˜è€ƒæŸ¥çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚é©¬å°”å¯å¤«é“¾ã€æ³Šæ¾è¿‡ç¨‹ã€éšæœºæ¸¸èµ°ç­‰ï¼‰
- æ˜ç¡®éœ€è¦æ±‚è§£çš„ç›®æ ‡

### è§£é¢˜è¿‡ç¨‹
ç»™å‡ºå®Œæ•´è¯¦ç»†çš„è§£é¢˜æ­¥éª¤ã€‚**æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼**ï¼š
- è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`ï¼Œä¾‹å¦‚ï¼š$P(X=k)$
- è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$`ï¼Œä¾‹å¦‚ï¼š
$$P_{{ij}} = P(X_{{n+1}}=j | X_n=i)$$

è¯·ç¡®ä¿ï¼š
1. æ¯ä¸€æ­¥æ¨å¯¼éƒ½æœ‰æ¸…æ™°çš„è§£é‡Š
2. å…¬å¼ä¹¦å†™è§„èŒƒï¼Œä½¿ç”¨æ­£ç¡®çš„ LaTeX è¯­æ³•
3. æ¦‚ç‡ç¬¦å·ã€æœŸæœ›ã€æ–¹å·®ç­‰ä½¿ç”¨æ ‡å‡†è®°å·

### ç­”æ¡ˆ
ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œç”¨æ•°å­¦å…¬å¼è¡¨è¾¾

### çŸ¥è¯†å»¶ä¼¸
- ç®€æ´æ˜äº†
- æ€»ç»“æœ¬é¢˜æ¶‰åŠçš„å®šç†å’Œæ€§è´¨
- åˆ—å‡ºç›¸å…³çš„é‡è¦å…¬å¼
- æŒ‡å‡ºå¸¸è§çš„è§£é¢˜æŠ€å·§å’Œæ˜“é”™ç‚¹

è¯·å¼€å§‹è§£ç­”ï¼š"""
    
    # è·å–æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
    model_config = get_model_config()
    chat_models = model_config.get("chat_models", ["gemini-3-pro-preview"])
    if not isinstance(chat_models, list):
        chat_models = [chat_models]
    
    # å¯¹æ¯ä¸ªæ¨¡å‹å°è¯•ï¼Œå¤±è´¥åˆ™åˆ‡æ¢ä¸‹ä¸€ä¸ª
    for model_idx, model_name in enumerate(chat_models):
        llm = get_llm(model_name)
        
        # æ¯ä¸ªæ¨¡å‹é‡è¯•2æ¬¡
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = llm.invoke(prompt)
                if model_idx > 0:
                    print(f"    âœ… ä½¿ç”¨å¤‡é€‰æ¨¡å‹ {model_name} æˆåŠŸ")
                return response.content
            except Exception as e:
                error_msg = str(e)
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    print(f"  âš ï¸ æ¨¡å‹ {model_name} å¤±è´¥ (é‡è¯• {attempt + 1}/{max_retries}): {error_msg[:80]}")
                    import time
                    time.sleep(wait_time)
                else:
                    if model_idx < len(chat_models) - 1:
                        print(f"  ğŸ”„ æ¨¡å‹ {model_name} å¤±è´¥ï¼Œåˆ‡æ¢åˆ° {chat_models[model_idx + 1]}...")
                    else:
                        print(f"  âŒ æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥: {error_msg[:100]}")
                        return None


def generate_supplementary_knowledge(llm, topic):
    """ä½¿ç”¨ API ç”Ÿæˆè¡¥å……çŸ¥è¯†ï¼Œæ­£ç¡®è¾“å‡ºæ•°å­¦å…¬å¼"""
    prompt = f"""ä½ æ˜¯ä¸€ä½æ¦‚ç‡è®ºä¸éšæœºè¿‡ç¨‹é¢†åŸŸçš„ä¸“å®¶æ•™æˆã€‚è¯·é’ˆå¯¹ä»¥ä¸‹ä¸»é¢˜æä¾›ç³»ç»Ÿã€è¯¦ç»†çš„çŸ¥è¯†è®²è§£ã€‚

**ä¸»é¢˜**ï¼š{topic}

**æ ¼å¼è¦æ±‚**ï¼š
- æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼
- è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`
- è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$`
- ä½¿ç”¨æ ‡å‡†çš„æ¦‚ç‡è®ºè®°å·ï¼ˆå¦‚ $P$, $E$, $\operatorname{{Var}}$, $\sigma$ ç­‰ï¼‰

---

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

## 1. åŸºæœ¬å®šä¹‰
ç»™å‡ºä¸¥æ ¼çš„æ•°å­¦å®šä¹‰ã€‚ä¾‹å¦‚ï¼š
$$P(A|B) = \\frac{{P(A \\cap B)}}{{P(B)}}$$

## 2. æ ¸å¿ƒæ€§è´¨
åˆ—ä¸¾é‡è¦çš„æ€§è´¨å’Œå®šç†ï¼Œæ¯ä¸ªæ€§è´¨ç”¨å…¬å¼è¡¨è¾¾

## 3. å…³é”®å…¬å¼
ç»™å‡ºå¸¸ç”¨çš„è®¡ç®—å…¬å¼ï¼Œå¦‚æ¦‚ç‡è®¡ç®—ã€æœŸæœ›ã€æ–¹å·®ç­‰
ç”¨å…¬å¼åˆ—è¡¨å½¢å¼å±•ç¤º

## 4. å…¸å‹ä¾‹å­
ç”¨å…·ä½“æ•°å€¼ä¸¾ä¾‹è¯´æ˜æ¦‚å¿µçš„åº”ç”¨
åŒ…å«å®Œæ•´çš„è®¡ç®—è¿‡ç¨‹

## 5. ä¸å…¶ä»–æ¦‚å¿µçš„è”ç³»
è¯´æ˜ä¸ç›¸å…³æ¦‚å¿µï¼ˆé©¬å°”å¯å¤«é“¾ã€æ³Šæ¾è¿‡ç¨‹ã€éšæœºæ¸¸èµ°ç­‰ï¼‰çš„å…³ç³»

## 6. å¸¸è§è€ƒç‚¹ä¸æ˜“é”™ç‚¹
- å­¦ä¹ è¦ç‚¹
- å¸¸è§é”™è¯¯
- è§£é¢˜æŠ€å·§

è¯·ä½¿ç”¨ä¸¥è°¨çš„æ•°å­¦è¯­è¨€è¿›è¡Œé˜è¿°ï¼š"""
    
    # è·å–æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
    model_config = get_model_config()
    chat_models = model_config.get("chat_models", ["gemini-3-pro-preview"])
    if not isinstance(chat_models, list):
        chat_models = [chat_models]
    
    # å¯¹æ¯ä¸ªæ¨¡å‹å°è¯•ï¼Œå¤±è´¥åˆ™åˆ‡æ¢ä¸‹ä¸€ä¸ª
    for model_idx, model_name in enumerate(chat_models):
        llm = get_llm(model_name)
        
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = llm.invoke(prompt)
                return response.content
            except Exception as e:
                error_msg = str(e)
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    print(f"  âš ï¸ æ¨¡å‹ {model_name} å¤±è´¥ (é‡è¯• {attempt + 1}/{max_retries}): {error_msg[:80]}")
                    import time
                    time.sleep(wait_time)
                else:
                    if model_idx < len(chat_models) - 1:
                        print(f"  ğŸ”„ åˆ‡æ¢åˆ° {chat_models[model_idx + 1]}...")
                    else:
                        print(f"  âŒ çŸ¥è¯†ç”Ÿæˆæœ€ç»ˆå¤±è´¥: {error_msg[:100]}")
                        return None


def create_vectorstore(documents, persist_directory):
    """åˆ›å»ºå‘é‡å­˜å‚¨"""
    print(f"ğŸ§  æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embeddings = get_embeddings()
    
    print(f"ğŸ’¾ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–åˆ°: {persist_directory}")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼")
    return vectorstore


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    USE_OCR = '--ocr' in sys.argv
    if USE_OCR:
        print("ğŸ”¬ å·²å¯ç”¨ OCR æ¨¡å¼ï¼ˆæ”¯æŒæ•°å­¦å…¬å¼è¯†åˆ«ï¼‰")
    
    # ä»é…ç½®è¯»å–å‚æ•°
    db_config = get_database_config()
    ing_config = get_ingestion_config()
    
    PDF_PATH = ing_config.get("pdf_path", "data/SP-10-12.pdf")
    CHROMA_DIR = db_config.get("chroma_dir", "./chroma_db")
    SOLUTIONS_DIR = db_config.get("solutions_dir", "./solutions")
    CHUNK_SIZE = ing_config.get("chunk_size", 800)
    CHUNK_OVERLAP = ing_config.get("chunk_overlap", 150)
    MAX_PROBLEMS = ing_config.get("max_problems_to_solve", 10)
    
    # æ£€æŸ¥ PDF æ˜¯å¦å­˜åœ¨
    if not os.path.exists(PDF_PATH):
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ° PDF æ–‡ä»¶ {PDF_PATH}")
        return
    
    # å¦‚æœå‘é‡æ•°æ®åº“å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦é‡å»º
    if os.path.exists(CHROMA_DIR):
        response = input(f"âš ï¸  å‘é‡æ•°æ®åº“ {CHROMA_DIR} å·²å­˜åœ¨ï¼Œæ˜¯å¦é‡å»ºï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("âŒ å–æ¶ˆæ“ä½œ")
            return
        print("ğŸ—‘ï¸  åˆ é™¤æ—§æ•°æ®åº“...")
        import shutil
        shutil.rmtree(CHROMA_DIR)
    
    # åˆ›å»ºç›®å½•
    Path(SOLUTIONS_DIR).mkdir(exist_ok=True)
    
    try:
        # æ­¥éª¤ 1: åŠ è½½å’Œåˆ‡åˆ†æ–‡æ¡£ï¼ˆä½¿ç”¨ OCR æˆ– PyMuPDFï¼‰
        splits, raw_documents = load_and_split_pdf(PDF_PATH, CHUNK_SIZE, CHUNK_OVERLAP, use_ocr=USE_OCR)
        
        # æ‰“å°ç¤ºä¾‹ç‰‡æ®µ
        print("\nğŸ“„ ç¤ºä¾‹æ–‡æ¡£ç‰‡æ®µ:")
        print("-" * 60)
        print(splits[0].page_content[:300])
        print("-" * 60)
        
        # æ­¥éª¤ 2: æå–ä¾‹é¢˜å’Œä¹ é¢˜
        print("\nğŸ” æ­£åœ¨æå–ä¾‹é¢˜å’Œä¹ é¢˜...")
        problems = extract_problems(raw_documents)
        print(f"âœ… æ‰¾åˆ° {len(problems)} ä¸ªä¾‹é¢˜/ä¹ é¢˜")
        
        # æ˜¾ç¤ºæ‰¾åˆ°çš„é¢˜ç›®åˆ—è¡¨
        examples = [p for p in problems if p['type'] == 'example']
        exercises = [p for p in problems if p['type'] == 'exercise']
        print(f"\nğŸ“‹ é¢˜ç›®ç»Ÿè®¡: {len(examples)} ä¸ªä¾‹é¢˜, {len(exercises)} ä¸ªä¹ é¢˜")
        
        # è¯»å–ç°æœ‰é¢˜ç›®ç´¢å¼•æ–‡ä»¶ä¸­çš„æ‰‹åŠ¨è¡¥å……å†…å®¹
        index_file = f"{SOLUTIONS_DIR}/é¢˜ç›®ç´¢å¼•.md"
        existing_problems = {}
        if os.path.exists(index_file):
            print(f"\nğŸ“– è¯»å–ç°æœ‰é¢˜ç›®ç´¢å¼•ï¼Œä¿ç•™æ‰‹åŠ¨è¡¥å……å†…å®¹...")
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            # è§£æç°æœ‰é¢˜ç›®å†…å®¹
            import re as re_existing
            for match in re_existing.finditer(r'### (ä¾‹é¢˜|ä¹ é¢˜)(\d+\.\d+)\n\n(.+?)(?=\n---|\Z)', content, re_existing.DOTALL):
                prob_type = match.group(1)
                prob_id = f"{prob_type}{match.group(2)}"
                prob_content = match.group(3).strip()
                # åªä¿ç•™é"PDFè§£æä¸å®Œæ•´"çš„å†…å®¹
                if 'PDFè§£æä¸å®Œæ•´' not in prob_content and 'è¯·å‚è€ƒåŸæ–‡' not in prob_content:
                    existing_problems[prob_id] = prob_content
            print(f"   âœ… å‘ç° {len(existing_problems)} ä¸ªæ‰‹åŠ¨è¡¥å……çš„é¢˜ç›®")
        
        # åˆå¹¶ï¼šä¼˜å…ˆä½¿ç”¨æ‰‹åŠ¨è¡¥å……çš„å†…å®¹
        for p in problems:
            if p['id'] in existing_problems:
                # å¦‚æœæ‰‹åŠ¨è¡¥å……äº†å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨æ‰‹åŠ¨ç‰ˆæœ¬
                if len(existing_problems[p['id']]) > len(p['content']) or 'PDFè§£æä¸å®Œæ•´' in p['content']:
                    p['content'] = existing_problems[p['id']]
        
        # ç”Ÿæˆé¢˜ç›®ç´¢å¼•æ–‡æ¡£
        print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆé¢˜ç›®ç´¢å¼•æ–‡æ¡£: {index_file}")
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write("# éšæœºè¿‡ç¨‹ - é¢˜ç›®ç´¢å¼•\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {Path(PDF_PATH).stem}\n\n")
            f.write(f"**æ€»è®¡**: {len(problems)} é“é¢˜ç›® ({len(examples)} ä¾‹é¢˜ + {len(exercises)} ä¹ é¢˜)\n\n")
            f.write("---\n\n")
            
            # å»é‡å‡½æ•°ï¼šä¿ç•™å†…å®¹æœ€å®Œæ•´çš„ç‰ˆæœ¬
            def deduplicate_problems(prob_list):
                seen = {}
                for p in prob_list:
                    pid = p['id']
                    content = p['content']
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒå“ªä¸ªæ›´å®Œæ•´
                    if pid in seen:
                        old_content = seen[pid]['content']
                        # ä¼˜å…ˆé€‰æ‹©ä¸åŒ…å«"PDFè§£æä¸å®Œæ•´"çš„ç‰ˆæœ¬
                        if 'PDFè§£æä¸å®Œæ•´' in old_content and 'PDFè§£æä¸å®Œæ•´' not in content:
                            seen[pid] = p
                        # æˆ–è€…é€‰æ‹©æ›´é•¿çš„ç‰ˆæœ¬
                        elif 'PDFè§£æä¸å®Œæ•´' not in old_content and 'PDFè§£æä¸å®Œæ•´' not in content:
                            if len(content) > len(old_content):
                                seen[pid] = p
                    else:
                        seen[pid] = p
                return list(seen.values())
            
            # å»é‡
            examples_dedup = deduplicate_problems(examples)
            exercises_dedup = deduplicate_problems(exercises)
            
            # å†™å…¥ä¾‹é¢˜ï¼ˆé™åˆ¶æ¯é¢˜å†…å®¹é•¿åº¦ä¸º300å­—ç¬¦ï¼‰
            if examples_dedup:
                f.write("## ä¾‹é¢˜åˆ—è¡¨\n\n")
                for p in sorted(examples_dedup, key=lambda x: [float(n) for n in re.findall(r'[\d.]+', x['id'])]):
                    content_preview = p['content'][:300] + "..." if len(p['content']) > 300 else p['content']
                    f.write(f"### {p['id']}\n\n")
                    f.write(f"{content_preview}\n\n")
                    f.write("---\n\n")
            
            # å†™å…¥ä¹ é¢˜
            if exercises_dedup:
                f.write("## ä¹ é¢˜åˆ—è¡¨ï¼ˆè¯¾åä½œä¸šï¼‰\n\n")
                for p in sorted(exercises_dedup, key=lambda x: [float(n) for n in re.findall(r'[\d.]+', x['id'])]):
                    content_preview = p['content'][:300] + "..." if len(p['content']) > 300 else p['content']
                    f.write(f"### {p['id']}\n\n")
                    f.write(f"{content_preview}\n\n")
                    f.write("---\n\n")
        
        print(f"âœ… é¢˜ç›®ç´¢å¼•å·²ä¿å­˜åˆ°: {index_file}")
        print(f"   (å»é‡å: {len(examples_dedup)} ä¾‹é¢˜, {len(exercises_dedup)} ä¹ é¢˜)")
        
        # æ˜¾ç¤ºéƒ¨åˆ†é¢˜ç›®é¢„è§ˆ
        print("\nğŸ“‹ é¢˜ç›®é¢„è§ˆ (å‰20ä¸ª):")
        all_dedup = examples_dedup + exercises_dedup
        for i, p in enumerate(all_dedup[:20], 1):
            print(f"  {i}. {p['id']}: {p['content'][:40]}...")
        if len(all_dedup) > 20:
            print(f"  ... è¿˜æœ‰ {len(all_dedup) - 20} ä¸ªé¢˜ç›® (å®Œæ•´åˆ—è¡¨è§ {index_file})")
        
        # æ­¥éª¤ 3: æŒ‰ç« èŠ‚ç»„ç»‡é¢˜ç›®å¹¶ä½¿ç”¨ API è§£ç­”
        llm = get_llm()
        solved_docs = []
        
        # æŒ‰ç« èŠ‚åˆ†ç»„é¢˜ç›®
        def get_chapter(prob_id):
            """ä»é¢˜ç›®IDæå–ç« èŠ‚å·"""
            nums = re.findall(r'[\d]+', prob_id)
            if nums:
                return nums[0]  # ç¬¬ä¸€ä¸ªæ•°å­—ä½œä¸ºç« èŠ‚å·
            return "å…¶ä»–"
        
        # åˆ†ç»„
        chapters = {}
        for prob in problems:
            chapter = get_chapter(prob['id'])
            if chapter not in chapters:
                chapters[chapter] = []
            chapters[chapter].append(prob)
        
        print(f"\nğŸ“š æŒ‰ç« èŠ‚åˆ†ç»„:")
        for ch, probs in sorted(chapters.items(), key=lambda x: int(x[0]) if x[0].isdigit() else 999):
            print(f"  ç¬¬ {ch} ç« : {len(probs)} é“é¢˜ç›®")
        
        # è§£ç­”æ‰€æœ‰é¢˜ç›®å¹¶æŒ‰ç« èŠ‚ç”Ÿæˆæ–‡æ¡£
        problems_to_solve = problems if MAX_PROBLEMS == 0 else problems[:MAX_PROBLEMS]
        
        if problems_to_solve:
            # æ£€æŸ¥å·²è§£ç­”çš„é¢˜ç›®
            solved_count = 0
            skipped_count = 0
            
            print(f"\nğŸ“ æ­£åœ¨ä½¿ç”¨ API è§£ç­” {len(problems_to_solve)} ä¸ªé¢˜ç›®...")
            
            # ç”¨äºæ”¶é›†æ¯ç« çš„è§£ç­”
            chapter_solutions = {}
            
            for i, prob in enumerate(problems_to_solve, 1):
                chapter = get_chapter(prob['id'])
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è§£ç­”æ–‡ä»¶
                safe_id = prob['id'].replace('.', '_')
                solution_file = f"{SOLUTIONS_DIR}/{safe_id}.md"
                
                need_resolve = False
                existing_solution = None
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è§£ç­”æ–‡ä»¶ï¼ˆä¾‹é¢˜å’Œä¹ é¢˜ä½¿ç”¨ç›¸åŒé€»è¾‘ï¼‰
                if os.path.exists(solution_file):
                    with open(solution_file, 'r', encoding='utf-8') as f:
                        existing_content = f.read()
                    
                    # æ£€æŸ¥ç°æœ‰æ–‡ä»¶ä¸­çš„é¢˜ç›®å†…å®¹æ˜¯å¦åŒ…å«"PDFè§£æä¸å®Œæ•´"
                    if 'PDFè§£æä¸å®Œæ•´' in existing_content or 'è¯·å‚è€ƒåŸæ–‡' in existing_content:
                        # æ£€æŸ¥æ–°æå–çš„é¢˜ç›®å†…å®¹æ˜¯å¦å·²ç»å®Œæ•´äº†
                        if 'PDFè§£æä¸å®Œæ•´' not in prob['content'] and 'è¯·å‚è€ƒåŸæ–‡' not in prob['content']:
                            # æ–°å†…å®¹å®Œæ•´ï¼Œéœ€è¦é‡æ–°è§£ç­”
                            need_resolve = True
                            print(f"  [{i}/{len(problems_to_solve)}] ğŸ”„ é‡æ–°è§£ç­” {prob['id']} (ä¹‹å‰è§£æä¸å®Œæ•´ï¼Œç°å·²ä¿®å¤)")
                        else:
                            # ä»ç„¶è§£æä¸å®Œæ•´ï¼Œè·³è¿‡
                            skipped_count += 1
                            print(f"  [{i}/{len(problems_to_solve)}] âš ï¸ è·³è¿‡ {prob['id']} (PDFè§£æä»ä¸å®Œæ•´)")
                            
                            # æå–è§£ç­”éƒ¨åˆ†
                            solution_match = re.search(r'## è§£ç­”\n\n(.+?)(?=\n---|$)', existing_content, re.DOTALL)
                            if solution_match:
                                existing_solution = solution_match.group(1).strip()
                            else:
                                existing_solution = existing_content
                    else:
                        # å·²æœ‰å®Œæ•´è§£ç­”ï¼Œè·³è¿‡
                        skipped_count += 1
                        print(f"  [{i}/{len(problems_to_solve)}] â­ï¸ è·³è¿‡ {prob['id']} (å·²æœ‰è§£ç­”)")
                        
                        # æå–è§£ç­”éƒ¨åˆ†
                        solution_match = re.search(r'## è§£ç­”\n\n(.+?)(?=\n---|$)', existing_content, re.DOTALL)
                        if solution_match:
                            existing_solution = solution_match.group(1).strip()
                        else:
                            existing_solution = existing_content
                else:
                    # æ²¡æœ‰è§£ç­”æ–‡ä»¶ï¼Œéœ€è¦è§£ç­”
                    need_resolve = True
                    print(f"  [{i}/{len(problems_to_solve)}] ğŸ†• æ­£åœ¨è§£ç­” {prob['id']}...")
                
                if not need_resolve and existing_solution:
                    # ä½¿ç”¨ç°æœ‰è§£ç­”
                    if chapter not in chapter_solutions:
                        chapter_solutions[chapter] = []
                    chapter_solutions[chapter].append({
                        'id': prob['id'],
                        'content': prob['content'],
                        'solution': existing_solution,
                        'type': prob['type']
                    })
                    
                    # æ·»åŠ åˆ°å‘é‡åº“
                    solved_docs.append(Document(
                        page_content=f"{prob['id']}ï¼š{prob['content']}\n\nè§£ç­”ï¼š{existing_solution}",
                        metadata={
                            'type': 'solved_problem', 
                            'problem_id': prob['id'],
                            'problem_type': prob['type'],
                            'chapter': chapter
                        }
                    ))
                    continue
                
                # éœ€è¦è°ƒç”¨ API è§£ç­”
                solution = None
                if need_resolve:
                    if not os.path.exists(solution_file):
                        print(f"  [{i}/{len(problems_to_solve)}] ğŸ†• æ­£åœ¨è§£ç­” {prob['id']}...")
                    
                    solution = solve_problem_with_api(llm, prob['id'], prob['content'], prob['type'])
                else:
                    continue  # åº”è¯¥ä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä½œä¸ºä¿æŠ¤
                
                if solution:
                    solved_count += 1
                    
                    # ç«‹å³ä¿å­˜åˆ°å•ç‹¬æ–‡ä»¶
                    with open(solution_file, 'w', encoding='utf-8') as f:
                        f.write(f"# {prob['id']}\n\n")
                        f.write(f"## é¢˜ç›®\n\n{prob['content']}\n\n")
                        f.write(f"## è§£ç­”\n\n{solution}\n\n")
                        f.write("---\n")
                    
                    # æ”¶é›†åˆ°å¯¹åº”ç« èŠ‚
                    if chapter not in chapter_solutions:
                        chapter_solutions[chapter] = []
                    chapter_solutions[chapter].append({
                        'id': prob['id'],
                        'content': prob['content'],
                        'solution': solution,
                        'type': prob['type']
                    })
                    
                    # æ·»åŠ åˆ°å‘é‡åº“
                    solved_docs.append(Document(
                        page_content=f"{prob['id']}ï¼š{prob['content']}\n\nè§£ç­”ï¼š{solution}",
                        metadata={
                            'type': 'solved_problem', 
                            'problem_id': prob['id'],
                            'problem_type': prob['type'],
                            'chapter': chapter
                        }
                    ))
                    print(f"    âœ… å·²è§£ç­”å¹¶ä¿å­˜åˆ° {solution_file}")
            
            print(f"\nğŸ“Š è§£ç­”ç»Ÿè®¡: æ–°è§£ç­” {solved_count} é“, è·³è¿‡å·²æœ‰ {skipped_count} é“")
            
            # æŒ‰ç« èŠ‚ç”Ÿæˆæ–‡æ¡£
            print(f"\nğŸ“„ æ­£åœ¨ç”Ÿæˆç« èŠ‚è§£ç­”æ–‡æ¡£...")
            for chapter, solutions in sorted(chapter_solutions.items(), key=lambda x: int(x[0]) if x[0].isdigit() else 999):
                chapter_file = f"{SOLUTIONS_DIR}/ç¬¬{chapter}ç« _é¢˜ç›®ä¸è§£ç­”.md"
                with open(chapter_file, 'w', encoding='utf-8') as f:
                    f.write(f"# ç¬¬ {chapter} ç«  - é¢˜ç›®ä¸è§£ç­”\n\n")
                    f.write(f"**æœ¬ç« å…± {len(solutions)} é“é¢˜ç›®**\n\n")
                    f.write("---\n\n")
                    
                    for sol in solutions:
                        type_label = "ä¾‹é¢˜" if sol['type'] == 'example' else "ä¹ é¢˜"
                        f.write(f"## {sol['id']}\n\n")
                        f.write(f"### é¢˜ç›®\n\n{sol['content']}\n\n")
                        f.write(f"### è§£ç­”\n\n{sol['solution']}\n\n")
                        f.write("---\n\n")
                
                print(f"  âœ… å·²ç”Ÿæˆ: {chapter_file} ({len(solutions)} é“é¢˜)")
        
        # ç”Ÿæˆè¡¥å……çŸ¥è¯†
        core_topics = get_topics()
        if core_topics:
            print(f"\nğŸ“š æ­£åœ¨ç”Ÿæˆ {len(core_topics)} ä¸ªä¸»é¢˜çš„è¡¥å……çŸ¥è¯†...")
            for i, topic in enumerate(core_topics, 1):
                print(f"  [{i}/{len(core_topics)}] {topic}...")
                knowledge = generate_supplementary_knowledge(llm, topic)
                if knowledge:
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    filename = f"{SOLUTIONS_DIR}/çŸ¥è¯†ç‚¹_{i}_{topic[:10]}.md"
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(f"# {topic}\n\n{knowledge}\n")
                    
                    # æ·»åŠ åˆ°å‘é‡åº“
                    solved_docs.append(Document(
                        page_content=f"{topic}\n\n{knowledge}",
                        metadata={'type': 'supplementary_knowledge', 'topic': topic}
                    ))
                    print(f"    âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜")
        
        # æ­¥éª¤ 4: åˆå¹¶æ‰€æœ‰æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨
        print(f"\nğŸ“Š åˆå¹¶æ–‡æ¡£...")
        all_documents = splits + solved_docs
        print(f"âœ… å…± {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ (åŸæ–‡: {len(splits)}, è§£ç­”+çŸ¥è¯†: {len(solved_docs)})")
        
        vectorstore = create_vectorstore(all_documents, CHROMA_DIR)
        
        # æµ‹è¯•æ£€ç´¢
        print("\nğŸ” æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        test_query = "ä»€ä¹ˆæ˜¯é©¬å°”å¯å¤«é“¾"
        results = vectorstore.similarity_search(test_query, k=2)
        print(f"æŸ¥è¯¢: '{test_query}'")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ")
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
        print("="*60)
        print(f"   ğŸ“ å‘é‡æ•°æ®åº“: {CHROMA_DIR}")
        print(f"   ğŸ“ è§£ç­”æ–‡ä»¶: {SOLUTIONS_DIR}")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(all_documents)}")
        print("   ğŸš€ è¿è¡Œ 'python main.py' å¼€å§‹é—®ç­”")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
