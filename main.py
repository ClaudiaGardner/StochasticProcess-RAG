"""
éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿä¸»ç¨‹åº
åŠŸèƒ½ï¼šåŸºäºå‘é‡æ•°æ®åº“è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®ç­”
"""

import os
from datetime import datetime
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from config_manager import (
    get_api_config, get_model_config, get_database_config, get_retrieval_config
)


def get_embeddings():
    """è·å– Embedding æ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ° HuggingFace æˆ– APIï¼‰"""
    model_config = get_model_config()
    embedding_model = model_config.get("embedding_model", "local")
    
    if embedding_model == "local":
        # è®¾ç½® HuggingFace é•œåƒ
        import os as _os
        _os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        from langchain_huggingface import HuggingFaceEmbeddings
        print("  ğŸ“¦ ä½¿ç”¨æœ¬åœ° HuggingFace Embedding æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    else:
        # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        api_config = get_api_config()
        return OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=api_config["api_key"],
            openai_api_base=api_config["base_url"],
        )


def get_llm(model_name=None, temperature=0.3):
    """è·å– LLM å®ä¾‹ï¼Œæ”¯æŒæŒ‡å®šæ¨¡å‹åç§°"""
    api_config = get_api_config()
    model_config = get_model_config()
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°å°±ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
    if model_name is None:
        chat_models = model_config.get("chat_models", ["gemini-3-pro-preview"])
        model_name = chat_models[0] if isinstance(chat_models, list) else chat_models
    
    return ChatOpenAI(
        model=model_name,
        temperature=temperature,
        openai_api_key=api_config["api_key"],
        openai_api_base=api_config["base_url"],
    )


def load_vectorstore():
    """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“"""
    db_config = get_database_config()
    persist_directory = db_config.get("chroma_dir", "./chroma_db")
    
    if not os.path.exists(persist_directory):
        raise FileNotFoundError(
            f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {persist_directory}\n"
            f"è¯·å…ˆè¿è¡Œ 'python ingest.py' æ„å»ºæ•°æ®åº“"
        )
    
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {persist_directory}")
    
    embeddings = get_embeddings()
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
    return vectorstore


def create_qa_chain(vectorstore, llm):
    """åˆ›å»ºç®€å•çš„ QA æ£€ç´¢å‡½æ•°"""
    retrieval_config = get_retrieval_config()
    top_k = retrieval_config.get("top_k", 5)
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": top_k}
    )
    
    class SimpleQAChain:
        def __init__(self, retriever, llm):
            self.retriever = retriever
            self.llm = llm
        
        def invoke(self, inputs):
            question = inputs.get("input", "")
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            docs = self.retriever.invoke(question)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n---\n\n".join([doc.page_content for doc in docs])
            
            # æ„å»ºæç¤º
            prompt = f"""ä½ æ˜¯ä¸€ä½æ¦‚ç‡è®ºä¸éšæœºè¿‡ç¨‹é¢†åŸŸçš„ä¸“å®¶æ•™æˆã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

**èƒŒæ™¯çŸ¥è¯†**:
{context}

---

**å›ç­”è¦æ±‚**ï¼š
1. ä¸¥æ ¼åŸºäºèƒŒæ™¯çŸ¥è¯†ä¸­çš„å®šä¹‰å’Œå®šç†è¿›è¡Œå›ç­”
2. **æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼**ï¼š
   - è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`ï¼Œå¦‚ $P(X=k)$
   - è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$`
3. ä½¿ç”¨æ ‡å‡†æ¦‚ç‡è®ºè®°å·ï¼ˆ$P$, $E$, $\\operatorname{{Var}}$, $\\sigma$ ç­‰ï¼‰
4. å¦‚æœæ˜¯ä¾‹é¢˜æˆ–ä¹ é¢˜ï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„è§£é¢˜æ­¥éª¤
5. å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ çš„ä¸“ä¸šè§è§£
6. å›ç­”æ—¶ä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œæ¡ç†

**å­¦ç”Ÿé—®é¢˜**: {question}

**æ•™æˆå›ç­”**:"""
            
            # è°ƒç”¨ LLM
            response = self.llm.invoke(prompt)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”å¯¹è±¡
            if isinstance(response, str):
                answer = response
            else:
                answer = response.content
            
            return {
                "input": question,
                "context": docs,
                "answer": answer
            }
    
    return SimpleQAChain(retriever, llm)


def save_answer_as_markdown(question, answer, context_docs, output_dir="./answers"):
    """å°†å›ç­”ä¿å­˜ä¸ºæ ¼å¼åŒ–çš„Markdownæ–‡æ¡£"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³é¿å…é‡å¤ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_question = "".join(c for c in question[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_question = safe_question.replace(' ', '_')
    filename = f"{timestamp}_{safe_question}.md"
    filepath = os.path.join(output_dir, filename)
    
    # æ„å»ºMarkdownå†…å®¹
    markdown_content = f"""# éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿ - å›ç­”è®°å½•

**æé—®æ—¶é—´**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}
**é—®é¢˜**: {question}

## ğŸ¤– AI å›ç­”

{answer}

---

## ğŸ“š å‚è€ƒæ¥æº ({len(context_docs)} ä¸ª)

"""
    
    # æ·»åŠ å‚è€ƒæ¥æº
    for i, doc in enumerate(context_docs[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ¥æº
        doc_type = doc.metadata.get('type', 'original')
        type_label = {
            'original': 'ğŸ“„ åŸæ–‡',
            'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜',
            'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
        }.get(doc_type, 'ğŸ“„')
        
        extra_info = ""
        if 'problem_id' in doc.metadata:
            extra_info = f" - ä¾‹é¢˜ {doc.metadata['problem_id']}"
        elif 'topic' in doc.metadata:
            extra_info = f" - {doc.metadata['topic']}"
        
        markdown_content += f"""
### [{i}] {type_label}{extra_info}

```
{doc.page_content[:500]}{"..." if len(doc.page_content) > 500 else ""}
```

---
"""
    
    markdown_content += f"""

## ğŸ“„ ç³»ç»Ÿä¿¡æ¯

- **å‘é‡æ•°æ®åº“**: Chroma
- **åµŒå…¥æ¨¡å‹**: æœ¬åœ° HuggingFace
- **LLM**: {get_model_config().get('chat_models', ['é»˜è®¤'])[0]}
- **æ£€ç´¢æ–‡æ¡£æ•°**: {len(context_docs)}

---
*ç”±éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿç”Ÿæˆ*
"""
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    return filepath


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼é—®ç­”"""
    print("="*60)
    print(" ğŸ“ éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿ")
    print("="*60)
    
    try:
        vectorstore = load_vectorstore()
        
        print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹...")
        model_config = get_model_config()
        llm = get_llm(temperature=model_config.get("temperature", 0.3))
        print("âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ”— æ­£åœ¨åˆ›å»º QA æ£€ç´¢é“¾...")
        qa_chain = create_qa_chain(vectorstore, llm)
        print("âœ… QA é“¾åˆ›å»ºæˆåŠŸ\n")
        
        print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰")
        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼š")
        print("   - ä»€ä¹ˆæ˜¯é©¬å°”å¯å¤«é“¾ï¼Ÿ")
        print("   - æ³Šæ¾è¿‡ç¨‹æœ‰ä»€ä¹ˆæ€§è´¨ï¼Ÿ")
        print("   - è§£é‡Šå¸¸è¿”æ€å’Œç¬æ—¶æ€çš„åŒºåˆ«")
        print()
        
        while True:
            question = input("ğŸ™‹ æ‚¨çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            try:
                print("\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†...")
                result = qa_chain.invoke({"input": question})
                
                print("\n" + "="*60)
                print("ğŸ¤– å›ç­”:")
                print("-"*60)
                print(result['answer'])
                print("="*60)
                
                # è‡ªåŠ¨ä¿å­˜å›ç­”ä¸ºMarkdownæ–‡ä»¶
                try:
                    filepath = save_answer_as_markdown(
                        question=result['input'],
                        answer=result['answer'],
                        context_docs=result['context']
                    )
                    print(f"ğŸ’¾ å›ç­”å·²è‡ªåŠ¨ä¿å­˜åˆ°: {filepath}")
                    print("ï¿½ å¯ä»¥ä½¿ç”¨ Markdown æŸ¥çœ‹å™¨æ‰“å¼€æ–‡ä»¶ï¼Œæ•°å­¦å…¬å¼å°†æ­£ç¡®æ˜¾ç¤º")
                except Exception as e:
                    print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                
                if result.get('context'):
                    print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['context'])} ä¸ª):")
                    for i, doc in enumerate(result['context'][:3], 1):
                        doc_type = doc.metadata.get('type', 'original')
                        type_label = {
                            'original': 'ğŸ“„ åŸæ–‡',
                            'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜',
                            'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
                        }.get(doc_type, 'ğŸ“„')
                        
                        extra_info = ""
                        if 'problem_id' in doc.metadata:
                            extra_info = f" - ä¾‹é¢˜ {doc.metadata['problem_id']}"
                        elif 'topic' in doc.metadata:
                            extra_info = f" - {doc.metadata['topic']}"
                        
                        print(f"\n[{i}] {type_label}{extra_info}")
                        print("-"*40)
                        content = doc.page_content[:150]
                        print(content + "..." if len(doc.page_content) > 150 else content)
                
                print("\n")
                
            except Exception as e:
                print(f"\nâŒ é—®ç­”å‡ºé”™: {str(e)}\n")
    
    except FileNotFoundError as e:
        print(f"\nâŒ {str(e)}\n")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}\n")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
