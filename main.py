"""
éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿä¸»ç¨‹åº
åŠŸèƒ½ï¼šåŸºäºå‘é‡æ•°æ®åº“è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é—®ç­”
"""

import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from config_manager import (
    get_api_config, get_model_config, get_database_config, get_retrieval_config
)


def get_embeddings():
    """è·å– Embedding æ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ° HuggingFace æˆ– APIï¼‰"""
    model_config = get_model_config()
    embedding_model = model_config.get("embedding_model", "local")
    
    if embedding_model == "local":
        # ä½¿ç”¨æœ¬åœ° HuggingFace æ¨¡å‹
        from langchain_huggingface import HuggingFaceEmbeddings
        return HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    else:
        # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        api_config = get_api_config()
        return OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=api_config["api_key"],
            openai_api_base=api_config["base_url"],
        )


def get_llm(temperature=0.3):
    """è·å– LLM å®ä¾‹"""
    api_config = get_api_config()
    model_config = get_model_config()
    
    return ChatOpenAI(
        model=model_config.get("chat_model", "gemini-3-pro-preview"),
        temperature=temperature,
        openai_api_key=api_config["api_key"],
        openai_api_base=api_config["base_url"],
    )


def load_vectorstore():
    """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“"""
    db_config = get_database_config()
    persist_directory = db_config.get("chroma_dir", "./chroma_db")
    
    if not os.path.exists(persist_directory):
        raise FileNotFoundError(
            f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {persist_directory}\n"
            f"è¯·å…ˆè¿è¡Œ 'python ingest.py' æ„å»ºæ•°æ®åº“"
        )
    
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {persist_directory}")
    
    embeddings = get_embeddings()
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
    return vectorstore


def create_qa_chain(vectorstore, llm):
    """åˆ›å»º QA æ£€ç´¢é“¾"""
    retrieval_config = get_retrieval_config()
    top_k = retrieval_config.get("top_k", 5)
    
    template = """ä½ æ˜¯ä¸€ä½æ¦‚ç‡è®ºä¸éšæœºè¿‡ç¨‹é¢†åŸŸçš„ä¸“å®¶æ•™æˆã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

**èƒŒæ™¯çŸ¥è¯†**:
{context}

---

**å›ç­”è¦æ±‚**ï¼š
1. ä¸¥æ ¼åŸºäºèƒŒæ™¯çŸ¥è¯†ä¸­çš„å®šä¹‰å’Œå®šç†è¿›è¡Œå›ç­”
2. **æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼**ï¼š
   - è¡Œå†…å…¬å¼ä½¿ç”¨ `$...$`ï¼Œå¦‚ $P(X=k)$
   - è¡Œé—´å…¬å¼ä½¿ç”¨ `$$...$$`
3. ä½¿ç”¨æ ‡å‡†æ¦‚ç‡è®ºè®°å·ï¼ˆ$P$, $E$, $\\operatorname{{Var}}$, $\\sigma$ ç­‰ï¼‰
4. å¦‚æœæ˜¯ä¾‹é¢˜æˆ–ä¹ é¢˜ï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„è§£é¢˜æ­¥éª¤
5. å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ çš„ä¸“ä¸šè§è§£
6. å›ç­”æ—¶ä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œæ¡ç†

**å­¦ç”Ÿé—®é¢˜**: {question}

**æ•™æˆå›ç­”**:"""

    PROMPT = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": top_k}
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    return qa_chain


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼é—®ç­”"""
    print("="*60)
    print(" ğŸ“ éšæœºè¿‡ç¨‹ RAG é—®ç­”ç³»ç»Ÿ")
    print("="*60)
    
    try:
        vectorstore = load_vectorstore()
        
        print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹...")
        model_config = get_model_config()
        llm = get_llm(temperature=model_config.get("temperature", 0.3))
        print("âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ”— æ­£åœ¨åˆ›å»º QA æ£€ç´¢é“¾...")
        qa_chain = create_qa_chain(vectorstore, llm)
        print("âœ… QA é“¾åˆ›å»ºæˆåŠŸ\n")
        
        print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰")
        print("ğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼š")
        print("   - ä»€ä¹ˆæ˜¯é©¬å°”å¯å¤«é“¾ï¼Ÿ")
        print("   - æ³Šæ¾è¿‡ç¨‹æœ‰ä»€ä¹ˆæ€§è´¨ï¼Ÿ")
        print("   - è§£é‡Šå¸¸è¿”æ€å’Œç¬æ—¶æ€çš„åŒºåˆ«")
        print()
        
        while True:
            question = input("ğŸ™‹ æ‚¨çš„é—®é¢˜: ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            try:
                print("\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†...")
                result = qa_chain.invoke({"query": question})
                
                print("\n" + "="*60)
                print("ğŸ¤– å›ç­”:")
                print("-"*60)
                print(result['result'])
                print("="*60)
                
                if result.get('source_documents'):
                    print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['source_documents'])} ä¸ª):")
                    for i, doc in enumerate(result['source_documents'][:3], 1):
                        doc_type = doc.metadata.get('type', 'original')
                        type_label = {
                            'original': 'ğŸ“„ åŸæ–‡',
                            'solved_problem': 'âœ… å·²è§£ç­”ä¾‹é¢˜',
                            'supplementary_knowledge': 'ğŸ“– è¡¥å……çŸ¥è¯†'
                        }.get(doc_type, 'ğŸ“„')
                        
                        extra_info = ""
                        if 'problem_id' in doc.metadata:
                            extra_info = f" - ä¾‹é¢˜ {doc.metadata['problem_id']}"
                        elif 'topic' in doc.metadata:
                            extra_info = f" - {doc.metadata['topic']}"
                        
                        print(f"\n[{i}] {type_label}{extra_info}")
                        print("-"*40)
                        content = doc.page_content[:150]
                        print(content + "..." if len(doc.page_content) > 150 else content)
                
                print("\n")
                
            except Exception as e:
                print(f"\nâŒ é—®ç­”å‡ºé”™: {str(e)}\n")
    
    except FileNotFoundError as e:
        print(f"\nâŒ {str(e)}\n")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}\n")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
